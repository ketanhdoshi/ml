{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_lib.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "y9nsrVG21R--",
        "Z1HFF90HGFkW",
        "7drYJPSy8mWF",
        "FPbMUu-fsu2N",
        "l_TByQrpGMjd",
        "iTY7gpLkW3CC",
        "H26FLixOGSyA",
        "7X_LUJ_uGYCa",
        "A2i9CvYzu5JT",
        "IyFlrde1vj1J",
        "liLQx3AFqA3r",
        "hHmMHJcws9EU",
        "grttVe4vtLVU",
        "qsU9v5FJx_Bt",
        "IiBaeYR7zixG",
        "0ykSPsTe1Nh3",
        "4n8HEjSktOYX",
        "3Dt6Map-z23s",
        "PQh4Iucv0yDz",
        "hQpVs0wL0BAc",
        "ZOTT_1L2zhlL",
        "tlAOWV4OFf6O",
        "f67GwyU4Xe0L"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanhdoshi/ml/blob/master/lib/image_lib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PPTvq4UHmso"
      },
      "source": [
        "### Image Processing Examples and Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED9jc6xzw4FQ"
      },
      "source": [
        "**Todos**\n",
        "*   Display of Keypt, Bounding Box\n",
        "*   Augmentation of Keypt, Bounding Box\n",
        "*   Display of results, including Keypt, Bounding Box\n",
        "*   Video processing\n",
        "*   Make image and video utilities independent of torch/keras framework\n",
        "*   ocv open Gif file code should be general for grey scale\n",
        "*   ocv byte tensor should handle grey scale 2D and colour 3D. should not permute if 2D\n",
        "*   ShowImg show_image should handle 3D and 2D greyscale, so mask can be shown as an image\n",
        "*   Comments for ShowImg\n",
        "*   Remove WAIT_show_grid and merge show_grid and new_grid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9nsrVG21R--"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVF14yO71kuO"
      },
      "source": [
        "#export\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math, random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmkGsyG0TSRq"
      },
      "source": [
        "import IPython.core.debugger as db\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.patches as patches\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "# options are 'all', 'none', 'last' and 'last_expr\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU23hChRz_Gb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "gn_path = 'gdrive/My Drive/Colab Notebooks'  #change dir to your project folder\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, gn_path + '/exp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1HFF90HGFkW"
      },
      "source": [
        "### Fetch images for experimentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvT7q_2kb6Qz"
      },
      "source": [
        "# Download some image files\n",
        "!wget 'https://live.staticflickr.com/4091/4994221690_d070e8a355_b_d.jpg'\n",
        "!mv 4994221690_d070e8a355_b_d.jpg mug.jpg\n",
        "\n",
        "!wget 'https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/01/bondi_beach.jpg'\n",
        "!mv bondi_beach.jpg beach.jpg\n",
        "\n",
        "!wget 'http://i.stack.imgur.com/SYxmp.jpg'\n",
        "!mv SYxmp.jpg girl.jpg\n",
        "\n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Dryocopus_pileatus_MP2.jpg/600px-Dryocopus_pileatus_MP2.jpg -q -O woodpecker.jpg\n",
        "\n",
        "from IPython.display import Image, display\n",
        "display(Image('mug.jpg'))\n",
        "display(Image('beach.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8dXlWrxtYF8"
      },
      "source": [
        "root_path = Path.cwd()\n",
        "gd_path = Path.cwd() / 'gdrive' / 'My Drive' / 'Colab Data' / 'fastai-v3' / 'data' / 'augment'\n",
        "\n",
        "mug_path = root_path/'mug.jpg'\n",
        "beach_path = root_path/'beach.jpg'\n",
        "girl_path = root_path/'girl.jpg'\n",
        "woodpecker_path = root_path / 'woodpecker.jpg'\n",
        "\n",
        "dog_path = gd_path / 'small dog.png'\n",
        "dog_mask_path = gd_path / 'small dog mask.png'\n",
        "parrot_path = gd_path / 'parrot.png'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7drYJPSy8mWF"
      },
      "source": [
        "### Display Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzoudkuKn9r1"
      },
      "source": [
        "#export\n",
        "\n",
        "import torch\n",
        "\n",
        "class ShowImg():\n",
        "  # ----------------------------\n",
        "  # Can display both image tensors and PIL image objects\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def show_image(img, ax):\n",
        "    #if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)\n",
        "    ax.axis('off')\n",
        "    if (isinstance(img, torch.Tensor)):\n",
        "      img = img.permute(1,2,0)\n",
        "    ax.imshow(img)\n",
        "\n",
        "  # ----------------------------\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def show_label(label, ax):\n",
        "    ax.set_title(f'{label}')\n",
        "\n",
        "  # ----------------------------\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def show_mask(mask, ax):\n",
        "    #!!!! what about OCV mask !!!! mask = mask.convert('RGBA')\n",
        "    ax.imshow(mask, alpha=0.7, cmap=\"Reds\")\n",
        "\n",
        "  # ----------------------------\n",
        "  # \n",
        "  # ----------------------------\n",
        "  @classmethod\n",
        "  def WAIT_show_grid(cls, x_imgs, y_labels, z_data=None, x_method=None, y_method=None, z_method=None, num_cols=10, figsize=None, **kwargs):\n",
        "    assert(len(x_imgs) == len(y_labels))\n",
        "    x_method = cls.show_image if (x_method is None) else x_method\n",
        "    y_method = cls.show_label if (y_method is None) else y_method\n",
        "    z_method = cls.show_label if (z_method is None) else z_method\n",
        "\n",
        "    num_imgs = len(x_imgs)\n",
        "    num_rows = int (math.ceil (num_imgs / num_cols))\n",
        "    if (figsize is None):\n",
        "      figsize=(num_cols * 3, num_rows * 3)\n",
        "    fig,axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "\n",
        "    for img, label, ax in zip (x_imgs, y_labels, axes.flat):\n",
        "      x_method(img, ax)\n",
        "      y_method(label, ax)\n",
        "      #self.show_image(img, ax)\n",
        "      #self.show_label(label, ax)\n",
        "\n",
        "    for i in range(len(x_imgs), len(axes.flat)): axes.flat[i].set_visible(False)\n",
        "\n",
        "  # ----------------------------\n",
        "  # \n",
        "  # ----------------------------\n",
        "  def show_grid(cls, x_imgs, y_data, z_data=None, x_method=None, y_method=None, z_method=None, num_cols=10, figsize=None, **kwargs):\n",
        "    x_method = cls.show_image if (x_method is None) else x_method\n",
        "    y_method = cls.show_label if (y_method is None) else y_method\n",
        "    z_method = cls.show_label if (z_method is None) else z_method\n",
        "\n",
        "    if (y_method == cls.show_label):\n",
        "      yz_type = 'label'\n",
        "    elif (y_method == cls.show_mask):\n",
        "      yz_type = 'mask'\n",
        "    elif (y_method == cls.show_image):\n",
        "      yz_type = 'image'\n",
        "\n",
        "    self._new_grid(cls, x_imgs, y_data, z_data, yz_type, num_cols=10, figsize=None, **kwargs)\n",
        "\n",
        "  # ----------------------------\n",
        "  # \n",
        "  # ----------------------------\n",
        "  def _new_grid(cls, x_imgs, y_data, z_data=None, yz_type='label', max_num_cols=10, figsize=None, **kwargs):\n",
        "    assert(len(x_imgs) == len(y_data))\n",
        "    assert((z_data is None) or (len(x_imgs) == len(z_data)))\n",
        "\n",
        "    # Number of image sets\n",
        "    num_img_sets = len(x_imgs)\n",
        "\n",
        "    # Number of images in each set\n",
        "    set_len = 1\n",
        "    if (yz_type == 'image'):\n",
        "       if (y_data is not None):\n",
        "         set_len += 1\n",
        "       if (z_data is not None):\n",
        "         set_len += 1\n",
        "    \n",
        "    elif (yz_type in ['mask', 'bbox', 'keypt']):\n",
        "      if (z_data is not None):\n",
        "        set_len += 1\n",
        "\n",
        "    # Calculate number of rows and columns based on the number of image sets, and\n",
        "    # the number of images per set.\n",
        "    num_sets_per_row = max_num_cols // set_len\n",
        "    num_cols = num_sets_per_row * set_len\n",
        "    num_rows = int (math.ceil (num_img_sets / num_sets_per_row))\n",
        "\n",
        "    if (figsize is None):\n",
        "      figsize=(num_cols * 3, num_rows * 3)\n",
        "    fig,axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "\n",
        "    y_data = y_data if y_data is not None else [None] * num_img_sets\n",
        "    z_data = z_data if z_data is not None else [None] * num_img_sets\n",
        "\n",
        "    i = 0\n",
        "    for x, y, z in zip (x_imgs, y_data, z_data):\n",
        "      if (yz_type == 'image'):\n",
        "        i = self._xyz_image(cls, x, y, z, axes.flat, i)\n",
        "      elif (yz_type == 'label'):\n",
        "        i = self._xyz_label(cls, x, y, z, axes.flat, i)\n",
        "      elif (yz_type in ['mask', 'bbox', 'keypt']):\n",
        "        i = self._xyz_overlay(cls, x, y, z, yz_type, axes.flat, i)\n",
        "      \n",
        "      i += 1\n",
        "\n",
        "    for j in range(i, len(axes.flat)): axes.flat[j].set_visible(False)\n",
        "\n",
        "  # ----------------------------\n",
        "  # \n",
        "  # ----------------------------\n",
        "  def _xyz_label(cls, x, y, z, axs_flat, i):\n",
        "    ax = axs_flat[i] \n",
        "    cls.show_image(x, ax)\n",
        "\n",
        "    label = f'{y}/{z}' if (z is not None) else f'{y}'\n",
        "    cls.show_label(label, ax)\n",
        " \n",
        "    return i\n",
        "\n",
        "  # ----------------------------\n",
        "  # \n",
        "  # ----------------------------\n",
        "  def _xyz_overlay(cls, x, y, z, yz_type, axs_flat, i):\n",
        "    ax = axs_flat[i] \n",
        "    cls.show_image(x, ax)\n",
        "\n",
        "    if (yz_type == 'mask'):\n",
        "      yz_method = cls.show_mask\n",
        "    elif (yz_type == 'bbox'):\n",
        "      yz_method = cls.show_bbox\n",
        "    elif (yz_type == 'keypt'):\n",
        "      yz_method = cls.show_keypt\n",
        "\n",
        "    if (y is not None):\n",
        "      yz_method(y, ax)\n",
        "\n",
        "    if (z is not None):\n",
        "      i += 1\n",
        "      ax = axs_flat[i]\n",
        "      cls.show_image(x, ax)\n",
        "      yz_method(z > 0, ax)\n",
        "    \n",
        "    return i\n",
        "\n",
        "  # ----------------------------\n",
        "  # \n",
        "  # ----------------------------\n",
        "  def _xyz_image(cls, x, y, z, axs_flat, i):\n",
        "    ax = axs_flat[i] \n",
        "    cls.show_image(x, ax)\n",
        "\n",
        "    if (y is not None):\n",
        "      i += 1\n",
        "      ax = axs_flat[i]\n",
        "      cls.show_image(y, ax)\n",
        "\n",
        "    if (z is not None):\n",
        "      i += 1\n",
        "      ax = axs_flat[i]\n",
        "      cls.show_image(z, ax)\n",
        "\n",
        "    return i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srRvbSSCxGuW"
      },
      "source": [
        "#------------------------------------------------------\n",
        "# Plot an image on an existing figure and axes\n",
        "#------------------------------------------------------\n",
        "def plot_image (im, cmap=None):\n",
        "  if (cmap is None):\n",
        "    plt.imshow(im)\n",
        "  else:\n",
        "    plt.imshow(im, cmap)\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Plot two images side-by-side for a before-after comparison\n",
        "#------------------------------------------------------\n",
        "def plot_image_compare (im1, im2, cmap1=None, cmap2=None):\n",
        "  plt.figure(figsize=(10,4))\n",
        "\n",
        "  plt.subplot(1,2,1)\n",
        "  plot_image (im1, cmap1)\n",
        "  plt.axis('off')\n",
        "  plt.title('Original Image')\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plot_image (im2, cmap2)\n",
        "  plt.axis('off')\n",
        "  plt.title('Grayscale Image')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "  \n",
        "#------------------------------------------------------\n",
        "# Plot the histogram of greyscale values along with a threshold line\n",
        "#------------------------------------------------------\n",
        "def plot_hist_threshold (im_grey, thresh):\n",
        "  im_pixels = im_grey.flatten()\n",
        "  plt.hist(im_pixels,bins=50)\n",
        "  plt.vlines(thresh, 0, 100000, linestyle='--')\n",
        "  plt.ylim([0,50000])\n",
        "  plt.title('Grayscale Histogram')  \n",
        "  \n",
        "#------------------------------------------------------\n",
        "# Draw a bounding box on an image\n",
        "#------------------------------------------------------\n",
        "def bounding_box (ax, x, y, width, height):\n",
        "  # Create a Rectangle patch\n",
        "  rect = patches.Rectangle((x,y),width,height,linewidth=1,edgecolor='r',facecolor='none')\n",
        "\n",
        "  # Add the patch to the Axes\n",
        "  ax.add_patch(rect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPbMUu-fsu2N"
      },
      "source": [
        "## Scipy-Image library experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP_RqE-Jc7QG"
      },
      "source": [
        "from skimage import io as skio\n",
        "from skimage.color import rgb2gray\n",
        "from skimage import exposure\n",
        "from skimage import filters\n",
        "from scipy import ndimage\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Read an image\n",
        "#------------------------------------------------------\n",
        "def read_image (file):\n",
        "  im = skio.imread(str(file))\n",
        "  return (im)\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Convert an image to greyscale\n",
        "#------------------------------------------------------\n",
        "def greyscale (im):\n",
        "  im_grey = rgb2gray(im)\n",
        "  return (im_grey)\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Segment a greyscale image into foreground and background by calculating\n",
        "# a separation threshold based on the histogram of pixel intensities\n",
        "# Also use the threshold to produce a mask of the foreground and background\n",
        "# Otsu's method, named after Nobuyuki Otsu is used to automatically perform clustering-based \n",
        "# image thresholding, or, the reduction of a graylevel image to a binary image. The \n",
        "# algorithm assumes that the image contains two classes of pixels following bi-modal \n",
        "# histogram (foreground pixels and background pixels), it then calculates the \n",
        "# optimum threshold separating the two classes so that their inter-class variance \n",
        "# is maximal. Otsu’s method exhibits relatively good performance if the histogram \n",
        "# can be assumed to have bimodal distribution and assumed to possess a deep and \n",
        "# sharp valley between two peaks\n",
        "#------------------------------------------------------\n",
        "def segment_foreback (im_grey):\n",
        "  thresh = filters.threshold_otsu(im_grey)\n",
        "  mask = np.where(im_grey > thresh, 1, 0)\n",
        "\n",
        "  return (mask, thresh)\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Get a list of all objects, along with separate masks for each object\n",
        "# To identify individual objects in the image we separate the connected components\n",
        "# where all the neighbouring pixels are connected.\n",
        "# Compute all the separate connected components in the image and label each \n",
        "# individual component with a different pixel value ie. a different colour\n",
        "#------------------------------------------------------\n",
        "def get_objects_with_mask (im_mask):\n",
        "  objects, num_objects = ndimage.label(synthetic_mask)\n",
        "  \n",
        "  # Make an array of masks, one for each object\n",
        "  object_masks = []\n",
        "  for object_idx in range(1, num_objects+1):\n",
        "    obj_mask = np.where(objects == object_idx, 1, 0)\n",
        "    object_masks.append(obj_mask)\n",
        "    \n",
        "  return (objects, num_objects, object_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_TByQrpGMjd"
      },
      "source": [
        "### Converting to greyscale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no2T7TbW11xQ"
      },
      "source": [
        "# Read the image into an array and print the image dimensions\n",
        "# It has 3 dimensions, where the 3rd dimension is the RGB channel\n",
        "im = read_image (IMAGE_GIRL)\n",
        "print('Original image shape: {}, Datatype: {}'.format(im.shape, im.dtype))\n",
        "\n",
        "# Convert it to a grey scale. It now has only 2 dimensions\n",
        "im_gray = greyscale (im)\n",
        "print('New image shape: {}'.format(im_gray.shape))\n",
        "\n",
        "# Plot the original and greyscale images\n",
        "plot_image_compare (im, im_gray, None, 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTY7gpLkW3CC"
      },
      "source": [
        "### Draw a bounding box on the image (with matplotlib, not scipy specific)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qop2P7ToW18_"
      },
      "source": [
        "import matplotlib.patches as patches\n",
        "\n",
        "# Create figure and axes\n",
        "fig,ax = plt.subplots(1, figsize=(7,5))\n",
        "\n",
        "# Display the image\n",
        "ax.imshow(im)\n",
        "\n",
        "# Draw a bounding box\n",
        "bounding_box (ax, 280, 10, 230, 320)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H26FLixOGSyA"
      },
      "source": [
        "### Segmenting into Foreground and Background"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrErzDRE1-WC"
      },
      "source": [
        "# Segment the image into foreground and background\n",
        "mask, thresh_val = segment_foreback (im_gray)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Plot the mask\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(mask, cmap='gray', interpolation='nearest')\n",
        "plt.title('Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "# Create a \"transparent\" mask by blanking out all the white (ie. 0) pixels in the\n",
        "# initial mask. Superimpose this mask in a different colour on the original image\n",
        "plt.subplot(1,2,2)\n",
        "mask_for_display = np.where(mask, mask, np.nan)\n",
        "plt.imshow(im_gray, cmap='gray')\n",
        "plt.imshow(mask_for_display, cmap='rainbow', alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.title('Image w/ Transparent Mask')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqdpAXEKu1JG"
      },
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plot_hist_threshold (im_gray, thresh_val)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "# Plot the histogram based on the skimage tutorial using skimage.exposure\n",
        "# Not sure why it looks different from the normal histogram on the left\n",
        "hist, bins_center = exposure.histogram(im_gray)\n",
        "plt.plot(bins_center, hist, lw=2)\n",
        "plt.axvline(thresh_val, color='g', ls='--')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7X_LUJ_uGYCa"
      },
      "source": [
        "### Identifying individual objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA0YNJZG8QnX"
      },
      "source": [
        "# Generate a synthetic image that contains several distinct blobs\n",
        "def generate_image ():\n",
        "  np.random.seed(1)\n",
        "  n = 10\n",
        "  l = 256\n",
        "  im = np.zeros((l, l))\n",
        "  points = l*np.random.random((2, n**2))\n",
        "  im[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n",
        "  im = ndimage.gaussian_filter(im, sigma=l/(4.*n))\n",
        "  return (im)\n",
        "\n",
        "# Plot bounding boxes for all the objects in the image\n",
        "def object_boxes (ax, objects):\n",
        "  for label_idx, (sliceY, sliceX) in enumerate(ndimage.find_objects(objects)):\n",
        "    x, y = sliceX.start, sliceY.start\n",
        "    width = sliceX.stop - sliceX.start\n",
        "    height = sliceY.stop - sliceY.start\n",
        "    bounding_box (ax, x, y, width, height)\n",
        "\n",
        "# To identify individual objects in the image, generate a suitable sample image\n",
        "synthetic_im = generate_image()\n",
        "# Generate a mask using a threshold of the mean value\n",
        "synthetic_mask = synthetic_im > synthetic_im.mean()\n",
        "\n",
        "#labels, nlabels = ndimage.label(synthetic_mask)\n",
        "labels, nlabels, _ = get_objects_with_mask (synthetic_mask)\n",
        "\n",
        "print('There are {} separate objects detected.'.format(nlabels))\n",
        "\n",
        "# Create figure and axes\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16,5))\n",
        "\n",
        "# ------- Plot 1 - Original image --------\n",
        "ax = axes[0]\n",
        "ax.imshow(synthetic_im)\n",
        "ax.set_title('Synthetic Image')\n",
        "ax.axis('off')\n",
        "\n",
        "# ------- Plot 2 - Mask ------- \n",
        "ax = axes[1]\n",
        "ax.imshow(synthetic_mask, cmap=plt.cm.gray)\n",
        "ax.set_title('Mask of Mean Pixel Values')\n",
        "ax.axis('off')\n",
        "\n",
        "# ------- Plot 3 - Objects based on Connected Components ------- \n",
        "ax = axes[2]\n",
        "# Create a random colormap so that each label value gets a distinct colour\n",
        "from matplotlib.colors import ListedColormap\n",
        "rand_cmap = ListedColormap(np.random.rand(256,3))\n",
        "# Create a transparent mask where non-labeled values are blanked out\n",
        "labels_for_display = np.where(labels > 0, labels, np.nan)\n",
        "ax.imshow(synthetic_im, cmap='gray')\n",
        "ax.imshow(labels_for_display, cmap=rand_cmap)\n",
        "ax.set_title('Connected Components ({} Objects)'.format(nlabels))\n",
        "ax.axis('off')\n",
        "\n",
        "# ------- Plot 4 - Bounding Boxes for all Objects -------\n",
        "ax = axes[3]\n",
        "ax.imshow(synthetic_im)\n",
        "# Draw all the bounding boxes\n",
        "object_boxes (ax, labels)\n",
        "ax.set_title('Bounding Boxes ({} Objects)'.format(nlabels))\n",
        "ax.axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.02, hspace=0.02, top=1, bottom=0, left=0, right=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdNSzP5mLBsV"
      },
      "source": [
        "# Display a grid of each object, zoomed in\n",
        "#-----------------------------------------------------------------------\n",
        "num_rows, num_cols = 4, 8\n",
        "fig,axes = plt.subplots(num_rows, num_cols, figsize=(18, 8))\n",
        "\n",
        "for label_idx, label_coords in enumerate(ndimage.find_objects(labels)):\n",
        "  # Get the slice of the image corresponding to this object\n",
        "  cell = synthetic_im[label_coords]\n",
        "\n",
        "  ax = axes[label_idx // num_cols, label_idx % num_cols]\n",
        "  ax.imshow(cell)\n",
        "  ax.set_title ('Object #{}\\nSize: {}'.format(label_idx, cell.shape))\n",
        "  #'Label #{}\\nSize: {}'.format(ii+1, cell.shape)\n",
        "  ax.axis('off')\n",
        "\n",
        "#plt.subplots_adjust(wspace=0.02, hspace=0.15, top=1, bottom=0, left=0, right=1)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2i9CvYzu5JT"
      },
      "source": [
        "### Edge Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu7GScJSu3q-"
      },
      "source": [
        "sobel = filters.sobel(im_gray)\n",
        "plt.imshow(sobel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyFlrde1vj1J"
      },
      "source": [
        "### Gaussian Blurring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwrUoer3vik8"
      },
      "source": [
        "blurred = filters.gaussian(sobel, sigma=2.0)\n",
        "plt.imshow(blurred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liLQx3AFqA3r"
      },
      "source": [
        "### This is an example from the scipy tutorials. To extract the largest object. Need to understand it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xPpQwb7JK10"
      },
      "source": [
        "\n",
        "np.random.seed(1)\n",
        "n = 10\n",
        "l = 256\n",
        "im = np.zeros((l, l))\n",
        "points = l*np.random.random((2, n**2))\n",
        "im[(points[0]).astype(np.int), (points[1]).astype(np.int)] = 1\n",
        "im = ndimage.gaussian_filter(im, sigma=l/(4.*n))\n",
        "\n",
        "mask = im > im.mean()\n",
        "\n",
        "label_im, nb_labels = ndimage.label(mask)\n",
        "\n",
        "# Find the largest connected component\n",
        "sizes = ndimage.sum(mask, label_im, range(nb_labels + 1))\n",
        "mask_size = sizes < 1000\n",
        "remove_pixel = mask_size[label_im]\n",
        "label_im[remove_pixel] = 0\n",
        "labels = np.unique(label_im)\n",
        "label_im = np.searchsorted(labels, label_im)\n",
        "\n",
        "# Now that we have only one connected component, extract it's bounding box\n",
        "slice_x, slice_y = ndimage.find_objects(label_im==4)[0]\n",
        "roi = im[slice_x, slice_y]\n",
        "\n",
        "# Original image\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.imshow(roi)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHmMHJcws9EU"
      },
      "source": [
        "## imageio library experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K67lf2lGs_-X"
      },
      "source": [
        "import imageio\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Read an image\n",
        "#------------------------------------------------------\n",
        "def read_image (file):\n",
        "  im = imageio.imread(str(file))\n",
        "  return (im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grttVe4vtLVU"
      },
      "source": [
        "## OpenCV library experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynj3sfGowkEG"
      },
      "source": [
        "import cv2\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Read an image\n",
        "# The image is read in the BGR colorspace. We have a third dimension as every \n",
        "# pixel is represented by it's B, G and R components. This is the default \n",
        "# colorspace in which images are read in OpenCV.\n",
        "#------------------------------------------------------\n",
        "def read_image (file):\n",
        "  im = cv2.imread(str(file))\n",
        "  return (im)\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Convert an image to greyscale\n",
        "# We reduced a dimension when we transformed from the BGR colorspace to grayscale. \n",
        "# This is because grayscale is a range of monochromatic shades from black to white. \n",
        "# Therefore, a grayscale image contains only shades of gray and no color (i.e it \n",
        "# primarily contains only black and white). Transforming the colorspace removes \n",
        "# all color information, leaving only the luminance of each pixel. Since digital \n",
        "# images are displayed using a combination of red, green, and blue (RGB) colors, \n",
        "# each pixel has three separate luminance values. Therefore, these three values \n",
        "# must be combined into a single value when removing color from an image. Luminance \n",
        "# can also be described as brightness or intensity, which can be measured on a \n",
        "# scale from black (zero intensity) to white (full intensity)\n",
        "#------------------------------------------------------\n",
        "def greyscale (im):\n",
        "  im_grey = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
        "  return (im_grey)\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Edge Detection using Sobel Filter\n",
        "#------------------------------------------------------\n",
        "def edge_detect_sobel (im_grey):\n",
        "  #cv2.Sobel arguments - the image, output depth, order of derivative of x, order of derivative of y, kernel/filter matrix size\n",
        "  sobelx = cv2.Sobel(im_grey,int(cv2.CV_64F),1,0,ksize=3) #ksize=3 means we'll be using the 3x3 Sobel filter\n",
        "  sobely = cv2.Sobel(im_grey,int(cv2.CV_64F),0,1,ksize=3)\n",
        "  sobel = np.sqrt(np.square(sobelx) + np.square(sobely))\n",
        "  return (sobelx, sobely, sobel)\n",
        "\n",
        "#------------------------------------------------------\n",
        "# Edge Detection using Canny\n",
        "#------------------------------------------------------\n",
        "def edge_detect_canny (im_grey):\n",
        "  canny = cv2.Canny(im_grey,100, 200)\n",
        "  return (canny)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBXWZCoExbWu"
      },
      "source": [
        "dog_ocv_img = cv2.cvtColor(read_image(dog_path), cv2.COLOR_BGR2RGB)\n",
        "sc = ShowImg\n",
        "sc.show_grid([dog_ocv_img], ['open'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yg1s4mdvxQDX"
      },
      "source": [
        "**Resampling Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5IZnD8PBI5c"
      },
      "source": [
        "#INTER_NEAREST – a nearest-neighbor interpolation \n",
        "#INTER_LINEAR – a bilinear interpolation (used by default) \n",
        "#INTER_AREA – resampling using pixel area relation. \n",
        "#INTER_CUBIC – a bicubic interpolation over 4×4 pixel neighborhood \n",
        "#INTER_LANCZOS4 – a Lanczos interpolation over 8×8 pixel neighborhood\n",
        "\n",
        "ex_img = dog_ocv_img\n",
        "resample_types = [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4]\n",
        "resampled_labels = ['Original'] + ['NEAREST', 'LINEAR', 'AREA', 'CUBIC', 'LANCZOS4']\n",
        "resampled_imgs = [ex_img] + [cv2.resize(ex_img, (224,224), interpolation=resample) for resample in resample_types]\n",
        "\n",
        "sc = ShowImg\n",
        "sc.show_grid(resampled_imgs, resampled_labels, num_cols=6, figsize=(30, 15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsU9v5FJx_Bt"
      },
      "source": [
        "### Converting to greyscale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOyU_hA5wvMM"
      },
      "source": [
        "# Read the image into an array and print the image dimensions\n",
        "# It has 3 dimensions, where the 3rd dimension is the RGB channel\n",
        "im = read_image (girl_path)\n",
        "print('Original image shape: {}, Datatype: {}'.format(im.shape, im.dtype))\n",
        "\n",
        "# Convert it to a grey scale. It now has only 2 dimensions\n",
        "im_gray = greyscale (im)\n",
        "print('New image shape: {}'.format(im_gray.shape))\n",
        "\n",
        "# Plot the original and greyscale images\n",
        "plot_image_compare (im, im_gray, None, 'gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ykSPsTe1Nh3"
      },
      "source": [
        "### Edge Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcnvHEpp1Pvg"
      },
      "source": [
        "sobelx, sobely, sobel = edge_detect_sobel (im_gray)\n",
        "canny = edge_detect_canny (im_gray)\n",
        "\n",
        "# Create figure and axes\n",
        "fig, axes = plt.subplots(1, 5, figsize=(16,5))\n",
        "\n",
        "# ------- Plot 1 - Original image --------\n",
        "ax = axes[0]\n",
        "ax.imshow(im_gray, cmap='gray')\n",
        "ax.set_title('Original Image')\n",
        "ax.axis('off')\n",
        "\n",
        "# ------- Plot 2 - Sobel X ------- \n",
        "ax = axes[1]\n",
        "ax.imshow(sobelx,cmap='gray')\n",
        "ax.set_title('Sobel X (vertical edges)')\n",
        "ax.axis('off')\n",
        "\n",
        "# ------- Plot 3 - Sobel Y ------- \n",
        "ax = axes[2]\n",
        "ax.imshow(sobely,cmap='gray')\n",
        "ax.set_title('Sobel Y (horizontal edges)')\n",
        "ax.axis('off')\n",
        "\n",
        "# ------- Plot 4 - Combined Sobel Filter -------\n",
        "ax = axes[3]\n",
        "ax.imshow(sobel,cmap='gray')\n",
        "ax.set_title('Sobel Filter')\n",
        "ax.axis('off')\n",
        "\n",
        "# ------- Plot 5 - Canny Edge Detection -------\n",
        "ax = axes[4]\n",
        "ax.imshow(canny,cmap='gray')\n",
        "ax.set_title('Canny')\n",
        "ax.axis('off')\n",
        "\n",
        "plt.subplots_adjust(wspace=0.02, hspace=0.02, top=1, bottom=0, left=0, right=1)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n8HEjSktOYX"
      },
      "source": [
        "## Pillow Experiments (Python Image Library, formerly PIL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxiy5uJFLn-e"
      },
      "source": [
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "dog_pil_img = Image.open(dog_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et2Wd3WnetSU"
      },
      "source": [
        "**Dihedral Transforms Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2yGWl2Mygg1"
      },
      "source": [
        "ex_img = dog_pil_img\n",
        "transposed_imgs = [ex_img] + [ex_img.transpose(i) for i in range(7)]\n",
        "transposed_labels = ['Original'] + ['FLIP_LEFT_RIGHT', 'FLIP_TOP_BOTTOM', 'ROTATE_90', 'ROTATE_180', 'ROTATE_270', 'TRANSPOSE', 'TRANSVERSE']\n",
        "\n",
        "sc = ShowImg\n",
        "sc.show_grid(transposed_imgs, transposed_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SfTfaXuej0B"
      },
      "source": [
        "**Resampling Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klEmSKKC0yjH"
      },
      "source": [
        "ex_img = dog_pil_img\n",
        "resample_types = [Image.ANTIALIAS, Image.NEAREST, Image.BICUBIC, Image.BILINEAR]\n",
        "resampled_labels = ['Original'] + ['ANTIALIAS', 'NEAREST', 'BICUBIC', 'BILINEAR']\n",
        "resampled_imgs = [ex_img] + [ex_img.resize((224,224), resample=resample) for resample in resample_types]\n",
        "\n",
        "sc = ShowImg\n",
        "sc.show_grid(resampled_imgs, resampled_labels, num_cols=5, figsize=(25, 15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCL1bJAAoMoS"
      },
      "source": [
        "## Image Augmentation (with PIL and OpenCV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Dt6Map-z23s"
      },
      "source": [
        "### PIL Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFEv4Msqzg0h"
      },
      "source": [
        "#export\n",
        "\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "class PilImg():\n",
        "  #------------------------------------------------------\n",
        "  # Get (width, height) of the image\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def pil_shape(img):\n",
        "    w, h = img.size\n",
        "    return w, h\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Load the file as an image\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def pil_open (file):\n",
        "    img = PIL.Image.open(file)\n",
        "    return (img)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Convert image to RGB format\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def pil_rgb (img):\n",
        "    img_rgb = img.convert('RGB')\n",
        "    return (img_rgb)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Resize image\n",
        "  # PIL.Image.NEAREST (use nearest neighbour), PIL.Image.BILINEAR (linear interpolation), or PIL.Image.BICUBIC (cubic spline interpolation)\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def pil_resize(img, newsz, resample=PIL.Image.BILINEAR):\n",
        "    assert(isinstance(newsz, tuple) and (len(newsz) == 2))\n",
        "    assert(resample in [PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC])\n",
        "    img_rsz = img.resize(newsz, resample=resample) \n",
        "    return (img_rsz)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # List of Flip-Rotate transforms\n",
        "  #------------------------------------------------------\n",
        "  pil_fr_types = {\n",
        "      'rotate_90': PIL.Image.ROTATE_90,\n",
        "      'rotate_180': PIL.Image.ROTATE_180,\n",
        "      'rotate_270': PIL.Image.ROTATE_270,\n",
        "      'fliv_h': PIL.Image.FLIP_LEFT_RIGHT,\n",
        "      'fliv_v': PIL.Image.FLIP_TOP_BOTTOM,\n",
        "      'transpose': PIL.Image.TRANSPOSE,\n",
        "      'transverse': PIL.Image.TRANSVERSE\n",
        "  }\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Apply Flip-Rotate transform\n",
        "  #------------------------------------------------------\n",
        "  @classmethod\n",
        "  def pil_flip_rotate(cls, img, fr_type):\n",
        "    assert (fr_type in ['rotate_90', 'rotate_180', 'rotate_270', 'fliv_h', 'fliv_v', 'transpose', 'transverse'])\n",
        "    fr_code = cls.pil_fr_types[fr_type]\n",
        "    fr_img = img.transpose(fr_code)\n",
        "\n",
        "    return (fr_img)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Crop image\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def pil_crop(img, newsz, crop_corners, resample=PIL.Image.BILINEAR):\n",
        "    assert(isinstance(newsz, tuple) and (len(newsz) == 2))\n",
        "    assert(isinstance(crop_corners, tuple) and (len(crop_corners) == 4))\n",
        "    assert(resample in [PIL.Image.NEAREST, PIL.Image.BILINEAR, PIL.Image.BICUBIC])\n",
        "    img_crop = img.transform(newsz, PIL.Image.EXTENT, crop_corners, resample=resample)\n",
        "    return (img_crop)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Perspective transform\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def pil_perspective(img, newsz, matrix, resample=PIL.Image.BILINEAR):\n",
        "    assert(isinstance(newsz, tuple) and (len(newsz) == 2))\n",
        "    pers_img = img.transform(newsz, PIL.Image.PERSPECTIVE, list(matrix), resample=resample)\n",
        "\n",
        "    return (pers_img)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Convert image to a tensor of bytes\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def pil_byte_tensor(img):\n",
        "    # PIL .tobytes() converts the image object to raw data, one byte per pixel\n",
        "    res = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes()))\n",
        "    w,h = img.size\n",
        "    return res.view(h,w,-1).permute(2,0,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGmM4m8zJRAw"
      },
      "source": [
        "pil_cls = PilImg\n",
        "open_img = pil_cls.pil_open(dog_path)\n",
        "rgb_img = pil_cls.pil_rgb(open_img)\n",
        "rsz_img = pil_cls.pil_resize(rgb_img, (128, 128))\n",
        "fr_img = pil_cls.pil_flip_rotate(rsz_img, 'rotate_180')\n",
        "\n",
        "sc = ShowImg\n",
        "sc.show_grid([open_img, rgb_img, rsz_img, fr_img], ['open', 'rgb', 'resize', 'flip_rotate'])\n",
        "pil_cls.pil_shape(open_img), pil_cls.pil_shape(rgb_img), pil_cls.pil_shape(rsz_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQh4Iucv0yDz"
      },
      "source": [
        "### OpenCV Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_7lJmKmxtGw"
      },
      "source": [
        "#export\n",
        "\n",
        "import cv2\n",
        "import imageio\n",
        "import torch\n",
        "from torch import tensor\n",
        "\n",
        "class OcvImg():\n",
        "  #------------------------------------------------------\n",
        "  # Get (width, height) of the image\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def ocv_shape(img):\n",
        "    h, w, d = img.shape\n",
        "    return w, h\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Load the file as an image\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def ocv_open (file):\n",
        "    if (str(file)[-4:] == '.gif'):\n",
        "      # OpenCV doesn't support .gif codec. So read the file into a numpy array \n",
        "      # with ImageIO and convert from RGB to BGR, so that it is in OpenCV format\n",
        "      img = imageio.imread(file)\n",
        "      img = img.astype('float32') \n",
        "      #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "      return (img)\n",
        "\n",
        "    img = cv2.imread(str(file))\n",
        "    return (img)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Convert image to RGB format\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def ocv_rgb (img):\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    return (img_rgb)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Resize image\n",
        "  # To shrink an image, it will generally look best with INTER_AREA interpolation, \n",
        "  # whereas to enlarge an image, it will generally look best with INTER_CUBIC (slow) \n",
        "  # or INTER_LINEAR (faster but still looks OK)\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def ocv_resize(img, new_sz, interpolation=cv2.INTER_LINEAR):\n",
        "    assert(isinstance(new_sz, tuple) and (len(new_sz) == 2))\n",
        "    img_rsz = cv2.resize(img, new_sz, interpolation=interpolation) \n",
        "    return (img_rsz)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # List of Flip-Rotate transform\n",
        "  #------------------------------------------------------\n",
        "  ocv_fr_types = {\n",
        "      'rotate_90': [(cv2.rotate, cv2.ROTATE_90_CLOCKWISE)],\n",
        "      'rotate_180': [(cv2.rotate, cv2.ROTATE_180)],\n",
        "      'rotate_270': [(cv2.rotate, cv2.ROTATE_90_COUNTERCLOCKWISE)],\n",
        "      'fliv_h': [(cv2.flip, 1)],\n",
        "      'fliv_v': [(cv2.flip, 0)],\n",
        "      'transpose': [(cv2.rotate, cv2.ROTATE_90_CLOCKWISE), (cv2.flip, 1)],\n",
        "      'transverse': [(cv2.rotate, cv2.ROTATE_90_COUNTERCLOCKWISE), (cv2.flip, 1)]\n",
        "  }\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Apply Flip-Rotate transform\n",
        "  #------------------------------------------------------\n",
        "  @classmethod\n",
        "  def ocv_flip_rotate(cls, img, fr_type):\n",
        "    assert (fr_type in ['rotate_90', 'rotate_180', 'rotate_270', 'fliv_h', 'fliv_v', 'transpose', 'transverse'])\n",
        "    for fr_fn, fr_code in cls.ocv_fr_types[fr_type]:\n",
        "      img = fr_fn(img, fr_code)\n",
        "\n",
        "    return (img)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Crop image\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def ocv_crop(img, new_sz, crop_corners, interpolation=cv2.INTER_LINEAR):\n",
        "    assert(isinstance(new_sz, tuple) and (len(new_sz) == 2))\n",
        "    assert(isinstance(crop_corners, tuple) and (len(crop_corners) == 4))\n",
        "    assert(interpolation in [cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LANCZOS4])\n",
        "    tl_x, tl_y, br_x, br_y = crop_corners\n",
        "    img_crop = img[tl_y:br_y, tl_x:br_x]\n",
        "    img_crop = cv2.resize(img_crop, new_sz, interpolation=interpolation)\n",
        "    return (img_crop)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Perspective transform\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def ocv_perspective(img, new_sz, src_coords, targ_coords, interpolation=cv2.INTER_LINEAR):\n",
        "    assert(isinstance(new_sz, tuple) and (len(new_sz) == 2))\n",
        "    matrix = cv2.getPerspectiveTransform(np.float32(src_coords), np.float32(targ_coords))\n",
        "    # Switch source and target for OpenCV to get the same matrix coefficients as for PIL\n",
        "    #cv2d_flip = cv2.getPerspectiveTransform(np.float32(targ_d), np.float32(src_d))\n",
        "    pers_img = cv2.warpPerspective(img, matrix, new_sz)\n",
        "\n",
        "    return (pers_img)\n",
        "\n",
        "  #------------------------------------------------------\n",
        "  # Convert image to a tensor of bytes\n",
        "  #------------------------------------------------------\n",
        "  @staticmethod\n",
        "  def ocv_byte_tensor(img):\n",
        "    img_bytes = torch.from_numpy(img.transpose((2, 0, 1)))\n",
        "    return (img_bytes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b68MdO78moN"
      },
      "source": [
        "ocv_cls = OcvImg\n",
        "open_img = ocv_cls.ocv_open(dog_path)\n",
        "rgb_img = ocv_cls.ocv_rgb(open_img)\n",
        "rsz_img = ocv_cls.ocv_resize(rgb_img, (128, 128))\n",
        "fr_img = ocv_cls.ocv_flip_rotate(rsz_img, 'rotate_180')\n",
        "\n",
        "sc = ShowImg\n",
        "sc.show_grid([open_img, rgb_img, rsz_img, fr_img], ['open', 'rgb', 'resize', 'flip_rotate'])\n",
        "ocv_cls.ocv_shape(open_img), ocv_cls.ocv_shape(rgb_img), ocv_cls.ocv_shape(rsz_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xHa7RNyUVn7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2wA2vhhxQDU"
      },
      "source": [
        "**Dihedral Transforms Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz0xqPrvLl-c"
      },
      "source": [
        "ex_img = rgb_img\n",
        "ocv_cls = OcvImg\n",
        "transposed_imgs = [ex_img] + [ocv_cls.ocv_flip_rotate(ex_img, fr_type) for fr_type in ocv_cls.ocv_fr_types.keys()]\n",
        "transposed_labels = ['Original'] + [fr_type for fr_type in ocv_cls.ocv_fr_types.keys()]\n",
        "\n",
        "sc = ShowImg\n",
        "sc.show_grid(transposed_imgs, transposed_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQpVs0wL0BAc"
      },
      "source": [
        "### Crop and Warp Calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ld8cXideoeF"
      },
      "source": [
        "**Resized Crop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbKt3jeVO0T_"
      },
      "source": [
        "#export\n",
        "\n",
        "# Take a crop from the original image and then resize that crop to a given 'final_sz', as \n",
        "# all final images should be the same size\n",
        "def imgaug_random_resized_crop(orig_sz, crop_area_range=(0.08, 1.0), crop_aspect_range=(3./4., 4./3.), crop_pos='random'):\n",
        "\n",
        "  # We are given three parameters to calculate the position and size of the cropped image\n",
        "  #   1. The area of the crop as a percentage of the original image\n",
        "  #         We are given a range of percentages and pick one at random\n",
        "  #   2. The aspect ratio of the crop\n",
        "  #         We are given a range of aspect ratios and pick one at random\n",
        "  #         If no range is given, we take the aspect ratio of the original image\n",
        "  #   3. The position of the crop within the original image\n",
        "  #         We either centre the crop within the original image, or find a position at\n",
        "  #         random\n",
        "  assert(isinstance(crop_area_range, tuple) and (len(crop_area_range) == 2))\n",
        "  assert((crop_area_range[0] > 0) and (crop_area_range[0] <= crop_area_range[1]) and (crop_area_range[1] <= 1.))\n",
        "  assert(crop_aspect_range is None or \n",
        "         (isinstance(crop_aspect_range, tuple) and (len(crop_aspect_range) == 2)))\n",
        "  assert(crop_aspect_range is None or \n",
        "         ((crop_aspect_range[0] > 0.5) and (crop_aspect_range[0] < crop_aspect_range[1]) and (crop_aspect_range[1] < 1.5)))\n",
        "  assert(crop_pos in ['random', 'ctr'])\n",
        "\n",
        "  orig_w, orig_h = orig_sz\n",
        "  orig_area = orig_w * orig_h\n",
        "  min_crop_area_pct, max_crop_area_pct = crop_area_range\n",
        "  if (crop_aspect_range is not None):\n",
        "    min_crop_aspect, max_crop_aspect = crop_aspect_range\n",
        "  else:\n",
        "    orig_aspect_ratio = orig_w / orig_h\n",
        "\n",
        "  # Try 'num_attempts' times to get a proper crop inside the image\n",
        "  num_attempts = 10\n",
        "  for i in range(num_attempts):\n",
        "\n",
        "    # Get the crop area as a random percentage of the original image area\n",
        "    crop_area = random.uniform(min_crop_area_pct, max_crop_area_pct) * orig_area\n",
        "\n",
        "    # Get the crop aspect ratio\n",
        "    if (crop_aspect_range is not None):\n",
        "      # Get a random crop aspect ratio within the given range\n",
        "      crop_aspect = math.exp(random.uniform(math.log(min_crop_aspect), math.log(max_crop_aspect)))\n",
        "    else:\n",
        "      # No range has been provided, so the crop aspect ratio will be the same as the original image\n",
        "      crop_aspect = orig_aspect_ratio\n",
        "\n",
        "    # Calculate crop width and height from the crop area and aspect ratio\n",
        "    #   crop_aspect = crop_w / crop_h\n",
        "    #   crop_area = crop_w * crop_h\n",
        "    crop_w = int(round(math.sqrt(crop_area * crop_aspect)))\n",
        "    crop_h = int(round(math.sqrt(crop_area / crop_aspect)))\n",
        "\n",
        "    # If the crop dimensions fit within the original image, get the crop position\n",
        "    if (crop_w <= orig_w and crop_h <= orig_h):\n",
        "\n",
        "      if (crop_pos == 'random'):\n",
        "        # Get a random (left, top) position\n",
        "        crop_pos_l = random.randint(0, orig_w - crop_w)\n",
        "        crop_pos_t  = random.randint(0, orig_h - crop_h)\n",
        "\n",
        "      elif (crop_pos == 'ctr'):\n",
        "        # Get the (left, top) position if the crop is centred\n",
        "        crop_pos_l = (orig_w - crop_w) // 2\n",
        "        crop_pos_t = (orig_h - crop_h) // 2\n",
        "\n",
        "      # Crop (right, bottom) position\n",
        "      crop_pos_r = crop_pos_l + crop_w\n",
        "      crop_pos_b = crop_pos_t + crop_h\n",
        "\n",
        "      # Return coordinates\n",
        "      return (crop_pos_l, crop_pos_t, crop_pos_r, crop_pos_b)\n",
        "\n",
        "  # We weren't able to get an appropriate crop after all our attempts\n",
        "  # So we fallback to squishing\n",
        "  # \n",
        "  # We will only get here if a crop aspect range was given and all the\n",
        "  # random crop aspect ratios that we attempted did not work\n",
        "  assert(crop_aspect_range is not None)\n",
        "  if orig_aspect_ratio < min_crop_aspect:\n",
        "    # Original aspect ratio was lower than minimum ie. image is narrow and tall\n",
        "    crop_aspect = min_crop_aspect\n",
        "    crop_w = orig_w\n",
        "    crop_h = int(crop_w / crop_aspect)\n",
        "\n",
        "  elif orig_aspect_ratio > max_crop_aspect:\n",
        "    # Original aspect ratio was higher than maximum ie. image was wide and short\n",
        "    crop_aspect = max_crop_aspect\n",
        "    crop_h = orig_h\n",
        "    crop_w = int(crop_h * crop_aspect)\n",
        "\n",
        "  else:\n",
        "    crop_aspect = orig_aspect_ratio\n",
        "    crop_w, crop_h = orig_w, orig_h\n",
        "\n",
        "  crop_pos_l = (orig_w - crop_w) // 2\n",
        "  crop_pos_t = (orig_h - crop_h) // 2\n",
        "  crop_pos_r = crop_pos_l + crop_w\n",
        "  crop_pos_b = crop_pos_t + crop_h\n",
        "\n",
        "  return (crop_pos_l, crop_pos_t, crop_pos_r, crop_pos_b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72Tq62EX3VsR"
      },
      "source": [
        "# Does a centre crop of approx 77% of the image area\n",
        "ex_img = dog_pil_img\n",
        "ex_sz = ex_img.size\n",
        "imgaug_random_resized_crop(ex_sz, crop_area_range=(0.7694, 0.7694), crop_aspect_range=None, crop_pos='ctr')\n",
        "\n",
        "random.seed(800)\n",
        "imgaug_random_resized_crop(ex_sz)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0QMZkewey-P"
      },
      "source": [
        "**Perspective Warp**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LIbbRRWe17c"
      },
      "source": [
        "#export\n",
        "from torch import FloatTensor\n",
        "\n",
        "def _find_coeffs(orig_pts, targ_pts):\n",
        "    matrix = []\n",
        "    #The equations we'll need to solve.\n",
        "    for p1, p2 in zip(targ_pts, orig_pts):\n",
        "        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])\n",
        "        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])\n",
        "\n",
        "    A = FloatTensor(matrix)\n",
        "    B = FloatTensor(orig_pts).view(8, 1)\n",
        "    #The 8 scalars we seek are solution of AX = B\n",
        "    return list(torch.solve(B,A)[0][:,0])\n",
        "\n",
        "def _warp(final_sz, src_coords):\n",
        "    w,h = final_sz\n",
        "    targ_coords = ((0,0),(0,h),(w,h),(w,0))\n",
        "    coeffs = _find_coeffs(src_coords,targ_coords)\n",
        "    return coeffs, src_coords, targ_coords\n",
        "\n",
        "def _uniform(a,b): return a + (b-a) * random.random()\n",
        "\n",
        "def _default_crop_size(w,h): return [w,w] if w < h else [h,h]\n",
        "\n",
        "def imgaug_perspective_warp(orig_sz, final_sz, magnitude=0.):\n",
        "  orig_w, orig_h = orig_sz\n",
        "  crop_w, crop_h = _default_crop_size(orig_w, orig_h)\n",
        "\n",
        "  left, top = random.randint(0, orig_w - crop_w), random.randint(0, orig_h - crop_h)\n",
        "  top_magn = min(magnitude, left / crop_w, (orig_w - left) / crop_w - 1)\n",
        "  lr_magn  = min(magnitude, top / crop_h, (orig_h - top) / crop_h - 1)\n",
        "\n",
        "  up_t, lr_t = _uniform(-top_magn, top_magn), _uniform(-lr_magn, lr_magn)\n",
        "  src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]])\n",
        "  src_corners = src_corners * tensor([crop_w, crop_h]).float() + tensor([left, top]).float()\n",
        "  src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners])\n",
        "  \n",
        "  res = _warp(final_sz, src_corners)\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwsIfgBdvf85"
      },
      "source": [
        "ex_img = dog_pil_img\n",
        "ex_sz = ex_img.size\n",
        "imgaug_perspective_warp(ex_sz, (128, 128), magnitude=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOTT_1L2zhlL"
      },
      "source": [
        "### PIL and OpenCV Crop and Warp Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFIwYxLURV0w"
      },
      "source": [
        "#------------------------------------------------------\n",
        "# Load an PIL and OCV image, convert them to byte tensors\n",
        "# and check that the tensors are identical\n",
        "#------------------------------------------------------\n",
        "def load_pil_ocv_img(img_path):\n",
        "  pil_cls = PilImg\n",
        "  pil_img = pil_cls.pil_open(img_path)\n",
        "  pil_img = pil_cls.pil_rgb(pil_img)\n",
        "  pil_tensor = pil_cls.pil_byte_tensor(pil_img)\n",
        "\n",
        "  ocv_cls = OcvImg\n",
        "  ocv_img = ocv_cls.ocv_open(img_path)\n",
        "  ocv_img = ocv_cls.ocv_rgb(ocv_img)\n",
        "  ocv_tensor = ocv_cls.ocv_byte_tensor(ocv_img)\n",
        "\n",
        "  return pil_img, ocv_img, pil_tensor, ocv_tensor\n",
        "\n",
        "pil_img, ocv_img, pil_tensor, ocv_tensor = load_pil_ocv_img(dog_path)\n",
        "\n",
        "pil_tensor.shape, ocv_tensor.shape\n",
        "print ('\\n Equality of PIL and OCV tensor:', torch.all(torch.eq(pil_tensor, ocv_tensor)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXL8kDR1eKIR"
      },
      "source": [
        "**Crop and Warp**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl179hhceOXo"
      },
      "source": [
        "def test_crop(img, img_sz, img_type='ocv'):\n",
        "  assert(img_type in ['ocv', 'pil'])\n",
        "\n",
        "  cropped_imgs, cropped_corners = [], []\n",
        "  new_sz = (128, 128)\n",
        "  base_crop_area=(0.9, 0.9)\n",
        "\n",
        "  num_count = 20\n",
        "  for i in range(num_count):\n",
        "    if (i < num_count // 2):\n",
        "      # Centre crop\n",
        "      crop_scale = base_crop_area[0] - (i * 0.08)\n",
        "      crop_area=(crop_scale, crop_scale)\n",
        "      crop_corners = imgaug_random_resized_crop(img_sz, crop_area_range=crop_area, crop_aspect_range=None, crop_pos='ctr')\n",
        "      cropped_corners.append(f'Ctr, {crop_scale:.2f}, {crop_corners}')\n",
        "    else:\n",
        "      # Random resize crop\n",
        "      crop_corners = imgaug_random_resized_crop(img_sz)\n",
        "      cropped_corners.append(f'Rnd, {crop_corners}')\n",
        "\n",
        "    if (img_type == 'ocv'):\n",
        "      img_crop = OcvImg.ocv_crop(img, new_sz, crop_corners, interpolation=cv2.INTER_LINEAR)\n",
        "    elif (img_type == 'pil'):\n",
        "      img_crop = PilImg.pil_crop(img, new_sz, crop_corners, resample=PIL.Image.BILINEAR)\n",
        "    cropped_imgs.append(img_crop)\n",
        "\n",
        "  sc = ShowImg\n",
        "  sc.show_grid(cropped_imgs, cropped_corners)\n",
        "\n",
        "def test_warp(img, img_sz, img_type='ocv'):\n",
        "  assert(img_type in ['ocv', 'pil'])\n",
        "\n",
        "  warped_imgs, warped_pts = [], []\n",
        "  new_sz = (128, 128)\n",
        "\n",
        "  for i in range(8):\n",
        "    coeffs, src_coords, targ_coords = imgaug_perspective_warp(img_sz, new_sz, magnitude=0.2)\n",
        "    if (img_type == 'ocv'):\n",
        "      img_warp = OcvImg.ocv_perspective(img, new_sz, src_coords, targ_coords, interpolation=cv2.INTER_LINEAR)\n",
        "    elif (img_type == 'pil'):\n",
        "      img_warp = PilImg.pil_perspective(img, new_sz, list(coeffs), resample=PIL.Image.BILINEAR)\n",
        "    warped_imgs.append(img_warp)\n",
        "    warped_pts.append(f'W, {src_coords}')\n",
        "\n",
        "  sc = ShowImg\n",
        "  sc.show_grid(warped_imgs, warped_pts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8SX70t-eWej"
      },
      "source": [
        "**Crop with PIL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC4ngNe5edRT"
      },
      "source": [
        "ex_img = pil_img\n",
        "ex_sz = ex_img.size\n",
        "test_crop(ex_img, ex_sz, img_type='pil')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fyx7wgGAepAJ"
      },
      "source": [
        "**Warp with PIL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jaubc1-TerRL"
      },
      "source": [
        "ex_img = pil_img\n",
        "ex_sz = ex_img.size\n",
        "test_warp(ex_img, ex_sz, img_type='pil')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io2XBOM5eeOz"
      },
      "source": [
        "**Crop with OCV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5bo5me9egce"
      },
      "source": [
        "ex_img = ocv_img\n",
        "ex_sz = OcvImg.ocv_shape(ex_img)\n",
        "test_crop(ex_img, ex_sz, img_type='ocv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxEwuoxjgQh4"
      },
      "source": [
        "**Warp with OCV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LtZnippgSsd"
      },
      "source": [
        "ex_img = ocv_img\n",
        "ex_sz = OcvImg.ocv_shape(ex_img)\n",
        "test_warp(ex_img, ex_sz, img_type='ocv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlAOWV4OFf6O"
      },
      "source": [
        "### Image Augmentation with Albumentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LZEBqgKp961"
      },
      "source": [
        "import imageio\n",
        "\n",
        "def load_example_images(dog_path, dog_mask_path, parrot_path, woodpecker_path):\n",
        "\n",
        "  # load mask for sample dog image\n",
        "  dog_pil_img = Image.open(dog_path)\n",
        "  dog_pil_mask = Image.open(dog_mask_path)\n",
        "\n",
        "  dog_iio_img = imageio.imread(dog_path)\n",
        "  dog_iio_mask = imageio.imread(dog_mask_path)\n",
        "\n",
        "  # Convert Path object to str, as imread() requires\n",
        "  parrot_ocv_img = cv2.imread(str(parrot_path))\n",
        "  parrot_ocv_img = cv2.cvtColor(parrot_ocv_img, cv2.COLOR_BGR2RGB)\n",
        "  parrot_ocv_img = cv2.resize(parrot_ocv_img, (512, 512))\n",
        "\n",
        "  woodpecker_ocv_img = cv2.imread(str(woodpecker_path))\n",
        "  woodpecker_ocv_img = cv2.cvtColor(woodpecker_ocv_img, cv2.COLOR_BGR2RGB)\n",
        "  woodpecker_pcv_img = cv2.resize(woodpecker_ocv_img, (512, 512))\n",
        "\n",
        "  return dog_pil_img, dog_pil_mask, dog_iio_img, dog_iio_mask, parrot_ocv_img, woodpecker_ocv_img\n",
        "\n",
        "\n",
        "dog_pil_img, dog_pil_mask, dog_iio_img, dog_iio_mask, parrot_ocv_img, woodpecker_ocv_img = load_example_images(dog_path, dog_mask_path, parrot_path, woodpecker_path)\n",
        "sc = ShowImg\n",
        "sc.show_grid([dog_pil_img, dog_pil_mask, dog_iio_img, dog_iio_mask, parrot_ocv_img, woodpecker_ocv_img], \n",
        "             ['PIL ' + dog_path.name, 'PIL ' + dog_mask_path.name, 'IIO ' + dog_path.name, 'IIO ' + dog_mask_path.name, 'OCV ' + parrot_path.name, 'OCV ' + woodpecker_path.name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zxQ3NWkFnZV"
      },
      "source": [
        "import matplotlib.patches as patches\n",
        "import albumentations as A\n",
        "\n",
        "def show_dog_with_mask (dog_img, dog_mask):\n",
        "  fig, axs = plt.subplots(1, 4, figsize=(20,5))\n",
        "\n",
        "  #plot the original data\n",
        "  axs[0].imshow(dog_img) \n",
        "  axs[0].axis('off')\n",
        "  axs[0].set_title('Image')\n",
        "\n",
        "  #plot the mask\n",
        "  axs[1].imshow(dog_mask)\n",
        "  axs[1].axis('off')   \n",
        "  axs[1].set_title('Mask')\n",
        "    \n",
        "  #plot image and add the mask\n",
        "  axs[2].imshow(dog_img)\n",
        "  axs[2].imshow(dog_mask, alpha = 0.5, cmap = \"Reds\")\n",
        "  axs[2].axis('off')   \n",
        "  axs[2].set_title('Image with mask overlay')\n",
        "\n",
        "  # Display the image\n",
        "  axs[3].imshow(dog_img)\n",
        "  # Create a Rectangle patch\n",
        "  rect = patches.Rectangle((0,1),236,128,linewidth=1,edgecolor='r',facecolor='none')\n",
        "  # Add the patch to the Axes\n",
        "  axs[3].add_patch(rect)\n",
        "  axs[3].axis('off') # disable axis\n",
        "\n",
        "  # set suptitle\n",
        "  plt.suptitle('Image with mask')\n",
        "  plt.show()\n",
        "\n",
        "show_dog_with_mask (dog_pil_img, dog_pil_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CA7MH9HROI_D"
      },
      "source": [
        "def gallery(array, ncols=3):\n",
        "    '''\n",
        "    Function to arange images into a grid.\n",
        "    INPUT:\n",
        "        array - numpy array containing images\n",
        "        ncols - number of columns in resulting imahe grid\n",
        "    OUTPUT:\n",
        "        result - reshaped array into a grid with given number of columns\n",
        "    '''\n",
        "    nindex, height, width, intensity = array.shape\n",
        "    nrows = nindex//ncols\n",
        "    assert nindex == nrows*ncols\n",
        "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols, intensity))\n",
        "    return result\n",
        "\n",
        "def albu_aug (image):\n",
        "  # initialize augmentations\n",
        "  gaus_noise = A.GaussNoise() # gaussian noise\n",
        "  elastic = A.ElasticTransform() # elastic transform\n",
        "  bright_contrast = A.RandomBrightnessContrast(p=1) # random brightness and contrast\n",
        "  gamma = A.RandomGamma(p=1) # random gamma\n",
        "  clahe = A.CLAHE(p=1) # CLAHE (see https://en.wikipedia.org/wiki/Adaptive_histogram_equalization#Contrast_Limited_AHE)\n",
        "  blur = A.Blur()\n",
        "\n",
        "  # apply augmentations\n",
        "  # pass image to the augmentation\n",
        "  img_gaus = gaus_noise(image = image)\n",
        "  img_elastic = elastic(image = image)\n",
        "  img_bc = bright_contrast(image = image)\n",
        "  img_gamma = gamma(image = image)\n",
        "  img_clahe = clahe(image = image)\n",
        "  img_blur = blur(image = image)\n",
        "\n",
        "  # access the augmented image by 'image' key\n",
        "  img_list = [img_gaus['image'], img_elastic['image'], img_bc['image'], img_gamma['image'], img_clahe['image'], img_blur['image']]\n",
        "\n",
        "  # visualize the augmented images\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(gallery(np.array(img_list), ncols = 3))\n",
        "  plt.title('Augmentation examples')\n",
        "\n",
        "albu_aug(dog_iio_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neq96z4qSHPF"
      },
      "source": [
        "def albu_aug_with_mask (image, mask):\n",
        "  # compose augmentation pipeline\n",
        "  aug_pipeline = A.Compose([\n",
        "      A.ShiftScaleRotate(p = 1),\n",
        "      A.RGBShift(),\n",
        "      A.Blur(p = 1),\n",
        "      A.GaussNoise(p = 1)\n",
        "  ],p=1)\n",
        "\n",
        "  # apply augmentations to image and a mask\n",
        "  augmented = aug_pipeline(image = image, mask = mask)\n",
        "\n",
        "  # visualize augmented image and mask\n",
        "  fig, ax = plt.subplots(1,4, figsize = (15, 10))\n",
        "\n",
        "  ax[0].axis('off')\n",
        "  ax[0].imshow(image)\n",
        "  ax[0].set_title('original image')\n",
        "\n",
        "  ax[1].axis('off')\n",
        "  ax[1].imshow(augmented['image'])\n",
        "  ax[1].set_title('augmented image')\n",
        "\n",
        "  ax[2].axis('off')\n",
        "  ax[2].imshow(augmented['image'])\n",
        "  ax[2].imshow(augmented['mask'].squeeze(), alpha = 0.5, cmap = \"Reds\")\n",
        "  ax[2].set_title('augmented image with mask')\n",
        "\n",
        "  ax[3].axis('off')\n",
        "  ax[3].imshow(augmented['mask'].squeeze(), alpha = 1.0, cmap = \"Reds\")\n",
        "  ax[3].set_title('augmented mask')\n",
        "\n",
        "albu_aug_with_mask (dog_iio_img, dog_iio_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VH0h2lxxS0ML"
      },
      "source": [
        "# create bounding boxes from mask with cv2\n",
        "import cv2\n",
        "\n",
        "def albu_aug_with_bbox(image, mask):\n",
        "  mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "  bboxes = cv2.boundingRect(cv2.findNonZero(mask))\n",
        "\n",
        "  # compose augmentation pipeline\n",
        "  aug_pipeline = A.Compose([\n",
        "      A.ShiftScaleRotate(rotate_limit=0, p = 1),\n",
        "      A.RGBShift(p = 1),\n",
        "      A.Blur(p = 1),\n",
        "      A.GaussNoise(p = 1)\n",
        "  ],p=1)\n",
        "\n",
        "  # augment image and bounding box\n",
        "  augmented_boxes = aug_pipeline(image = image, bboxes = [bboxes])\n",
        "  box_aug = augmented_boxes['bboxes'][0]\n",
        "\n",
        "  # visualize augmented image and bbox\n",
        "  fig, ax = plt.subplots(1,2, figsize = (15, 10))\n",
        "\n",
        "  ax[0].axis('off')\n",
        "  ax[0].imshow(image)\n",
        "  rect = patches.Rectangle((bboxes[0],bboxes[1]),bboxes[2],bboxes[3],linewidth=1,edgecolor='r',facecolor='none')\n",
        "  ax[0].add_patch(rect)\n",
        "  ax[0].set_title('original image')\n",
        "\n",
        "  ax[1].axis('off')\n",
        "  ax[1].imshow(augmented_boxes['image'])\n",
        "  rect = patches.Rectangle((box_aug[0],box_aug[1]),box_aug[2],box_aug[3],linewidth=1,edgecolor='r',facecolor='none')\n",
        "  ax[1].add_patch(rect)\n",
        "  ax[1].set_title('augmented image')\n",
        "\n",
        "albu_aug_with_bbox(dog_iio_img, dog_iio_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU3tbvfuTdUZ"
      },
      "source": [
        "def albu_aug_pipeline(image):\n",
        "  # compose complex augmentation pipeline\n",
        "  augmentation_pipeline = A.Compose(\n",
        "      [\n",
        "          A.HorizontalFlip(p = 0.5), # apply horizontal flip to 50% of images\n",
        "          A.OneOf(\n",
        "              [\n",
        "                  # apply one of transforms to 50% of images\n",
        "                  A.RandomContrast(), # apply random contrast\n",
        "                  A.RandomGamma(), # apply random gamma\n",
        "                  A.RandomBrightness(), # apply random brightness\n",
        "              ],\n",
        "              p = 0.5\n",
        "          ),\n",
        "          A.OneOf(\n",
        "              [\n",
        "                  # apply one of transforms to 50% images\n",
        "                  A.ElasticTransform(\n",
        "                      alpha = 120,\n",
        "                      sigma = 120 * 0.05,\n",
        "                      alpha_affine = 120 * 0.03\n",
        "                  ),\n",
        "                  A.GridDistortion(),\n",
        "                  A.OpticalDistortion(\n",
        "                      distort_limit = 2,\n",
        "                      shift_limit = 0.5\n",
        "                  ),\n",
        "              ],\n",
        "              p = 0.5\n",
        "          )\n",
        "      ],\n",
        "      p = 1\n",
        "  )\n",
        "\n",
        "  # apply pipeline to sample image\n",
        "  images_aug = np.array([augmentation_pipeline(image = image)['image'] for _ in range(16)])\n",
        "\n",
        "  # visualize augmentation results\n",
        "  plt.figure(figsize=(10,10))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(gallery(images_aug, ncols = 4))\n",
        "  plt.title('Augmentation pipeline examples')\n",
        "\n",
        "albu_aug_pipeline(dog_iio_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm4zW-eCid6s"
      },
      "source": [
        "#export\n",
        "import albumentations as A\n",
        "\n",
        "albu_augs = {'Vertical Flip': A.VerticalFlip(p=0.5),\n",
        "             'Horizontal Flip': A.HorizontalFlip(p=0.5),\n",
        "             'Flip': A.Flip(p=0.5),\n",
        "             'Random Rotate': A.RandomRotate90(p=0.5),\n",
        "             'Rotate': A.Rotate(limit=286, p=0.5),\n",
        "             'Transpose': A.Transpose(p=0.5),\n",
        "             'Shift Scale Rotate': A.ShiftScaleRotate(shift_limit=0.8, scale_limit=1.4, rotate_limit=360, p=0.5),\n",
        "             'Center Crop': A.CenterCrop(height=134, width=94, p=0.5),\n",
        "             'Random Brightness': A.RandomBrightness(limit=1.3, p=0.5),\n",
        "             'Random Brightness Contrast': A.RandomBrightnessContrast(p=0.5),\n",
        "             'Random Gamma': A.RandomGamma(p=0.5),\n",
        "             'Clahe': A.CLAHE(p=0.5),\n",
        "             'Hue Saturation Value': A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=50, val_shift_limit=50, p=0.5),\n",
        "             'RGB Shift': A.RGBShift(r_shift_limit=105, g_shift_limit=45, b_shift_limit=40, p=0.5),\n",
        "             'Channel Shuffle': A.ChannelShuffle(p=0.5),\n",
        "             'Jpeg Compression': A.JpegCompression(quality_lower=7, quality_upper=100, p=0.5),\n",
        "             'Random Contrast': A.RandomContrast(limit=0.9, p=0.5),\n",
        "             'Blur': A.Blur(blur_limit=17, p=0.5),\n",
        "             'Gauss Noise': A.GaussNoise(var_limit=(10.0, 80.0), p=0.5),\n",
        "             'Invert Image': A.InvertImg(),\n",
        "}\n",
        "\n",
        "def WAIT_imgaug_albu(img, aug_name, p=0.5, **kwargs):\n",
        "  aug = albu_augs[aug_name]\n",
        "  aug.p = p\n",
        "  augmented = aug(image=img)\n",
        "  return augmented['image']\n",
        "\n",
        "def imgaug_albu(img, aug_name, mask=None, p=0.5, **kwargs):\n",
        "  aug = albu_augs[aug_name]\n",
        "  aug.p = p\n",
        "  augmented = aug(image=img, mask=mask)\n",
        "  return augmented['image'], augmented['mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsLZ0Ryzr6vh"
      },
      "source": [
        "aug_names = list(albu_augs.keys())\n",
        "aug_labels = ['Original'] + aug_names\n",
        "aug_imgs = [woodpecker_ocv_img] + [imgaug_albu(woodpecker_ocv_img, aug_name, p=1.0)[0] for aug_name in aug_names]\n",
        "\n",
        "sc = ShowImg\n",
        "sc.show_grid(aug_imgs, aug_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6DP84KSRCB_"
      },
      "source": [
        "aug_names = list(albu_augs.keys())\n",
        "aug_labels = ['Original'] + aug_names\n",
        "aug_imgs = [(dog_iio_img, dog_iio_mask)] + [imgaug_albu_mask(dog_iio_img, dog_iio_mask, aug_name, p=1.0) for aug_name in aug_names]\n",
        "\n",
        "foo_imgs, foo_masks = zip(*aug_imgs)\n",
        "sc = ShowImg\n",
        "sc.show_grid(foo_imgs, foo_masks, y_method=sc.show_mask)\n",
        "sc.show_grid(foo_imgs, aug_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiBaeYR7zixG"
      },
      "source": [
        "### (TODO - convert to OpenCV) Segmenting into Foreground and Background "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj4_D_LDzhj8"
      },
      "source": [
        "# Segment the image into foreground and background\n",
        "# TODO - Replace with OpenCV equivalent\n",
        "# mask, thresh_val = segment_foreback (im_gray)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "\n",
        "# Plot the mask\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(mask, cmap='gray', interpolation='nearest')\n",
        "plt.title('Mask')\n",
        "plt.axis('off')\n",
        "\n",
        "# Create a \"transparent\" mask by blanking out all the white (ie. 0) pixels in the\n",
        "# initial mask. Superimpose this mask in a different colour on the original image\n",
        "plt.subplot(1,2,2)\n",
        "mask_for_display = np.where(mask, mask, np.nan)\n",
        "plt.imshow(im_gray, cmap='gray')\n",
        "plt.imshow(mask_for_display, cmap='rainbow', alpha=0.5)\n",
        "plt.axis('off')\n",
        "plt.title('Image w/ Transparent Mask')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ffsj1-tkzDTB"
      },
      "source": [
        "#Distribution of the intensity values of all the pixels\n",
        "# TODO - make this into a function\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "sns.distplot(im_gray.flatten(),kde=False)#This is to flatten the matrix and put the intensity values of all the pixels in one single row vector\n",
        "plt.title('Distribution of intensity values')\n",
        "\n",
        "#To zoom in on the distribution and see if there is more than one prominent peak \n",
        "plt.subplot(1,2,2)\n",
        "sns.distplot(im_gray.flatten(),kde=False) \n",
        "plt.ylim(0,30000) \n",
        "plt.title('Distribution of intensity values (Zoomed In)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67GwyU4Xe0L"
      },
      "source": [
        "### Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPRG233iuTeu"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ketanhdoshi/ml/master/lib/nb_export.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYy95Q_juc1i"
      },
      "source": [
        "from nb_export import notebook2scriptSingle\n",
        "notebook2scriptSingle(gn_path + '/lib/image_lib.ipynb', gn_path + '/exp')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}