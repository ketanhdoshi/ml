{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_lib.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "veoZCmHIA211",
        "cuOb59kE8fjS",
        "vaSIOas8d_LR",
        "iMC2GmeSVURq",
        "WPf0Sjpb7Nh_",
        "iD1TfZNsn5Ou",
        "A4OG3pUo8v8I",
        "YXW-dtI_ucsA",
        "HCChmw72cTc1",
        "MMdTYjgCc9cf",
        "JvmoFp9RdTUe",
        "u27FW5yod2oh",
        "Mr-qWSvUd5je",
        "dn3oqFT0d71O",
        "YrarhQ1cd_mT",
        "qJVi6nX9i_S1",
        "9tWTOeD4lUJg",
        "F8sH3Uo6IRA1",
        "0ymOtKzCFKMe",
        "jgulSb12lmi1",
        "QqWrvFiifAdd",
        "rXMvvOobffLy",
        "GTYFzFfHf7n7",
        "IazGtlaHgKbl",
        "DS9SwUCWbY5Y",
        "O-MFmsw8m7DM",
        "f67GwyU4Xe0L",
        "iim4HEAK-alg",
        "5TOD2Kt_8AGO",
        "ux3TJZVWCAYq",
        "XoL1QeLI7m9Z"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanhdoshi/ml/blob/master/lib/data_lib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_pesjpdviLC"
      },
      "source": [
        "## Datablock API\n",
        "\n",
        "Notebooks - [datablock](https://github.com/fastai/course-v3/blob/master/nbs/dl2/08_data_block.ipynb), [augmentation](https://github.com/fastai/course-v3/blob/master/nbs/dl2/10_augmentation.ipynb), [text](https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED9jc6xzw4FQ"
      },
      "source": [
        "**Todos**\n",
        "*   Figure out if it is possible to export kaggle_data. Importing .py file gives error\n",
        "*   Rename extract_col to extract_colval in data bundles in other files eg. Attention Bi-dir LSTM\n",
        "*   Create a Tuple Dataset or something which passes in the 'y' target as input to the model. So it returns a tuple of (x, y) as the 'x' value and 'y' as the 'y' value\n",
        "*   Facial Keypoints dataset\n",
        "*   dfitemlist Extract column takes a post-processing function\n",
        "*   Rename db.do to db.process in other files which use it\n",
        "*   Rename extract_col to extract_colval in other files  \n",
        "*   Mask Itemlist\n",
        "*   Keypt Itemlist\n",
        "*   Bounding Box Itemlist\n",
        "*   Display of Keypt, Bounding Box\n",
        "*   Augmentation of Keypt, Bounding Box\n",
        "*   Comments for Paired get item\n",
        "*   Clean up float_tensor xform for Mask. Also, call it only if img_pair is not NULL. Mask should be converted into tensor in byte_tensor?\n",
        "*   Cleanup any places with 'WAIT_' and '!!!!!!!'\n",
        "*   Document concepts of Paired, DL Post Processing in Datablock Overview section\n",
        "*   Make the non-Dataset and not-Dataloader sections independent of torch/keras framework?\n",
        "*   Should Plotter be in this file??\n",
        "*   DONE Comments for Display Data\n",
        "*   DONE Display Results\n",
        "*   DONE Separate DataBundle class, DataBundle specs, ItemContainer and ItemList into separate cells\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veoZCmHIA211"
      },
      "source": [
        "### Create test CSV file data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bID3qways9cd"
      },
      "source": [
        "!rm tc.csv\n",
        "!echo 'reviews','sentiments' >>tc.csv\n",
        "!echo 'this is my first sentence','happy' >>tc.csv\n",
        "!echo 'another longer sentence','sad' >>tc.csv\n",
        "!echo 'this is a text classification set','sad' >>tc.csv\n",
        "!echo 'fourth line','happy' >>tc.csv\n",
        "!echo 'fifth line','sad' >>tc.csv\n",
        "!ls\n",
        "!cat tc.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuOb59kE8fjS"
      },
      "source": [
        "### Imagenette Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuwDF3Ju8iiF"
      },
      "source": [
        "from fastai import datasets\n",
        "from pathlib import Path\n",
        "\n",
        "IMAGENETTE_160_URL='https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160'\n",
        "path_imagenette = datasets.untar_data(datasets.URLs.IMAGENETTE_160)\n",
        "path_imagenette, list(path_imagenette.iterdir()), list((path_imagenette/'val').iterdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9exk6uST9qIG"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy\n",
        "import PIL\n",
        "\n",
        "path_tench = path_imagenette/'val'/'n01440764'\n",
        "file_tench = list(path_tench.iterdir())[0]\n",
        "img_tench = PIL.Image.open(file_tench)\n",
        "npa_tench = numpy.array(img_tench)\n",
        "print (npa_tench.shape)\n",
        "plt.imshow(img_tench)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaSIOas8d_LR"
      },
      "source": [
        "### IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtvZZ2N9eB94"
      },
      "source": [
        "path_imdb = datasets.untar_data(datasets.URLs.IMDB)\n",
        "path_imdb, list(path_imdb.iterdir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMC2GmeSVURq"
      },
      "source": [
        "### Kaggle Data Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh_3u9DXOEJu"
      },
      "source": [
        "def kaggle_data(data_type, data_name):\n",
        "  # Run this cell and select the kaggle.json file downloaded\n",
        "  # from the Kaggle account settings page.\n",
        "  from google.colab import files\n",
        "  files.upload()\n",
        "\n",
        "  # Let's make sure the kaggle.json file is present.\n",
        "  !ls -lha kaggle.json\n",
        "\n",
        "  # Next, install the Kaggle API client after forcing an upgrade\n",
        "  !pip uninstall -y kaggle\n",
        "  !pip install --upgrade pip\n",
        "  !pip install kaggle==1.5.6\n",
        "  !kaggle -v\n",
        "\n",
        "  # Reason for doing a force-upgrade. The underlying problem: Colab installs both py2 and py3 \n",
        "  # packages, and (for historical reasons) the py2 packages are installed second. kaggle is a \n",
        "  # wrapper installed by the kaggle python package; since we do py2 second, the py2 wrapper \n",
        "  # is in /usr/local/bin, and happens to be an older version.\n",
        "\n",
        "  # The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "  # so move it there.\n",
        "  !mkdir -p ~/.kaggle\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "\n",
        "  # This permissions change avoids a warning on Kaggle tool startup.\n",
        "  !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "  # List available datasets.\n",
        "  !kaggle {data_type} list\n",
        "\n",
        "  # First, you have to login to Kaggle, go to that competition's page, navigate to \n",
        "  # the Rules tab and accept the terms and conditions. Unless you do that, you will get\n",
        "  # a 403-Forbidden error when you run the command below\n",
        "\n",
        "  # Copy the carvana data set locally.\n",
        "  !kaggle {data_type} download {data_name}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPf0Sjpb7Nh_"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHLPROiP6Lpk"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI5e1uoLdVMQ"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-kF6y5l7H7O"
      },
      "source": [
        "#export\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import IPython.core.debugger as db\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_categorical_dtype\n",
        "import torch\n",
        "from torch import tensor, LongTensor\n",
        "from torch.utils.data import Sampler, DataLoader, SequentialSampler, RandomSampler\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import PIL, mimetypes\n",
        "from PIL import Image\n",
        "import spacy, html, re\n",
        "from spacy.symbols import ORTH\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdMu88LpvoaV"
      },
      "source": [
        "from torch import FloatTensor,LongTensor\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU23hChRz_Gb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "gn_path = 'gdrive/My Drive/Colab Notebooks'\n",
        "gd_path = 'gdrive/My Drive/Colab Data/fastai-v3'\n",
        "\n",
        "import sys\n",
        "sys.path.insert(1, gn_path + '/exp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83F5MZgc_Y7_"
      },
      "source": [
        "#export\n",
        "\n",
        "from nb_util import ListContainer, parallel, Counter\n",
        "# from nb_audio import AudioUtil - this is imported in the AudioItemList after installing torchaudio\n",
        "from nb_image import ShowImg, PilImg, OcvImg, imgaug_random_resized_crop, imgaug_perspective_warp, imgaug_albu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD1TfZNsn5Ou"
      },
      "source": [
        "### Display Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jLswMlKpa7j"
      },
      "source": [
        "#export\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Generic Plotter class which encapsulates all plotting functionality\n",
        "# All methods are meant to be statics, so you can create a single\n",
        "# dummy instance and call them\n",
        "# -----------------------------------------------------\n",
        "class Plotter(object):\n",
        "    def __init__(self, grid_row = 4, grid_col = 4, figsize = 18):\n",
        "        self.grid_row = grid_row\n",
        "        self.grid_col = grid_col\n",
        "        self.figsize = figsize\n",
        "        self.fig = self._create_fig ()\n",
        "\n",
        "    # Create a grid of subplots using the given plot type. Each \n",
        "    # subplot graphs one feature in the dataframe using the same\n",
        "    # plot type\n",
        "    def plot_grid (self, ptype, df, features, target=None, **kwargs):\n",
        "        # Create a subplot for each feature\n",
        "        for feature in features:\n",
        "          if (ptype == 'line'):\n",
        "            self.create_subplot (ptype, df, y=feature, x=target, **kwargs)\n",
        "          else:\n",
        "            self.create_subplot (ptype, df, x=feature, y=target, **kwargs)\n",
        "\n",
        "    # Create a new subplot at the next position in the grid\n",
        "    def create_subplot (self, ptype, df, x, y=None, **kwargs):       \n",
        "        fig = self.fig\n",
        "        # New subplot will be created at a position that is 1 more \n",
        "        # than the existing count of subplots\n",
        "        pos = len(fig.axes) + 1\n",
        "        ax = fig.add_subplot(self.grid_row, self.grid_col, pos)\n",
        "\n",
        "        ax.yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "        #plt.ticklabel_format(style='plain', axis='y', useOffset=False)\n",
        "\n",
        "        # Get the plotting function based on the plot type...\n",
        "        plot_func = self._get_plottype (ptype)\n",
        "        # ... and call the function\n",
        "        plot_func (df, x, y, ax=ax, **kwargs)\n",
        "        return (ax)\n",
        "\n",
        "    # Create the top-level figure\n",
        "    def _create_fig (self):\n",
        "        fig = plt.figure(figsize = (self.figsize, self.figsize))\n",
        "        # Spacing between the subplots\n",
        "        plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n",
        "        return (fig)\n",
        "    \n",
        "    # Get the method for the required plot type\n",
        "    def _get_plottype (self, ptype):\n",
        "        # Generate the method name for the plot type\n",
        "        method_name = '_plot_' + ptype\n",
        "        # Get the method from 'self'. Default to a lambda.\n",
        "        method = getattr(self, method_name, lambda: \"Invalid plot type\")\n",
        "        # Return the method\n",
        "        return method\n",
        " \n",
        "    # There is a separate method for each type of plot. Each method\n",
        "    # name follows a naming convention ie. \"_plot_<plot type>\"\n",
        "    # Bar graph of the total number of values for each 'x'\n",
        "    def _plot_count(self, df, x, y, ax):\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\", fontsize=10)\n",
        "        sns.countplot(x=x, data=df)\n",
        " \n",
        "    # Histogram\n",
        "    def _plot_dist(self, df, x, y, ax):\n",
        "        # Trim low and high outliers at both extremes\n",
        "        trim_df = df[(df[x] > df[x].quantile(0.005)) & (df[x] < df[x].quantile(0.95))]\n",
        "\n",
        "        sns.distplot(trim_df[x], kde=False)\n",
        "        #sns.distplot(df[x])\n",
        " \n",
        "    def _plot_line(self, df, x, y, ax, hue=None):\n",
        "        sns.lineplot(x=x, y=y, hue=hue, data=df)\n",
        "\n",
        "    # Bar graph plots the mean of all the 'y' values for each 'x'\n",
        "    def _plot_bar(self, df, x, y):\n",
        "        sns.barplot(x=x, y=y, data=df)\n",
        "\n",
        "    def _plot_multicol(self, df, x, y):\n",
        "        sns.barplot(df.index, df[x])\n",
        "        \n",
        "    def _plot_series(self, df, x, y):\n",
        "        sns.barplot(df.index, df.values)\n",
        "        \n",
        "    def _plot_series_horiz(self, df, x, y):\n",
        "        sns.barplot(df.values, df.index)\n",
        "        \n",
        "    def _plot_box(self, df, x, y):\n",
        "        sns.boxplot(df[x], df[y])\n",
        "        \n",
        "    def _plot_reg(self, df, x, y):\n",
        "        sns.regplot(x=x, y=y, data=df)\n",
        "\n",
        "    def _plot_scatter(self, df, x, y):\n",
        "        sns.scatterplot(x=x, y=y, data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzoudkuKn9r1"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Displays data and results in the appropriate format\n",
        "# It is able to display data as images, text and tabular format. We will add\n",
        "# additional formats (eg. audio, video) as we encounter them.\n",
        "#----------------------------------------------------\n",
        "class DisplayData():\n",
        "  # This is not required right now. All the methods are really statics, so an instance\n",
        "  # need not be initialised. Leaving this for now in case it is required for saving any\n",
        "  # state.\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  # ----------------------------\n",
        "  # Display image data, using the Image library\n",
        "  # ----------------------------\n",
        "  def display_images(self, x_imgs, y_data, z_data, xyz_procedures, **kwargs):\n",
        "    sc = ShowImg()\n",
        "    xyz_methods = self._get_xyz_methods(sc, xyz_procedures)\n",
        "    sc.show_grid(x_imgs, y_data, z_data, **xyz_methods, **kwargs)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get the actual Image library method names for displaying different variations\n",
        "  # of image data such as mask, bbox, keypts etc.\n",
        "  # ----------------------------\n",
        "  def _get_xyz_methods(self, sc, xyz_procedures):\n",
        "    xyz_procedures = map(lambda p: 'show_' + p, xyz_procedures) \n",
        "    x_procedure, y_procedure, z_procedure = xyz_procedures\n",
        "\n",
        "    x_method = getattr(sc, x_procedure)\n",
        "    y_method = getattr(sc, y_procedure)\n",
        "    z_method = getattr(sc, z_procedure)\n",
        "    assert((x_method is not None) and (y_method is not None) and (z_method is not None))\n",
        "    return {'x_method': x_method, 'y_method': y_method, 'z_method': z_method}\n",
        "\n",
        "  # ----------------------------\n",
        "  # Display text data as a Pandas Dataframe\n",
        "  # ----------------------------\n",
        "  def display_texts(self, x_texts, y_data, z_data, **kwargs):\n",
        "    from IPython.display import display, HTML\n",
        "    df_data = {'x': x_texts, 'y': y_data}\n",
        "    if (z_data): \n",
        "      df_data['z'] = z_data\n",
        "    df = pd.DataFrame(df_data)\n",
        "    display(HTML(df.to_html(index=False)))\n",
        "\n",
        "  # ----------------------------\n",
        "  # Display tabular data as a Pandas Dataframe\n",
        "  # ----------------------------\n",
        "  def display_table(self):\n",
        "    pass\n",
        "\n",
        "  # ----------------------------\n",
        "  # Show a Dataframe's metadata summary\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def display_summary(mds):\n",
        "    with pd.option_context(\n",
        "        'float_format', '{:.2f}'.format,\n",
        "        'display.max_rows', None\n",
        "        ):\n",
        "      display(mds)\n",
        "\n",
        "  @staticmethod\n",
        "  def display_rel(fan_df, x, merge_df):\n",
        "    p = Plotter()\n",
        "    p.create_subplot ('dist', fan_df, x)\n",
        "\n",
        "    with pd.option_context(\n",
        "        'float_format', '{:.2f}'.format,\n",
        "        ):\n",
        "      display(merge_df.head(15))\n",
        "\n",
        "  @staticmethod\n",
        "  def data_distrib (df, cat_cols, num_cols):\n",
        "      p = Plotter()\n",
        "      #Frequency of categorical values\n",
        "      p.plot_grid ('count', df, cat_cols)\n",
        "\n",
        "      # Histograms of Numerical features\n",
        "      p.plot_grid ('dist', df, num_cols)\n",
        "\n",
        "  # -----------------------------------------------------\n",
        "  # Bivariate plot of each feature with the target. \n",
        "  # If the target is Categorical, use Bar Graphs\n",
        "  # If the target is Numerical, use Scatter plots for numeric features and..\n",
        "  #                         ...Box plots for categorical features\n",
        "  # -----------------------------------------------------\n",
        "  def data_target (df, cat_cols, num_cols, target):\n",
        "      p = Plotter()\n",
        "      if (is_categorical (df, target)):\n",
        "          plot_pair_target (train_df, cat_cols, target)\n",
        "          #plot_stackhist(train_df, ['Pclass', 'SibSp', 'Parch', 'Fare', 'Age', 'Sex'], 'Survived')\n",
        "      else:\n",
        "          p.plot_grid ('scatter', df, num_cols, target)\n",
        "          p.plot_grid ('box', df, cat_cols, target)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Display audio wave or spectrogram data, using the Audio library\n",
        "  # ----------------------------\n",
        "  def display_audio(self, x_data, y_data, z_data, num_cols=8, figsize=None, **kwargs):\n",
        "    assert(len(x_data) == len(y_data))\n",
        "\n",
        "    # Compute number of rows and columns in grid\n",
        "    num_data = len(x_data)\n",
        "    num_rows = int (math.ceil (num_data / num_cols))\n",
        "\n",
        "    # Create figure and subplots as per number of rows and columns\n",
        "    if (figsize is None):\n",
        "      figsize=(num_cols * 3, num_rows * 3)\n",
        "    fig,axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
        "\n",
        "    for xd, label, ax in zip (x_data, y_data, axes.flat):\n",
        "      if (isinstance(xd, tuple)):\n",
        "        # Data is an audio waveform\n",
        "        AudioUtil.show_wave(aud=xd, label=label, ax=ax)\n",
        "      else:\n",
        "        AudioUtil.show_spectro(spec=xd, label=label, ax=ax)\n",
        "\n",
        "    for i in range(num_data, len(axes.flat)): axes.flat[i].set_visible(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4OG3pUo8v8I"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LSx2C5c623B"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Dataset containing a multi-dimensional (2D or larger) array\n",
        "#----------------------------------------------------\n",
        "class MultiDimDataset():\n",
        "  def __init__(self, x, y):\n",
        "    self.x, self.y = x, y\n",
        "    \n",
        "  def __len__(self):\n",
        "    return (self.x.shape[0])\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    return (self.x[i], self.y[i])\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Dataset containing an ItemList\n",
        "#----------------------------------------------------\n",
        "class ILDataset():\n",
        "  def __init__(self, x, y):\n",
        "    self.x, self.y = x, y\n",
        "    \n",
        "  def __len__(self):\n",
        "    return (len(self.x))\n",
        "  \n",
        "  # ----------------------------\n",
        "  # Get a (x, y) pair for one sample that can be passed to the model. \n",
        "  # ----------------------------\n",
        "  def __getitem__(self, i):\n",
        "    return (self.x[i], self.y[i])\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get the display data from the the 'x' and 'y' ItemList\n",
        "  # by converting from the 'obj' format to human-readable.\n",
        "  # ----------------------------\n",
        "  def get_display_data(self, i):\n",
        "    x_obj, y_obj = self[i]\n",
        "    x_display_data = self.x.get_display_data(x_obj, i)\n",
        "    y_display_data = self.y.get_display_data(y_obj, i)\n",
        "    return (x_display_data, y_display_data)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Dataset containing a Paired ItemList as needed for\n",
        "# Image data with an image + mask, image + bbox, image + key points\n",
        "# for Segmentation, Object Detection etc.\n",
        "#----------------------------------------------------\n",
        "class ILPairedDataset(ILDataset):\n",
        "  def __getitem__(self, i):\n",
        "    x_il, y_il = self.x, self.y\n",
        "    # Get the corresponding 'y' objects for the same index 'i'\n",
        "    y_objs = y_il[i]\n",
        "\n",
        "    # Get the 'x' objects after providing the 'x' item list with the paired 'y' objects so\n",
        "    # that it can process their transforms together\n",
        "    xy_tuples = x_il.get_paired_items(y_objs, i)\n",
        "\n",
        "    # If 'i' is a single value, we get back a tuple (x, y) \n",
        "    # If 'i' is a slice range, we get back a list of tuples [(x, y), (x, y), ...]\n",
        "    if (isinstance(xy_tuples, list)):\n",
        "      # Separate the 'x' values from the 'y' values into two lists\n",
        "      xs, ys = zip(*xy_tuples)\n",
        "      x, y = list(xs), list(ys)\n",
        "    else:\n",
        "      x, y = xy_tuples\n",
        "    return (x, y)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Example Dataset for Tabular data. It is not used anywhere at the moment but\n",
        "# is here as an illustration.\n",
        "#----------------------------------------------------\n",
        "class TabDemoDataset(ILDataset):\n",
        "  # ----------------------------\n",
        "  # Note that the structure and format of both 'x' and 'y' is entirely up to us. \n",
        "  # For instance, they can be lists, numpy arrays, tensors etc of any shape. Pytorch's\n",
        "  # default_collate function can handle many such formats and can convert them into\n",
        "  # tensors suitable for feeding to a model. And we can define a custom collate\n",
        "  # function for formats which are not handled by the default collate.\n",
        "  #\n",
        "  # 'x' and 'y' don't have to be a single object eg. 'x' can be a tuple of three\n",
        "  # objects. This allows us to pass multiple objects to the forward method of\n",
        "  # the model. For instance, below we could partition the 'x' into a tuple of\n",
        "  # two objects for categorical variables and continuous variables respectively.\n",
        "  # The model's forward function would then be 'forward(self, x_cat, x_cont)'\n",
        "  # ----------------------------\n",
        "  def __getitem__(self, i):\n",
        "    return ([self.x[i][:3], self.x[i][3:-1]], self.y[i][-1])\n",
        "\n",
        "#----------------------------------------------------\n",
        "# The Fastai Language Model example creates batches in a non-standard way\n",
        "# See https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb\n",
        "# Instead of breaking up the contiguous stream into sequential samples, it puts\n",
        "# them in different batches\n",
        "#----------------------------------------------------\n",
        "class FastaiLMDataset(ILDataset):\n",
        "    def __init__(self, x, y, bs=64, bptt=70, shuffle=False, **kwargs):\n",
        "        self.x, self.y = x, y\n",
        "        self.bs,self.bptt,self.shuffle = bs,bptt,shuffle\n",
        "        total_len = len(x)\n",
        "        self.n_batch = total_len // bs\n",
        "        self.batchify()\n",
        "    \n",
        "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        source = self.batched_data[idx % self.bs]\n",
        "        seq_idx = (idx // self.bs) * self.bptt\n",
        "        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]\n",
        "    \n",
        "    def get_display_data(self, i):\n",
        "      x_obj, y_obj = self[i]\n",
        "      x_display_data = self.x.get_display_data(x_obj, i)\n",
        "      y_display_data = self.x.get_display_data(y_obj, i)\n",
        "      return (x_display_data, y_display_data)\n",
        "\n",
        "    def batchify(self):\n",
        "        #if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
        "        stream = tensor(self.x)\n",
        "        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)\n",
        "\n",
        "def get_dls(train_ds, valid_ds, bs, repro=False, **kwargs):\n",
        "    return (DataLoader(train_ds, batch_size=bs, shuffle=not repro, **kwargs),\n",
        "            DataLoader(valid_ds, batch_size=bs*2, **kwargs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXW-dtI_ucsA"
      },
      "source": [
        "### DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NkovpEPuaoh"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Pytorch's Dataloader automatically invokes a Sampler function while preparing a batch to\n",
        "# figure out which samples should be included in the batch, and in what order. A Sampler\n",
        "# function returns one mini-batch per iteration, as a list of indexes of samples\n",
        "# [sample_1_idx, sample_2_idx, ...]\n",
        "#\n",
        "# Pytorch then fetches the sample data for those indexes from the Dataset and \n",
        "# calls a Collate function with the list of samples in one mini-batch. Each sample\n",
        "# is a sub-list (or tuple) of two elements, for the 'x' and 'y' data.\n",
        "# [[sample_1_x, sample_1_y], [sample_2_x, sample_2_y], ...] OR\n",
        "# [(sample_1_x, sample_1_y), (sample_2_x, sample_2_y), ...]\n",
        "#\n",
        "# The Collate function joins all the 'x' samples into a single 'x' tensor, and \n",
        "# the 'y' samples into a single 'y' tensor\n",
        "#\n",
        "# Pytorch has default Sampler and Collate functions, or we can write custom functions\n",
        "#\n",
        "# During text classification, since texts can be of different lengths, the samplers create mini-batches\n",
        "# with texts of similar lengths to make it efficient, so that the model doesn't have to handle\n",
        "# texts of vastly varying lengths in the same batch.\n",
        "#\n",
        "# Then the collate function pads those texts to make them the same length so that they can be\n",
        "# converted into a tensor with a rectangular shape\n",
        "#----------------------------------------------------\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Sort samples by text length, used for validation data\n",
        "#----------------------------------------------------\n",
        "class SortSampler(Sampler):\n",
        "    def __init__(self, data_source, key): \n",
        "      self.data_source,self.key = data_source,key\n",
        "\n",
        "    def __len__(self): return len(self.data_source)\n",
        "    def __iter__(self):\n",
        "      # Sort the list using the key function\n",
        "      return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))\n",
        "\n",
        "#----------------------------------------------------\n",
        "# For training data, we want some kind of randomness in addition to sorting by text length. \n",
        "# So first, we shuffle the texts and build megabatches of size 50 * bs. We sort those \n",
        "# megabatches by length before splitting them into 50 minibatches. That way we have randomized \n",
        "# batches of roughly the same length.\n",
        "#\n",
        "# Then we make sure to have the biggest batch first and shuffle the order of the other \n",
        "# batches. We also make sure the last batch stays at the end because its size is probably \n",
        "# lower than batch size.\n",
        "#----------------------------------------------------\n",
        "class SortishSampler(Sampler):\n",
        "    def __init__(self, data_source, key, bs):\n",
        "        self.data_source,self.key,self.bs = data_source,key,bs\n",
        "\n",
        "    def __len__(self) -> int: return len(self.data_source)\n",
        "\n",
        "    def __iter__(self):\n",
        "        #idxs = torch.randperm(len(self.data_source))\n",
        "        idxs = list(range(len(self.data_source)))\n",
        "        random.shuffle(idxs)\n",
        "\n",
        "        # Create megabatches\n",
        "        megabatch_len = self.bs * 50\n",
        "        megabatches = [idxs[i:i+megabatch_len] for i in range(0, len(self.data_source), megabatch_len)]\n",
        "\n",
        "        # Sort within a megabatch and then flatten all megabatches into a single list\n",
        "        sorted_megabatches = [sorted(b, key=self.key, reverse=True) for b in megabatches]\n",
        "        sorted_idxs = [j for i in sorted_megabatches for j in i]\n",
        "\n",
        "        # Create mini-batches\n",
        "        batches = [sorted_idxs[i:i+self.bs] for i in range(0, len(sorted_idxs), self.bs)]\n",
        "\n",
        "        # Move the mini-batch with the longest length to the top\n",
        "        max_idx = max([ck[0] for ck in batches], key=self.key)\n",
        "        max_idx = max(list(range(len(batches))), key=lambda x: self.key(batches[x][0]))\n",
        "        batches[0],batches[max_idx] = batches[max_idx],batches[0]\n",
        "\n",
        "        mid_batch_idxs = list(range(1, len(batches)-1))\n",
        "        random.shuffle(mid_batch_idxs)\n",
        "        shuffled_mid_batch = [batches[i] for i in mid_batch_idxs]\n",
        "\n",
        "        sorted_idxs = batches[0] + [item for sublist in shuffled_mid_batch for item in sublist] + batches[-1]\n",
        "        return iter(sorted_idxs)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Collate function for image data\n",
        "# It gets a list of (x, y) tuples\n",
        "# NB: This is not used right now. It depends on the 'xs' and 'ys' being tensors.\n",
        "#----------------------------------------------------\n",
        "def collate(b):\n",
        "  # 'xs' and 'ys' are a list of tensors\n",
        "  xs,ys = zip(*b)\n",
        "\n",
        "  # Join the list of 'x' into one tensor, and similarly for the 'y'\n",
        "  return torch.stack(xs),torch.stack(ys)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Collate function for text classification data\n",
        "# Add the 'pad_idx' token at the end of each sequence to make them \n",
        "# all the same size when batching them. \n",
        "#----------------------------------------------------\n",
        "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
        "  # Get the length of the longest sentence in the batch\n",
        "  max_len = max([len(s[0]) for s in samples])\n",
        "\n",
        "  # Create empty 'x' result tensor filled with the 'pad_idx'\n",
        "  res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
        "\n",
        "  # Go through each sample and fill its data into the result tensor\n",
        "  for i,s in enumerate(samples):\n",
        "    if pad_first: res[i, -len(s[0]):] = LongTensor(s[0])\n",
        "    else:         res[i, :len(s[0]) ] = LongTensor(s[0])\n",
        "\n",
        "  # Return 'x' result tensor, and 'y' data\n",
        "  return res, tensor([s[1] for s in samples])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCChmw72cTc1"
      },
      "source": [
        "### DataBundle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_LgMbUTcm44"
      },
      "source": [
        "#----------------------------------------------------\n",
        "# There are three main logical entities for data preparation:\n",
        "#   ItemContainer - the original source of the data, from which an ItemList is loaded.\n",
        "#                   eg. Folder, CSV File, Dataframe, Json File\n",
        "#   ItemList - represents a list of 'rows' or 'items' of a certain type of data format. Operations include splitting it row-wise into subsets and converting from one type to another. \n",
        "#              This is the core data structure that is processed throughout the data preparation pipeline.\n",
        "#              eg. File Paths, Image Files, Audio Files, Text Files, Class Names, Class Ids, Multi Class Names, Sentences, Words, Word Ids\n",
        "#              eg. Chars, Docs, Bounding Box, Image Point, Segmentation Mask\n",
        "#   DataBundle - represents a data preparation flow for an application use case. It does all the orchestration.\n",
        "#                eg. Image Classification, Text Classification, Language Model, Multi Image Classification, Object Detection, Object Segmentation\n",
        "#\n",
        "# Once loaded, an ItemList goes through one split and a series of data type conversions (viz. extractions, converters and transforms) till we end up with \n",
        "# data in the format required for model training. Each of these takes an ItemList and outputs another ItemList, but the semantics are slightly different:\n",
        "#   Split - split the items row-wise \n",
        "#   Extract - take the 'raw loaded rows' and extract the minimal 'columns' needed for the ML model viz. 'x' feature items and 'y' label items\n",
        "#   Convert - enrich the 'x' and 'y' items (eg. fill missing values) or convert them from one data type to another\n",
        "#   Transform - as opposed to Convert which is a 'permanent one-time' conversion, this is a 'dynamic run-time' conversion where the converted output is \n",
        "#               processed for model training but not persisted.\n",
        "#\n",
        "# The end outcome is to create a DataBundle with three Datasets (for training, validation and test), each with a 'x' and 'y' tensor of floats\n",
        "#   and three Dataloaders which wrap the corresponding Dataset, which can be used for model training\n",
        "#\n",
        "# The end-to-end flow to prepare data for training is:\n",
        "#   Load data from an ItemContainer which is a 'logical container' for your data. It returns an ItemList which contains a list of all the 'logical rows' of your data.\n",
        "#      An ItemContainer has subclasses of different types (eg. a Folder Container which contains data rows as files, or a CSV Container which contains data rows)\n",
        "#      An ItemList has subclasses of different types (eg. an Image File item list holds image files, a Sentence item list holds text sentences)\n",
        "#      Each 'logical row' of data is known as an 'item'\n",
        "#   Split all the data rows from the ItemList into three separate ItemLists for training, validation and test data respectively\n",
        "#      There are various procedures to do this split - based on a containing folder for file-based lists, randomly based on a percentage, based on a custom function etc\n",
        "#   Extract the 'x' items from each of the three ItemLists\n",
        "#      There are various procedures to do the extraction - based on a column for Dataframe lists, based on a parent folder for file-based lists, by reading the text for a text-file list etc\n",
        "#   Convert the extracted 'x' items via a sequential pipeline of conversion steps. Each conversion results in a target ItemList of the same or different type than the source\n",
        "#      There are various procedures which know how to do a specific type of conversion (eg. from a text sentence to a list of words)\n",
        "#   Similarly, extract the 'y' items from each of the three ItemLists and then convert them via another pipeline\n",
        "#   Create three Datasets from each of the three pairs of 'x' and 'y' ItemLists, passing it any custom parameters if needed\n",
        "#   Then create DataLoaders from those Datasets, passing it any custom sampler and/or collate functions if needed. Note that we may need\n",
        "#      to use different parameter settings for the training and validation DataLoaders\n",
        "#   During training, optionally transform the 'x' items dynamically by performing additional conversions via a sequential pipeline.\n",
        "#      Since these conversions are performed at 'runtime' the converted data is not persisted but used for training immediately\n",
        "#      eg. Images can be loaded from image files and augmented, resized, cropped etc. They are then converted to pixels and eventually to tensors.\n",
        "#          Doing this pre-training and storing the transformed data would be too memory-intensive.\n",
        "#          So the image data is stored as image filenames and transformed batch-wise during training\n",
        "#   \n",
        "#   Each step above is intended to be composed in different ways to allow you to flexibly create an end-to-end pipeline for different application use cases.\n",
        "#   A DataBundle processes this whole pipeline and lets you specify the pipeline declaratively.\n",
        "#      A DataBundle has many subclasses for each of the common use cases, which know how to build a pipeline for that use case.\n",
        "#      eg. an Image Classification data bundle, Language Model, Object Detection etc\n",
        "#----------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPR73l9WcXE6"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Defines and orchestrates an end-to-end data preparation pipeline for different use cases\n",
        "# The pipeline is defined via several declarative parameters for each step (ie. Load, Split, Extract, Convert and Transform)\n",
        "#----------------------------------------------------\n",
        "class DataBundle:\n",
        "  def __init__(self, load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=None, ds_params=None, dl_params=None):\n",
        "    self.all_rows = None\n",
        "    self.load_params = load_params\n",
        "    self.split_params = split_params\n",
        "    self.extract_x_params, self.extract_y_params = extract_x_params, extract_y_params\n",
        "    self.convert_x_params, self.convert_y_params = convert_x_params, convert_y_params\n",
        "    self.xform_x_params = xform_x_params\n",
        "    self.ds_params = ds_params if (ds_params is not None) else {'target_ds': ILDataset}\n",
        "    self.dl_params = dl_params \n",
        "    self.train_rows, self.valid_rows, self.test_rows = None, None, None\n",
        "\n",
        "  # ----------------------------\n",
        "  # !!!!!!!! TODO get rid of this once all callers have been renamed\n",
        "  # ----------------------------\n",
        "  def do(self):\n",
        "    return self.process()\n",
        "\n",
        "  # ----------------------------\n",
        "  # Orchestrates and data preparation pipeline using the specified parameters for each step\n",
        "  # ----------------------------\n",
        "  def process(self, steps=['load', 'post_load']):\n",
        "    if ('load' in steps):\n",
        "      # Create a Container and Load all the data rows into an ItemList\n",
        "      self.all_rows = self._load (**self.load_params)\n",
        "      if (steps == ['load']): return self.all_rows\n",
        "\n",
        "    if ('post_load' in steps):\n",
        "      # Continue with post-loading split, extract and convert\n",
        "      assert(self.all_rows)\n",
        "      self._post_load()\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split, extract and convert the loaded data\n",
        "  # ----------------------------\n",
        "  def _post_load(self):\n",
        "    # Split the the data rows into three subset ItemLists for training, validation and test\n",
        "    self._split(**self.split_params)\n",
        "\n",
        "    # Prepare three Datasets by extracting 'x' and 'y' data items and going through a sequence of conversion steps to produce the required ItemList data type\n",
        "    ds_list = self._rows_to_ds(self.extract_x_params, self.extract_y_params, self.convert_x_params, self.convert_y_params, self.xform_x_params, self.ds_params)\n",
        "    \n",
        "    # Free up memory\n",
        "    del self.all_rows, self.train_rows, self.valid_rows, self.test_rows\n",
        "    self.all_rows, self.train_rows, self.valid_rows, self.test_rows = None, None, None, None\n",
        "\n",
        "    self._print_ds(ds_list)\n",
        "\n",
        "    # Prepare three Dataloaders from the Datasets\n",
        "    dl_list = self._ds_to_dl(ds_list, self.dl_params)\n",
        "    \n",
        "    assert((len(ds_list) == len(dl_list)) and (0 < len(ds_list) <= 3))\n",
        "\n",
        "    (self.train_ds, _) = ds_list[0]\n",
        "    self.train_dl = dl_list[0]\n",
        "\n",
        "    if (len(ds_list) > 1): \n",
        "      (self.valid_ds, _) = ds_list[1]\n",
        "      self.valid_dl = dl_list[1]\n",
        "    else:\n",
        "      self.valid_ds, self.valid_dl = None, None\n",
        "\n",
        "    if (len(ds_list) > 2):\n",
        "      (self.test_ds, _) = ds_list[2]\n",
        "      self.test_dl = dl_list[2]\n",
        "    else:\n",
        "      self.test_ds, self.test_dl = None, None\n",
        "\n",
        "  # ----------------------------\n",
        "  # Load the data from the source Container into the target ItemList\n",
        "  # ----------------------------\n",
        "  def _load(self, source, target_cls, **kwargs):\n",
        "    # Create a Container of subclass type 'source' and load the data\n",
        "    cntnr = source(**kwargs)\n",
        "\n",
        "    # The Container creates an ItemList of subclass type 'target_cls'\n",
        "    all_rows = cntnr.load(target_cls)\n",
        "    \n",
        "    print (\"%s loaded %s items of type %s\" % (cntnr.__class__.__name__, len(all_rows), all_rows.__class__.__name__))\n",
        "    return (all_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split the loaded data based on a given 'split_procedure' and any arguments required by that\n",
        "  # split procedure\n",
        "  # ----------------------------\n",
        "  def _split(self, split_procedure, **kwargs):\n",
        "    if (split_procedure is None):\n",
        "      print ('No Split')\n",
        "      self.train_rows, self.valid_rows, self.test_rows = self.all_rows, None, None\n",
        "      return\n",
        "\n",
        "    # Map the given 'split_procedure' to the corresponding method on the\n",
        "    # loaded ItemList\n",
        "    split_method = getattr(self.all_rows, split_procedure)\n",
        "\n",
        "    # Call the split method and pass it any additional arguments \n",
        "    # The loaded data is split into three separate ItemLists\n",
        "    self.train_rows, self.valid_rows, self.test_rows = split_method(**kwargs)\n",
        "    print (\"Split using %s into %s, %s and %s items of type %s\" % (split_procedure, len(self.train_rows), len(self.valid_rows), len(self.test_rows) if self.test_rows is not None else 0, self.train_rows.__class__.__name__))\n",
        "\n",
        "  # ----------------------------\n",
        "  # Prepare three Datasets from the training, validation and test data rows\n",
        "  # First extract the 'x' and 'y' items and then convert each one to the required\n",
        "  # type of ItemList\n",
        "  # ----------------------------\n",
        "  def _rows_to_ds(self, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params, ds_params):\n",
        "    # Initialise list for three Datasets\n",
        "    ds_list = []\n",
        "\n",
        "    # We loop through the training, validation and test data and execute the\n",
        "    # extraction and conversion pipeline for each\n",
        "    #\n",
        "    # We maintain a Boolean flag indicating whether we're processing training data \n",
        "    # or not (ie. validation/test data), which will be used by the converters.\n",
        "    rows_list = [(self.train_rows, True)]\n",
        "    if (self.valid_rows is not None): rows_list.append((self.valid_rows, False))\n",
        "    if (self.test_rows is not None): rows_list.append((self.test_rows, False))\n",
        "    #\n",
        "    # Some converters may need to save state while processing the training data\n",
        "    # and then reuse that state while processing validation or test data. For instance\n",
        "    # it may create a vocab using the training data and use that same vocab for\n",
        "    # converting the validation and test data\n",
        "    #\n",
        "    # We initialise a dictionary to hold Conversion State and pass it to each converter.\n",
        "    # Any converter can then store its state under its own key in this dictionary. That\n",
        "    # state is then available to it while processing validation and test data\n",
        "    self.convert_state_x, self.convert_state_y = {}, {}\n",
        " \n",
        "    # Go through three iterations for the training, validation and test data\n",
        "    for rows, in_train in rows_list:\n",
        "      # Extract the 'x' and 'y' items\n",
        "      x_il = self._extract(rows, **extract_x_params)\n",
        "      y_il = self._extract(rows, **extract_y_params)\n",
        "\n",
        "      # Do all the conversion steps for the 'x' items. The output of one step\n",
        "      # becomes the input for the subsequent step\n",
        "      for converter in convert_x_params:\n",
        "        x_il = x_il.convert(self.convert_state_x, in_train, **converter)\n",
        "\n",
        "      # Do all the conversion steps for the 'y' items.\n",
        "      for converter in convert_y_params:\n",
        "        y_il = y_il.convert(self.convert_state_y, in_train, **converter)\n",
        "\n",
        "      # Set the transforms to the 'x' ItemList. They are not executed now, but\n",
        "      # will be executed dynamically during model training\n",
        "      if (xform_x_params is not None):\n",
        "        x_il.add_xform(xform_x_params)\n",
        "\n",
        "      # Create a Dataset using the 'x' and 'y' ItemLists\n",
        "      ds = self._create_ds(x_il, y_il, **ds_params)\n",
        "\n",
        "      # Add the Dataset to the list of Datasets, along with the corresponding\n",
        "      # in_train flag\n",
        "      ds_list.append((ds, in_train))\n",
        "\n",
        "    return (ds_list)\n",
        "  \n",
        "  # ----------------------------\n",
        "  # Extract items (either 'x' or 'y') from the given data rows\n",
        "  # ----------------------------\n",
        "  def _extract(self, rows_il, extract_procedure, target_cls, **kwargs):\n",
        "    if (extract_procedure is None):\n",
        "      print (\"No extraction\")\n",
        "      return (rows_il)\n",
        "\n",
        "    # Get the extract method on the ItemList and invoke it. It returns a Python list of extracted items\n",
        "    # Create a new target_cls ItemList using the extracted Python list of items\n",
        "    extract_method = getattr(rows_il, extract_procedure)\n",
        "    assert(extract_method is not None)\n",
        "\n",
        "    # Pass in an empty dictionary which the 'extract_method' can fill with extra arguments\n",
        "    # that it wants to pass back. Those extra arguments are then available to the constructor\n",
        "    # of the new target ItemList\n",
        "    kwargs['extra_args'] = {}\n",
        "    extract_list = extract_method(**kwargs)\n",
        "\n",
        "    # If the 'target_cls' is of the same type, then clone the data rows with the extracted\n",
        "    # list and any extra arguments\n",
        "    if (rows_il.__class__ == target_cls):\n",
        "      extract_il = rows_il.clone(extract_list, **kwargs)\n",
        "    else:\n",
        "      extract_il = target_cls(extract_list, **kwargs)\n",
        "\n",
        "    print (\"Extracted %s items of type %s using %s\" % (len(extract_il), extract_il.__class__.__name__, extract_procedure))\n",
        "    return (extract_il)\n",
        "\n",
        "  #-------------------------\n",
        "  # Create a Dataset using the given parameters\n",
        "  # ----------------------------\n",
        "  def _create_ds(self, x_il, y_il, target_ds, **kwargs):\n",
        "    assert (target_ds is not None)\n",
        "    ds = target_ds(x_il, y_il, **kwargs)\n",
        "    return ds\n",
        "\n",
        "  # ----------------------------\n",
        "  # Prepare a list of DataLoaders from the list of Datasets\n",
        "  # ----------------------------\n",
        "  def _ds_to_dl(self, ds_list, dl_params):\n",
        "    # dl_params is a tuple (params for training dl, params for valid/test dl)\n",
        "    if (dl_params is None):\n",
        "      # If no params are passed in, use an empty param list to create a DataLoader with defaults \n",
        "      dl_params=({}, {})\n",
        "    train_dl_params, valid_dl_params = dl_params\n",
        "\n",
        "    # Create a DataLoader for each Dataset in the list\n",
        "    dl_list = []\n",
        "    for ds, in_train in ds_list:\n",
        "      # Use the training dl params or the valid/test dl params\n",
        "      params = train_dl_params if in_train else valid_dl_params\n",
        "      dl = self._create_dl(ds, in_train, **params)\n",
        "      dl_list.append(dl)\n",
        "\n",
        "      # Do any post-processing steps if required\n",
        "      post_proc_params = getattr(self, 'post_proc_params', None)\n",
        "      if (post_proc_params is not None):\n",
        "        for post_proc_param in post_proc_params:\n",
        "          self._post_proc(dl, ds, in_train, **post_proc_param)\n",
        "\n",
        "    return dl_list\n",
        "\n",
        "  # ----------------------------\n",
        "  # Do a data post-processing step using one batch of data that we fetch \n",
        "  # from the DataLoader\n",
        "  #\n",
        "  # NB: Right now the only use case for this is to do normalisation for image datasets\n",
        "  # by calculating the mean and std on a batch of data. So below, we make the assumption\n",
        "  # that we should fetch only one batch of data, do post processing on only the 'x' \n",
        "  # ItemList and don't get any return values from the post-processing step. If we\n",
        "  # find any other use case where those assumptions are not valid, we will have to\n",
        "  # change the logic below as required.\n",
        "  # ----------------------------\n",
        "  def _post_proc(self, dl, ds, in_train, proc_procedure, **kwargs):\n",
        "    # Fetch a batch of data from the Dataloader\n",
        "    xb, yb = next(iter(dl))\n",
        "    # Get the x and y ItemLists\n",
        "    x_il, y_il = ds.x, ds.y\n",
        "\n",
        "    # Call the post-processing step. If this is a training dataset, the method may\n",
        "    # add arguments into the 'kwargs', so that they can be passed to the validation\n",
        "    # datasets.\n",
        "    proc_method = getattr(x_il, proc_procedure)\n",
        "    assert(proc_method is not None)\n",
        "    proc_method(xb, in_train, **kwargs)\n",
        "\n",
        "  #-------------------------\n",
        "  # Create a DataLoader from a Dataset using the given parameters\n",
        "  # ----------------------------\n",
        "  def _create_dl(self, ds, in_train, bs=64, shuffle=False, sampler_fn=None, collate_fn=None, **kwargs):\n",
        "    sampler = self.get_sampler(ds, in_train, bs, sampler_fn, **kwargs)\n",
        "    dl = DataLoader(ds, batch_size=bs, shuffle=shuffle, sampler=sampler, collate_fn=collate_fn)\n",
        "    return dl\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get a Sampler for the DataLoader\n",
        "  # We make this a separate method to allow subclass DataBundles to override with a custom method\n",
        "  # ----------------------------\n",
        "  def get_sampler(self, ds, in_train, bs, sampler_fn, **kwargs):\n",
        "    sampler = sampler_fn(ds) if sampler_fn is not None else None\n",
        "    return sampler\n",
        "\n",
        "  # ----------------------------\n",
        "  # Temporary utility to print the contents of a Dataset\n",
        "  # ----------------------------\n",
        "  def _print_ds(self, ds_list):\n",
        "    for ds, _ in ds_list:\n",
        "      print ('Final', ds.x, '\\n', ds.y)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Display a batch of data from the given Dataset. Use the given indexes for the\n",
        "  # data rows to display, or pick them randomly\n",
        "  # ----------------------------\n",
        "  def display_batch(self, ds_type='train', idxs=None, **kwargs):\n",
        "    # Get the Dataset\n",
        "    ds = getattr(self, f'{ds_type}_ds')\n",
        "\n",
        "    # Pick random indexes if none are provided\n",
        "    if (idxs is None):\n",
        "      all_items = range(len(ds))\n",
        "      num_batch = 16\n",
        "      idxs = random.sample(all_items, num_batch)\n",
        "\n",
        "    # Get the display data for all the index rows\n",
        "    # We get back a list of (x, y) tuples - [(x1, y1), (x2, y2),...] which we\n",
        "    # convert into separate 'x' and 'y' lists - [x1, x2, ...] and [y1, y2, ...]\n",
        "    xy_tuples = [ds.get_display_data(i) for i in idxs]\n",
        "    x_display_data, y_display_data = zip(*xy_tuples)\n",
        "\n",
        "    # !!!!!!!! REMOVE\n",
        "    #x_display_data, y_display_data = list(x_display_data), list(y_display_data)\n",
        "    #x_display_data = [ds.x.get_display_data(i) for i in idxs]\n",
        "    #y_display_data = [ds.y.get_display_data(i) for i in idxs]\n",
        "\n",
        "    # Pass the data for display along with any display arguments. Any arguments\n",
        "    # passed in override the pre-define display arguments.\n",
        "    kwargs = {**self.display_params, **kwargs}\n",
        "    self._display(x_display_data, y_display_data, z_display_data=None, **kwargs)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Use the 'DisplayData' class to display (x_feature, y_target, z_predicted) data. This data\n",
        "  # could be training input data (in which case there is no 'z_predicted'), or result data.\n",
        "  # ----------------------------\n",
        "  def _display(self, x_display_data, y_display_data, z_display_data, layout_procedure, xyz_procedures=None, **kwargs):\n",
        "\n",
        "    # DisplayData supports a variety of 'layouts' that are appropriate for different data formats such as images, text\n",
        "    # and tabular data\n",
        "    sd = DisplayData()\n",
        "    layout_method = getattr(sd, layout_procedure)\n",
        "    assert(layout_method is not None)\n",
        "\n",
        "    if (xyz_procedures == 'custom'):\n",
        "      # This is how we handle custom situations, but hopefully we are not going to need\n",
        "      # this. If we do need it, we can decide whether 'xyz_procedures' is the string 'custom'\n",
        "      # or whether it is still a tuple like ('custom', 'image', 'custom') which might be overkill\n",
        "      pass\n",
        "    elif (xyz_procedures is not None):\n",
        "      # 'xyz_procedures' are used by image data to display different types of image-related\n",
        "      # artifacts such as images, segmentation masks, bounding boxes and key points.\n",
        "      assert(isinstance(xyz_procedures, tuple) and (len(xyz_procedures) == 3))\n",
        "      layout_method(x_display_data, y_display_data, z_display_data, xyz_procedures, **kwargs)\n",
        "    else:\n",
        "      # Display the data\n",
        "      layout_method(x_display_data, y_display_data, z_display_data, **kwargs)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Display a batch of results from the given (x, y, z) data. This is model data\n",
        "  # in tensor format rather than ItemLists, so convert it into human-displayable\n",
        "  # format. Use the given indexes for the rows to display, or pick them randomly.\n",
        "  # ----------------------------\n",
        "  def display_results(self, x_objs, y_objs, pred_objs, idxs=None, **kwargs):\n",
        "    # Pick random indexes if none are provided\n",
        "    if (idxs is None):\n",
        "      all_items = range(len(x_objs))\n",
        "      num_batch = 8\n",
        "      idxs = random.sample(all_items, num_batch)\n",
        "\n",
        "    # We just need the Dataset class, not the instance, so that the appropriate methods \n",
        "    # for the type of data are called. So either train or valid Dataset is fine.\n",
        "    ds = self.train_ds\n",
        "\n",
        "    # Convert the data from model-readable object format to human-displayable\n",
        "    # format. We get back three separate 'x', 'y' and 'z' lists - [x1, x2, ...],\n",
        "    # [y1, y2, ...] and [z1, z2, ...]\n",
        "    x_display_data = [ds.x.get_display_data(x_objs[i], 0) for i in idxs]\n",
        "    y_display_data = [ds.y.get_display_data(y_objs[i], 0) for i in idxs]\n",
        "    pred_display_data = [ds.y.get_display_data(pred_objs[i], 0) for i in idxs]\n",
        "\n",
        "    # Pass the data for display along with any display arguments. Any arguments\n",
        "    # passed in override the pre-define display arguments.\n",
        "    kwargs = {**self.display_params, **kwargs}\n",
        "    self._display(x_display_data, y_display_data, pred_display_data, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMdTYjgCc9cf"
      },
      "source": [
        "### Item Container"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLykNp4qdK5g"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Base class for a Container which loads data rows\n",
        "#----------------------------------------------------\n",
        "class ItemContainer:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def load(self, target_cls):\n",
        "    print ('ItemContainer load not implemented')\n",
        "\n",
        "#----------------------------------------------------\n",
        "# A Folder is a Container where each file under the root 'folder_path' represents an individual\n",
        "# data row. The list of files can be filtered by selecting only specific sub-folders directly under the\n",
        "# folder_path (ie. one level under it) and specific file extensions.\n",
        "#----------------------------------------------------\n",
        "class FolderItemContainer(ItemContainer):\n",
        "  def __init__(self, folder_path, include_subfolders=None, recursive=True, extensions=None):\n",
        "    self.folder_path = Path(folder_path)\n",
        "    if (include_subfolders is not None):\n",
        "      # Make a set of the included sub folders one level under the folder_path\n",
        "      self.include_subfolders = set ([self.folder_path / sub_folder for sub_folder in include_subfolders])\n",
        "    else:\n",
        "      self.include_subfolders = None\n",
        "    self.recursive = recursive\n",
        "    self.extensions = extensions\n",
        "    super().__init__()\n",
        "\n",
        "  # ----------------------------\n",
        "  # Return an ItemList of type 'target_cls' using a list of all file names under the \n",
        "  # folder_path that satisfy the filter criteria\n",
        "  # NB: Right now the recursive flag is not used and defaults to True\n",
        "  # ----------------------------\n",
        "  def load(self, target_cls):\n",
        "    # Read all files recursively under the folder\n",
        "    file_list = []\n",
        "    self._get_files(self.folder_path, file_list)\n",
        "\n",
        "    # Create the Item List with the list of files\n",
        "    all_rows = target_cls(file_list, self.folder_path)\n",
        "    return all_rows\n",
        "\n",
        "  # ----------------------------\n",
        "  # Recursive function that reads all files under the folder_path and adds their paths\n",
        "  # to the 'file_list'\n",
        "  # ----------------------------\n",
        "  def _get_files(self, folder_path, file_list):\n",
        "    # NB: Use the folder_path argument that is passed in recursively. Do not use self.folder_path\n",
        "\n",
        "    # Loop through all files in the folder_path\n",
        "    scan_list = os.scandir(folder_path) \n",
        "    for o in scan_list:\n",
        "\n",
        "      if (o.is_dir() and self._is_included(o)):\n",
        "        # Recursively process any sub-directories which are included based on the filter criteria\n",
        "        self._get_files(o.path, file_list)\n",
        "      elif (o.is_file()):\n",
        "        # Add any files that have the required extension\n",
        "        op = Path(o.path)\n",
        "        if ((self.extensions is None) or (op.suffix.lower() in self.extensions)):\n",
        "          file_list.append(op)\n",
        "\n",
        "    scan_list.close()\n",
        "\n",
        "  # ----------------------------\n",
        "  # Return True if the current 'dir_obj' directory meets the 'include_subfolders' filter\n",
        "  # criteria.\n",
        "  #\n",
        "  # If a list of 'include_subfolders' is defined, check whether the current directory \n",
        "  # or any of its ancestors is in that list\n",
        "  # ----------------------------\n",
        "  def _is_included(self, dir_obj):\n",
        "    assert(dir_obj.is_dir())\n",
        "\n",
        "    # Return True if no criteria is defined\n",
        "    if (self.include_subfolders is None):\n",
        "      return True\n",
        "    \n",
        "    # Check if any of the ancestors of the current directory overlaps with\n",
        "    # the list of included sub folders.\n",
        "    dir_p = Path(dir_obj.path)\n",
        "    # Set of current directory and its ancestors\n",
        "    dir_parents = set(list(dir_p.parents) + [dir_p])\n",
        "    if (bool(self.include_subfolders & dir_parents)):\n",
        "      return True\n",
        "\n",
        "    return False\n",
        "\n",
        "#----------------------------------------------------\n",
        "# A CSV file is a Container where each line in the file represents an individual\n",
        "# data row. In the simple case, only a 'csv_path' is provided which it loads into\n",
        "# a Pandas DfItemList.\n",
        "#\n",
        "# In the advanced case, it loads a TabularItemList. Here, we could optionally be \n",
        "# given a test CSV file, additional CSV files with related data rows, and \n",
        "# a 'prepare_fn' to do any data cleanup after loading.\n",
        "#----------------------------------------------------\n",
        "class CSVItemContainer(ItemContainer):\n",
        "  def __init__(self, csv_path, test_csv_path=None, related_csv_paths=None, prepare_fn=None):\n",
        "    # Primary CSV file\n",
        "    self.csv_path = csv_path\n",
        "\n",
        "    # CSV file for test data\n",
        "    self.test_csv_path = test_csv_path\n",
        "\n",
        "    # CSV files for related data\n",
        "    assert((related_csv_paths is None) or isinstance(related_csv_paths, list))\n",
        "    self.related_csv_paths = related_csv_paths\n",
        "\n",
        "    # Data preparation function\n",
        "    self.prepare_fn = prepare_fn\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "  # ----------------------------\n",
        "  # Return a DfItemList by using Pandas to read the CSV file into a Dataframe\n",
        "  # ----------------------------\n",
        "  def _load_csv(self, target_cls, is_test=False):\n",
        "    # Load related CSV data (for Tabular only)\n",
        "    if (self.related_csv_paths):\n",
        "      related_dfs = {rp.stem: target_cls.open_csv(rp) for rp in self.related_csv_paths}\n",
        "    else:\n",
        "      related_dfs = None\n",
        "\n",
        "    # Load the CSV data and create the ItemList\n",
        "    csv_file = self.csv_path if not is_test else self.test_csv_path\n",
        "    df = target_cls.open_csv(csv_file)\n",
        "    rows = target_cls(items=None, df=df, related_dfs=related_dfs)\n",
        "\n",
        "    # Data preparation (for Tabular only)\n",
        "    if (self.prepare_fn):\n",
        "      rows = self.prepare_fn(rows, is_test=is_test)\n",
        "\n",
        "    return rows\n",
        "\n",
        "  # ----------------------------\n",
        "  # Return a DfItemList by using Pandas to read the CSV file into a Dataframe\n",
        "  # ----------------------------\n",
        "  def load(self, target_cls):\n",
        "    assert(target_cls in [DfItemList, TabularItemList])\n",
        "\n",
        "    # Load primary data (For Tabular, also load related data and run data preparation)\n",
        "    all_rows = self._load_csv(target_cls)\n",
        "\n",
        "    # Test CSV file goes through the identical steps to the primary CSV file including\n",
        "    # the same related dfs and data preparation\n",
        "    if (self.test_csv_path):\n",
        "      test_rows = self._load_csv(target_cls, is_test=True)\n",
        "      # Save the test data with the primary ItemList object\n",
        "      all_rows.add_test_rows(test_rows)\n",
        "\n",
        "    return all_rows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvmoFp9RdTUe"
      },
      "source": [
        "### ItemList"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efqK2JuwdXeN"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Base class for an ItemList which holds a list of items of a certain data format\n",
        "# Operations on the ItemList are: Split, Extract, Convert and Transform\n",
        "# Transforms are applied dynamically when you get an item from the ItemList\n",
        "#----------------------------------------------------\n",
        "class ItemList(ListContainer):\n",
        "  def __init__(self, items, **kwargs):\n",
        "    # Used when we split one ItemList into three subsets. The '_copy_new' list contains all\n",
        "    # attributes of the source ItemList which need to be copied as initialisation arguments \n",
        "    # into each of the subset ItemLists\n",
        "    self._copy_new = []\n",
        "\n",
        "    self.xforms = None\n",
        "    super().__init__(items)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split the data sequentially (as opposed to randomly) based on a percentage ratio\n",
        "  # ----------------------------\n",
        "  def split_sequential(self, train_ratio, valid_ratio, test_ratio=0.0):\n",
        "    return self._split_ratio(train_ratio, valid_ratio, test_ratio, randomly=False)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split randomly into three buckets based on a percentage ratio\n",
        "  # ----------------------------\n",
        "  def split_random(self, train_ratio, valid_ratio, test_ratio=0.0):\n",
        "    return self._split_ratio(train_ratio, valid_ratio, test_ratio, randomly=True)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Internal function to split the data based on a percentage ratio. We can put\n",
        "  # data into each bucket either randomly or sequentially\n",
        "  # ----------------------------\n",
        "  def _split_ratio(self, train_ratio, valid_ratio, test_ratio=0.0, randomly=True):\n",
        "    # We are allowed to choose ratios that don't add up to 100%\n",
        "    assert(train_ratio + valid_ratio + test_ratio <= 1.0)\n",
        "\n",
        "    # Make a list of row indexes\n",
        "    num_idxs = len(self)\n",
        "    idxs = list(range(num_idxs))\n",
        "\n",
        "    # Shuffle them randomly if required. Otherwise they are in sequential order\n",
        "    if (randomly):\n",
        "      random.shuffle(idxs)\n",
        "  \n",
        "    # Use the percentage ratio to calculate the number of rows going into each bucket.\n",
        "    # Take the first 'n' indexes for training, the next 'm' for validation\n",
        "    # and the next 'p' for test\n",
        "    train_num_idxs = math.ceil(num_idxs * train_ratio)\n",
        "    valid_num_idxs = math.floor(num_idxs * valid_ratio)\n",
        "    assert(train_num_idxs + valid_num_idxs <= num_idxs)\n",
        "    train_idxs = idxs[:train_num_idxs]\n",
        "    valid_idxs = idxs[train_num_idxs:train_num_idxs + valid_num_idxs]\n",
        "    test_idxs = None\n",
        "    if (test_ratio > 0.0):\n",
        "      test_num_idxs = math.floor(num_idxs * test_ratio)\n",
        "      assert(train_num_idxs + valid_num_idxs + test_num_idxs <= num_idxs)\n",
        "      test_idxs = idxs[train_num_idxs + valid_num_idxs:train_num_idxs + valid_num_idxs + test_num_idxs]\n",
        "\n",
        "    # Get the ItemLists of rows corresponding to the indexes in each bucket\n",
        "    train_rows, valid_rows, test_rows = self.split_idxs(train_idxs=train_idxs, valid_idxs=valid_idxs, test_idxs=test_idxs)\n",
        "\n",
        "    return (train_rows, valid_rows, test_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split based on a given list of indexes for each bucket\n",
        "  # ----------------------------\n",
        "  def split_idxs(self, train_idxs, valid_idxs, test_idxs=None):\n",
        "    # Get the list of items based on the indexes in each bucket\n",
        "    \n",
        "    # !!!!!!!!!!! See if we can get this to work and get rid of the \n",
        "    # function _get_item_for_split_wo_xform\n",
        "    #train_items = self.items[train_idxs]\n",
        "    #valid_items = self.items[valid_idxs]\n",
        "    #test_items = self.items[test_idxs] if (test_idxs is not None) else None\n",
        "\n",
        "    train_items = self._get_item_for_split_wo_xform(train_idxs)\n",
        "    valid_items = self._get_item_for_split_wo_xform(valid_idxs)\n",
        "    test_items = self._get_item_for_split_wo_xform(test_idxs) if (test_idxs is not None) else None\n",
        "\n",
        "    # Get the ItemLists of rows corresponding to the items in each bucket\n",
        "    train_rows, valid_rows, test_rows = self.split_list(train_items, valid_items, test_items)\n",
        "\n",
        "    return (train_rows, valid_rows, test_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split based on a given Python list of items for each bucket\n",
        "  # ----------------------------\n",
        "  def split_list(self, train_item_list, valid_item_list, test_item_list=None):\n",
        "     # Clone myself (viz. the source ItemList) to get three ItemLists using the\n",
        "    # list of items in each bucket\n",
        "    train_rows = self.clone(train_item_list)\n",
        "    valid_rows = self.clone(valid_item_list)\n",
        "    test_rows = self.clone(test_item_list)\n",
        "\n",
        "    return (train_rows, valid_rows, test_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split based on a custom provided function\n",
        "  # ----------------------------\n",
        "  def split_custom(self, custom_fn, **kwargs):\n",
        "    print ('ItemList split_custom', custom_fn)\n",
        "    train_rows, valid_rows, test_rows = custom_fn(**kwargs)\n",
        "    return (train_rows, valid_rows, test_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split based on a folder path - only applies to file-path-based ItemLists\n",
        "  # ----------------------------\n",
        "  def split_path(self, train_folder, valid_folder, test_folder=None):\n",
        "    print ('ItemList split_path not implemented')\n",
        "\n",
        "  # ----------------------------\n",
        "  # Set the list of dynamic transforms\n",
        "  # ----------------------------\n",
        "  def add_xform(self, xform_x_params):\n",
        "    self.xforms = xform_x_params\n",
        "\n",
        "  # ----------------------------\n",
        "  # Make a clone ItemList with the same initialisation arguments as myself but based on\n",
        "  # the given Python list of items. Optionally, we can be given some extra arguments to\n",
        "  # be used for initialisation. ItemLists that store data in backend systems rather than\n",
        "  # in the list of 'items' could use those extra arguments for initialisation, instead \n",
        "  # of or in addition to the items themselves.\n",
        "  # ----------------------------\n",
        "  def clone(self, items, extra_args={}, **kwargs):\n",
        "    # To prepare the clone, we need either a new set of items, or some extra arguments.\n",
        "    # If neither are given, there is nothing to do.\n",
        "    if (not items and not extra_args):\n",
        "      return (None)\n",
        "\n",
        "    # Keyword arguments for initialisation using the attributes to be copied over \n",
        "    # to the cloned ItemList\n",
        "    copy_args = {k:getattr(self, k, None) for k in self._copy_new}\n",
        "    # Add the list of items to the keyword arguments, along with the copied attributes.\n",
        "    # The extra arguments are merged last as they should override the copies attributes.\n",
        "    kwargs = {'items': items, **copy_args, **extra_args}\n",
        "    # Now create a new instance of my ItemList type using the keyword arguments\n",
        "    il = self.__class__(**kwargs)\n",
        "\n",
        "    return (il)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Make a clone ItemList with the same initialisation arguments as myself but based on\n",
        "  # the given Python list of items \n",
        "  # !!!!!!!!!!!\n",
        "  # ----------------------------\n",
        "  def OBSOLETE_new(self, items):\n",
        "    if ((items is None) or len(items) == 0):\n",
        "      return (None)\n",
        "    \n",
        "    # Keyword arguments for initialisation using the attributes to be copied over \n",
        "    # to the cloned ItemList\n",
        "    copy_args = {k:getattr(self, k, None) for k in self._copy_new}\n",
        "    # Add the list of items to the keyword arguments\n",
        "    kwargs = {'items': items, **copy_args}\n",
        "    # Now create a new instance of my ItemList type using the keyword arguments\n",
        "    il = self.__class__(**kwargs)\n",
        "\n",
        "    return (il)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Use 'convert_procedure' to convert myself to a 'target_cls' ItemList\n",
        "  # The 'in_train' flag is true if this is training data. 'convert_state' dict can be\n",
        "  # used to save state generated while processing training data, so that we can refer to\n",
        "  # it later while processing validation or test data.\n",
        "  # ----------------------------\n",
        "  def convert(self, convert_state, in_train, target_cls, convert_procedure, **kwargs):\n",
        "    if (convert_procedure == 'Custom'):\n",
        "      # Use a custom converter\n",
        "      convert_method = self.convert_custom\n",
        "    else:\n",
        "      # Get the convert method\n",
        "      convert_method = getattr(self, convert_procedure, None)\n",
        "      assert(convert_method is not None)\n",
        "\n",
        "\n",
        "    # Invoke the convert method on all items to get a Python list of target items\n",
        "    items = self.items[:]\n",
        "    target_items = convert_method(items=items, convert_state=convert_state, in_train=in_train, **kwargs)\n",
        "\n",
        "    if (target_cls != self.__class__):\n",
        "      # Create a new 'target_cls' ItemList using the target items\n",
        "      target_kwargs = {**kwargs, **convert_state}\n",
        "      target_il = target_cls(target_items, **target_kwargs)\n",
        "      assert(target_il.__class__ == target_cls)\n",
        "    else:\n",
        "      # If the target class is the same as our ItemList's existing class, we assume\n",
        "      # that the convert method has completed the conversion, and there is nothing\n",
        "      # further to do. Just return ourselves.\n",
        "      target_il = self\n",
        "        \n",
        "    print (\"Converted %s items to type %s using %s\" % (len(target_il), target_il.__class__.__name__, convert_procedure))\n",
        "    return (target_il)\n",
        " \n",
        "  # ----------------------------\n",
        "  # Convert items using the given 'custom_fn'\n",
        "  # ----------------------------\n",
        "  def convert_custom(self, items, convert_state, in_train, custom_fn, **kwargs):\n",
        "    print ('ItemList convert_custom')\n",
        "\n",
        "  # ----------------------------\n",
        "  # Extract items (either 'x' or 'y') using the given 'custom_fn'\n",
        "  # ----------------------------\n",
        "  def extract_custom(self, custom_fn, **kwargs):\n",
        "    # Apply the custom function to every item to produce a Python list\n",
        "    y_list = list(map(custom_fn, self[:]))\n",
        "    return (y_list)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Extract items using the a given column\n",
        "  # Implemented in the subclass\n",
        "  # !!!!!! 'col' should be a list not a single column\n",
        "  # ----------------------------\n",
        "  def extract_colval(self, col):\n",
        "    y_list = None\n",
        "    print ('ItemList extract_colval not implemented')\n",
        "    return (y_list)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Some ItemLists may store the 'real' data in another backend system eg. a Dataframe\n",
        "  # or a database. Those ItemLists would then override this method to map the 'item' in \n",
        "  # the ItemList to the real data 'object'. For all other ItemLists the default behaviour \n",
        "  # is that the data object is the same as the item.\n",
        "  # ----------------------------\n",
        "  def _get_object(self, item): return item\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform items dynamically using the given 'xform_procedure'\n",
        "  # ----------------------------\n",
        "  def _do_xform(self, obj, xform_procedure, **kwargs):\n",
        "    # Get the transform method and invoke it\n",
        "    xform_method = getattr(self, xform_procedure, None)\n",
        "    assert(xform_method is not None)\n",
        "    obj = xform_method(obj, **kwargs)\n",
        "    return (obj)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Dynamically apply any defined transforms on an item and return the resulting object\n",
        "  # ----------------------------\n",
        "  def _xform(self, obj):\n",
        "    # Apply transforms on that object. The output of each transform becomes the input\n",
        "    # for the next transform\n",
        "    if (self.xforms is not None):\n",
        "      for xform in self.xforms:\n",
        "        obj = self._do_xform(obj, **xform)\n",
        "\n",
        "    return obj\n",
        "\n",
        "  # ----------------------------\n",
        "  # The return is either a paired tuple or a list of paired tuples\n",
        "  # [(obj, paired_obj), (obj, paired_obj), ...]\n",
        "  # ----------------------------\n",
        "  def get_paired_items(self, paired_objs, idxs):\n",
        "    items = super().__getitem__(idxs)\n",
        "\n",
        "    if isinstance(items,list):\n",
        "      assert (len(items) == len(paired_objs))\n",
        "\n",
        "      # Return a list of objects after applying transforms on each one\n",
        "      objs = [(self._get_object(i), po) for i, po in zip(items, paired_objs)]\n",
        "      return [self._xform(o) for o in objs]\n",
        "    else:\n",
        "      # Return a single object after applying transforms on it\n",
        "      obj = (self._get_object(items), paired_objs)\n",
        "      return self._xform(obj)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Built-in dunder method to get an object or a list of objects from the ItemList, using\n",
        "  # standard indexing and slicing. So 'idx' can be either a single index, a range or a slice\n",
        "  # We get the data object, apply transforms on it and return the result\n",
        "  # The return is an object or a list of objects\n",
        "  # [obj, obj, ...]\n",
        "  # ----------------------------\n",
        "  def __getitem__(self, idxs):\n",
        "    # Get the item (or list of items) corresponding to the given indices\n",
        "    items = super().__getitem__(idxs)\n",
        "    \n",
        "    if isinstance(items,list):\n",
        "      # Return a list of objects after applying transforms on each one\n",
        "      objs = [self._get_object(i) for i in items]\n",
        "      return [self._xform(o) for o in objs]\n",
        "    else:\n",
        "      # Return a single object after applying transforms on it\n",
        "      obj = self._get_object(items)\n",
        "      return self._xform(obj)\n",
        "\n",
        "  # ----------------------------\n",
        "  # While doing a split, we want to keep the original raw item without any transforms \n",
        "  # rather than the data object. So we skip the above __getitem__ method (which\n",
        "  # gets the data object and transforms it)\n",
        "  # !!!!!!!!!!!! There is no need for this special function to bypass the transforms\n",
        "  # Instead of doing self[idx], you can do self.items[idx] which has the same effect\n",
        "  # ----------------------------\n",
        "  def _get_item_for_split_wo_xform(self, idx):\n",
        "    # Get the raw item using the parent ListContainer class and return it without\n",
        "    # applying any transforms\n",
        "    raw = super().__getitem__(idx)\n",
        "    return (raw)\n",
        "\n",
        "  def get_display_data(self, obj, idx):\n",
        "    return obj"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27FW5yod2oh"
      },
      "source": [
        "### File ItemLists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDJ3dCH5eCNH"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# An ItemList of File Paths, which are relative to 'folder_path'\n",
        "# It can have subclasses for specific file types such as Image Files, Audio Files, Text Files etc\n",
        "# It implements Split and Extract methods that are relevant to files\n",
        "#----------------------------------------------------\n",
        "class FileNameItemList(ItemList):\n",
        "  def __init__(self, items, folder_path, **kwargs):\n",
        "    file_list = [folder_path / Path(item) for item in items]\n",
        "    # File paths in 'file_list' are relative to 'folder_path'\n",
        "    self.folder_path = folder_path\n",
        "    super().__init__(file_list)\n",
        "    self._copy_new.append('folder_path')\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split files into training, validation and test based on the sub-folder in which they reside\n",
        "  # The 'train_folder' (and 'valid_folder' and 'test_folder') are relative to the base\n",
        "  # 'folder_path' of the ItemList\n",
        "  # ----------------------------\n",
        "  def split_path(self, train_folder, valid_folder, test_folder=None):\n",
        "    train_file_list, valid_file_list, test_file_list = [], [], []\n",
        "    sub_folders = [train_folder, valid_folder, test_folder]\n",
        "    file_lists = [train_file_list, valid_file_list, test_file_list]\n",
        "\n",
        "    # Check each file path item in our list to see which sub folder it resides and\n",
        "    # add it to the appropriate file list for training, validation and test\n",
        "    for file_path in self._get_item_for_split_wo_xform(slice(None, None)):\n",
        "      self.in_sub_folder(file_path, self.folder_path, zip(sub_folders, file_lists))\n",
        "\n",
        "    # Use the list of items for training, validation and test to create three ItemLists \n",
        "    # Necessary attributes are copied over from the source ItemList\n",
        "    train_rows = self.clone(train_file_list)\n",
        "    valid_rows = self.clone(valid_file_list)\n",
        "    test_rows = self.clone(test_file_list)\n",
        "\n",
        "    return (train_rows, valid_rows, test_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get the n-th level ancestor folder of the given 'file_path' \n",
        "  #\n",
        "  # !!!!!!!!! Add a built-in extract procedure 'extract_ancestor' which only works for FileName item lists\n",
        "  # rather than use a custom function\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def get_ancestor(file_path, up_level=1):\n",
        "    sub_folder_path = file_path.parents[up_level - 1].name\n",
        "    return (sub_folder_path)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Check which sub-folder the 'file_path' resides in and add it to the corresponding list\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def in_sub_folder(file_path, folder_path, zip_folder_files):\n",
        "    # 'zip_folder_files' is [(sub_folder_tr, file_list_tr), (sub_folder_val, file_list_val), ...]\n",
        "    # For each sub-folder, check if 'file_path' resides in 'folder_path' / 'sub_folder' and add\n",
        "    # it to the corresponding file_list\n",
        "    for sub_folder, file_list in zip_folder_files:\n",
        "      if (sub_folder is not None):\n",
        "        sub_folder_path = folder_path / sub_folder\n",
        "        if (sub_folder_path in file_path.parents):\n",
        "          file_list.append(file_path)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Subclass of File Name Item List for Image Files\n",
        "# Doesn't do much by itself, most of the file-specific functionality is in the parent\n",
        "# class and the image-specific functionality is contained in ImageItemList\n",
        "#----------------------------------------------------\n",
        "class ImageFileItemList(FileNameItemList):\n",
        "  def FileToImage(self, items, convert_state, in_train, **kwargs):\n",
        "    imagefile_items = items\n",
        "    image_items = imagefile_items\n",
        "    return (image_items)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Subclass of File Name Item List for Text Files\n",
        "#----------------------------------------------------\n",
        "class TextFileItemList(FileNameItemList):\n",
        "  # Read the entire text file as one document into a single sentence\n",
        "  @staticmethod\n",
        "  def _read_doc(textfile_name):\n",
        "    with open(textfile_name, 'r', encoding = 'utf8') as f: return f.read()\n",
        "\n",
        "  # Read the text file line by line into a list of sentences\n",
        "  @staticmethod\n",
        "  def _read_sentence(textfile_name):\n",
        "    print ('To be implemented - read the text file line by line')\n",
        "    return(0)\n",
        "\n",
        "  # ----------------------------\n",
        "  # The entire doc in the text file becomes a single sentence item\n",
        "  # ----------------------------\n",
        "  def extract_doc(self, **kwargs):\n",
        "    y_list = [self.__class__._read_doc(t) for t in self[:]]\n",
        "    return (y_list)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Each line in the text file becomes a separate sentence item\n",
        "  # ----------------------------\n",
        "  def extract_sentence(self):\n",
        "    y_list = [self.__class__._read_sentence(t) for t in self[:]]\n",
        "    return (y_list)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Dummy extraction\n",
        "  # ----------------------------\n",
        "  def extract_dummy(self, **kwargs):\n",
        "    y_list = [0] * len(self)\n",
        "    return (y_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mr-qWSvUd5je"
      },
      "source": [
        "### Image ItemLists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM4H2zPUeGe7"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList for Images. Each item is an image file, which is dynamically loaded. It also\n",
        "# implements several transforms which can then be applied in sequence to the loaded image.\n",
        "#----------------------------------------------------\n",
        "class ImageItemList(ItemList):\n",
        "  def __init__(self, items, img_type='ocv', pair_type=None, **kwargs):\n",
        "    self.img_type = img_type\n",
        "    self.pair_type = pair_type\n",
        "    self.mean = None\n",
        "    self.std = None\n",
        "    super().__init__(items, **kwargs)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Load the file as an image\n",
        "  # ----------------------------\n",
        "  def _get_object(self, item):\n",
        "    if (self.img_type == 'ocv'):\n",
        "      return OcvImg.ocv_open(item)\n",
        "      #return PIL.Image.open(item)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Denormalise the tensor for display\n",
        "  # ----------------------------\n",
        "  def get_display_data(self, obj, idx):\n",
        "    obj = self.denormalise(obj)\n",
        "    return obj\n",
        "\n",
        "  # ----------------------------\n",
        "  # Decorator used with all transforms to handle paired input objects and paired\n",
        "  # return values\n",
        "  #\n",
        "  # Since Image transforms support paired objects, the generic \n",
        "  # Item List's transform processing expects every Image transform to work in \n",
        "  # two modes:\n",
        "  #   1. It gets passed a single image object 'img' and returns a single \n",
        "  #      transformed image 'img'\n",
        "  #   2. It gets passed an (image, paired_image) tuple in the 'img' argument and \n",
        "  #      returns a transformed (image, paired_image) 'img' tuple\n",
        "  #\n",
        "  # In other words, the same 'img' argument can be either a single image or a tuple.\n",
        "  #\n",
        "  # Rather than requiring every transform to include the logic for the handling\n",
        "  # the two modes, this logic is provided by this decorator. This simplifies every\n",
        "  # transform so they can all work in one mode only:\n",
        "  #   a) They always receive separate 'img' and 'img_pair' arguments. The 'img_pair' can\n",
        "  #   be None.\n",
        "  #   b) They always return separate transformed 'img' and 'img_pair' values. The\n",
        "  #   returned 'img_pair' is None if the input 'img_pair' was None\n",
        "  #\n",
        "  # The decorator does the mapping between what the two modes that the generic \n",
        "  # processing expects and the single mode that the transform function supports.\n",
        "  # ----------------------------\n",
        "  def paired_xform(xform):\n",
        "    # Note that the method above cannot take a 'self' argument. However the 'self' \n",
        "    # is available within the method and is passed to the 'pair' wrapper function.\n",
        "\n",
        "    def pair(self, obj, *args, **kwargs):\n",
        "      if (isinstance(obj, tuple)):\n",
        "        # If a tuple is passed in, separate it into 'img' and 'img_pair' arguments\n",
        "        img, img_pair = obj\n",
        "      else:\n",
        "        # If a single image is passed in, set the 'img_pair' to None\n",
        "        img, img_pair = obj, None\n",
        "\n",
        "      # Call the transform function\n",
        "      res = xform(self, img, img_pair, *args, **kwargs)\n",
        "\n",
        "      # If a single image was passed in, then return a single image and ignore\n",
        "      # the 'img_pair' in the return value. Note that below we are checking the\n",
        "      # input 'obj' for tupleness, not the 'res' result value. \n",
        "      if (not isinstance(obj, tuple)):\n",
        "        res = res[0]\n",
        "\n",
        "      return res\n",
        "    return pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform for RGB\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def make_rgb(self, img, img_pair):\n",
        "    if (self.img_type == 'ocv'):\n",
        "      img = OcvImg.ocv_rgb(img)\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform for image resize\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def resize(self, img, img_pair, size):\n",
        "    assert(isinstance(size, int))\n",
        "    if (self.img_type == 'ocv'):\n",
        "      img = OcvImg.ocv_resize(img, (size, size))\n",
        "      if (img_pair is not None):\n",
        "        img_pair = OcvImg.ocv_resize(img_pair, (size, size))\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Randomly choose from a set of rotate and flip transforms. We also have the\n",
        "  # probability of doing nothing\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def flip_rotate(self, img, img_pair, types=None, prob=0.75):\n",
        "    if random.random() > prob:\n",
        "      # Do nothing with some probability\n",
        "      return img, img_pair\n",
        "\n",
        "    elif (self.img_type == 'ocv'):\n",
        "      if (types is None):\n",
        "        types = ['rotate_90', 'rotate_180', 'rotate_270', 'fliv_h', 'fliv_v', 'transpose', 'transverse']\n",
        "      fr_type = random.choice(types)\n",
        "      img = OcvImg.ocv_flip_rotate(img, fr_type)\n",
        "      if (img_pair is not None):\n",
        "        img_pair = OcvImg.ocv_flip_rotate(img_pair, fr_type)\n",
        "      return img, img_pair\n",
        "\n",
        "    elif (self.img_type == 'pil'):\n",
        "      if (types is None):\n",
        "        types = ['rotate_90', 'rotate_180', 'rotate_270', 'fliv_h', 'fliv_v', 'transpose', 'transverse']\n",
        "      fr_type = random.choice(types)\n",
        "      img = PilImg.pil_flip_rotate(img, fr_type)\n",
        "      if (img_pair is not None):\n",
        "        img_pair = PilImg.pil_flip_rotate(img_pair, fr_type)\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform for Centre Crop\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def centre_crop(self, img, img_pair, crop_size):\n",
        "    assert(isinstance(crop_size, int))\n",
        "    if (self.img_type == 'ocv'):\n",
        "      img_sz = OcvImg.ocv_shape(img)\n",
        "\n",
        "      # Get the crop coordinates. We will apply those same coordinates to both\n",
        "      # the 'img' and the 'img_pair'\n",
        "      crop_corners = imgaug_random_resized_crop(img_sz, \n",
        "                                  crop_area_range=(0.7694, 0.7694), \n",
        "                                  crop_aspect_range=None, \n",
        "                                  crop_pos='ctr')\n",
        "      \n",
        "      # Crop the image\n",
        "      img = OcvImg.ocv_crop(img, (crop_size, crop_size), crop_corners)\n",
        "\n",
        "      if (img_pair is not None):\n",
        "        # Crop the paired image\n",
        "        img_pair = OcvImg.ocv_crop(img_pair, (crop_size, crop_size), crop_corners)\n",
        "\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform for Random Resized Crop\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def random_resized_crop(self, img, img_pair, crop_size):\n",
        "    assert(isinstance(crop_size, int))\n",
        "    if (self.img_type == 'ocv'):\n",
        "      img_sz = OcvImg.ocv_shape(img)\n",
        "\n",
        "      # Get the crop coordinates. We will apply those same coordinates to both\n",
        "      # the 'img' and the 'img_pair'\n",
        "      crop_corners = imgaug_random_resized_crop(img_sz)\n",
        "\n",
        "      # Crop the image\n",
        "      img = OcvImg.ocv_crop(img, (crop_size, crop_size), crop_corners)\n",
        "\n",
        "      if (img_pair is not None):\n",
        "        # Crop the paired image\n",
        "        img_pair = OcvImg.ocv_crop(img_pair, (crop_size, crop_size), crop_corners)\n",
        "\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform for Perspective Warp\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def perspective_warp(self, img, img_pair, crop_size):\n",
        "    assert(isinstance(crop_size, int))\n",
        "    if (self.img_type == 'ocv'):\n",
        "      img_sz = OcvImg.ocv_shape(img)\n",
        "\n",
        "      # Get the warped coordinates. We will apply those same coordinates to both the\n",
        "      # 'img' and the 'img_pair'. Note that with 'ocv', we use the src_coords and targ_coords\n",
        "      # and ignore the 'coeffs' as they are not in a suitable format for OCV. We let the \n",
        "      # 'ocv_perspective' function calculate the matrix coefficients for the warp in the \n",
        "      # needed format for OCV, by using the src and targ coords.\n",
        "      # On the other hand, With PIL, we use the matrix 'coeffs' directly as they are \n",
        "      # already calculated in the needed format for PIL.\n",
        "      coeffs, src_coords, targ_coords = imgaug_perspective_warp(img_sz, (crop_size, crop_size), magnitude=0.2)\n",
        "\n",
        "      # Warp the image\n",
        "      img = OcvImg.ocv_perspective(img, (crop_size, crop_size), src_coords, targ_coords)\n",
        "\n",
        "      if (img_pair is not None):\n",
        "        # Warp the paired image\n",
        "        img_pair = OcvImg.ocv_perspective(img_pair, (crop_size, crop_size), src_coords, targ_coords)\n",
        "\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Tranform using a variety of augmentations provided by the Albumentations\n",
        "  # library. The 'aug_name' chooses the augmentation we want to use.\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def aug(self, img, img_pair, aug_name, p=0.5):\n",
        "    if (self.img_type == 'ocv'):\n",
        "      img, img_pair = imgaug_albu(img, aug_name, mask=img_pair, p=p)\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform for converting image to a tensor of bytes\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def to_byte_tensor(self, img, img_pair):\n",
        "    if (self.img_type == 'ocv'):\n",
        "      img = OcvImg.ocv_byte_tensor(img)\n",
        "      return img, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform for converting from a byte tensor to float tensor\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def to_float_tensor(self, img_tensor, img_pair):\n",
        "    img_tensor = img_tensor.float().div_(255.)\n",
        "    if (img_pair is not None):\n",
        "      img_pair = tensor(img_pair).div_(255.)\n",
        "    return img_tensor, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Set the mean and std to be used when normalising the image tensor. This is\n",
        "  # called during data post processing, and is not a transform.\n",
        "  #\n",
        "  # mean and std are both lists, with one value per image channel depth.\n",
        "  # If we're given pre-calculated 'mean' and 'std' then use them directly\n",
        "  # If not, then calculate mean and std using the given data batch\n",
        "  # ----------------------------\n",
        "  def set_mean_std(self, datab, in_train, mean=None, std=None, **kwargs):\n",
        "\n",
        "    # We can assume that if mean is None, std will also be None\n",
        "    if ((mean is None) and in_train):\n",
        "        # If this is a training dataset, calculate the mean and std from the \n",
        "        # given batch and save them in the 'kwargs' so that they get passed in the \n",
        "        # 'mean' and 'std' arguments to the validation dataset.\n",
        "        mean = datab.mean()\n",
        "        std = datab.std()\n",
        "        kwargs['mean'] = mean\n",
        "        kwargs['std'] = std\n",
        "    \n",
        "    # Save them for use when doing normalisation transforms\n",
        "    self.mean, self.std = tensor(mean), tensor(std)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform to normalise the image tensor, based on mean and std that were\n",
        "  # pre-calculated earlier during data post processing\n",
        "  # ----------------------------\n",
        "  @paired_xform\n",
        "  def normalise(self, img_tensor, img_pair):\n",
        "    mean, std = self.mean, self.std\n",
        "\n",
        "    # If normalise() is called, mean and std should have been pre-assigned\n",
        "    # and should never be null. But there is one instance when it will be\n",
        "    # null ie. when we fetch a batch from the DataLoader for the very first\n",
        "    # time during data post-processing, in order to calculate the mean and std\n",
        "    # This is a chicken-and-egg situation. So rather than figuring out a way\n",
        "    # to set some flag to differentiate that case, we simply do nothing if\n",
        "    # mean and std is not set. And we assume that, except for that first \n",
        "    # time, the caller has correctly set mean and std.\n",
        "    if (mean is None):\n",
        "      return img_tensor, img_pair\n",
        "\n",
        "    assert(isinstance(img_tensor, torch.Tensor))\n",
        "    assert(isinstance(mean, torch.Tensor) and (mean.size(0) == img_tensor.size(0)))\n",
        "    assert(isinstance(std, torch.Tensor) and (std.size(0) == img_tensor.size(0)))\n",
        "\n",
        "    norm = (img_tensor - mean[...,None,None]) / std[...,None,None]\n",
        "    return norm, img_pair\n",
        "\n",
        "  # ----------------------------\n",
        "  # Denormalise the image tensor for display. This is not a transform\n",
        "  # ----------------------------\n",
        "  def denormalise(self, obj_tensor):\n",
        "    mean, std = self.mean, self.std\n",
        "\n",
        "    if (mean is None):\n",
        "      # Do nothing if the tensor was not normalised\n",
        "      return obj_tensor\n",
        "\n",
        "    assert(isinstance(obj_tensor, torch.Tensor))\n",
        "    assert(isinstance(mean, torch.Tensor) and (mean.size(0) == obj_tensor.size(0)))\n",
        "    assert(isinstance(std, torch.Tensor) and (std.size(0) == obj_tensor.size(0)))\n",
        "\n",
        "    denorm = (obj_tensor * std[...,None,None]) + mean[...,None,None]\n",
        "    return denorm\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn3oqFT0d71O"
      },
      "source": [
        "### Text ItemLists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iW52EoxeGzA"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList where each item is a text sentence\n",
        "# eg. [\"sentence 1\", \"sentence 2\", ..]\n",
        "#----------------------------------------------------\n",
        "#special tokens\n",
        "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
        "def_pre_rules = [\"fixup_text\", \"replace_rep\", \"replace_wrep\", \"spec_add_spaces\", \"rm_useless_spaces\", \"sub_br\"]\n",
        "def_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
        "def_post_rules = [\"deal_caps\", \"replace_all_caps\", \"add_eos_bos\"]\n",
        "\n",
        "class SentenceItemList(ItemList):\n",
        "  def __init__(self, items, lang=\"en\", pre_rules=def_pre_rules, spec_tok=def_spec_tok, post_rules=def_post_rules, **kwargs):\n",
        "    self.pre_rules, self.spec_tok, self.post_rules = pre_rules, spec_tok, post_rules\n",
        "    self.chunksize, self.max_workers = 2000, 4\n",
        "    self.tokenizer = spacy.blank(lang).tokenizer\n",
        "    for w in self.spec_tok:\n",
        "      self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
        "    super().__init__(items)\n",
        "  \n",
        "  @staticmethod\n",
        "  def sub_br(t):\n",
        "    \"Replace <br > or <br /> by \\n\"\n",
        "    # Pattern is '<', then 0 or more spaces, then 'br', then 0 or more spaces, then 0 or 1 '/', then '>'\n",
        "    re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
        "    return re_br.sub(\"\\n\", t)\n",
        "\n",
        "  @staticmethod\n",
        "  def spec_add_spaces(t):\n",
        "    \"Add spaces around / and #\"\n",
        "    return re.sub(r'([#/])', r' \\1 ', t)\n",
        "  \n",
        "  @staticmethod\n",
        "  def rm_useless_spaces(t):\n",
        "    \"Remove multiple spaces\"\n",
        "\n",
        "    # Look for 1 or more occurrences of a space and replace with a single space. This works\n",
        "    # but results in a replacement even for single spaces, which are quite common. So the\n",
        "    # alternate expression below is more efficient\n",
        "    #return re.sub(r'[\\s]+', r' ', t)\n",
        "\n",
        "    # Look for 2 or more occurrences of a space ie. ' ' and replaces with a single space\n",
        "    return re.sub(' {2,}', ' ', t)  \n",
        "\n",
        "  @staticmethod\n",
        "  def replace_rep(t):\n",
        "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
        "\n",
        "    # Use a replacement function to find the number of repeating characters\n",
        "    def replace_func(m):\n",
        "      c = m.group()\n",
        "\n",
        "      # len(c) - number of repeats, c[0] - repeating character\n",
        "      return (f' {TK_REP} {len(c)} {c[0]} ')\n",
        "  \n",
        "    # Find any non-space characters which repeat 4 or more times. \n",
        "    # The (\\S) finds the first character, and then the \\1{3,} looks for 3 more repeats ie. total of 4 repeats = 1 + 3\n",
        "    # NB: Alternately '(\\S)\\1+' - will find repeats of 2 or more times ie. 1 + 1\n",
        "    re_rep = re.compile(r'(\\S)\\1{3,}')\n",
        "    return re_rep.sub(replace_func, t)\n",
        "\n",
        "  @staticmethod\n",
        "  def replace_wrep(t):\n",
        "    \"Replace word repetitions: word word word -> TK_WREP 3 word\"\n",
        "\n",
        "    # Use a replacement function to find the number of repeating words\n",
        "    def replace_func(m):\n",
        "      w = m.group()\n",
        "    \n",
        "      # Make a list of repeating words\n",
        "      ws = w.split()\n",
        "\n",
        "      # len(ws) - number of repeats, ws[0] - repeating word\n",
        "      return (f' {TK_WREP} {len(ws)} {ws[0]} ')\n",
        "  \n",
        "    # Find any words which repeat 4 or more times. \n",
        "    # The (\\b\\w+\\W+) finds an empty string marking a word boundary, followed by multiple word characters, followed by multiple non-word characters\n",
        "    # then the \\1{3,} looks for 3 more repeats ie. total of 4 repeats = 1 + 3\n",
        "    re_rep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
        "    return re_rep.sub(replace_func, t)\n",
        "\n",
        "  @staticmethod\n",
        "  def fixup_text(x):\n",
        "    \"Various messy things we've seen in documents\"\n",
        "    re1 = re.compile(r'  +')\n",
        "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
        "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
        "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
        "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
        "    return re1.sub(' ', html.unescape(x))\n",
        "\n",
        "  @staticmethod\n",
        "  def replace_all_caps(x):\n",
        "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
        "    res = []\n",
        "    for t in x:\n",
        "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
        "        else: res.append(t)\n",
        "    return res\n",
        "\n",
        "  @staticmethod\n",
        "  def deal_caps(x):\n",
        "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
        "    res = []\n",
        "    for t in x:\n",
        "        if t == '': continue\n",
        "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
        "        res.append(t.lower())\n",
        "    return res\n",
        "\n",
        "  @staticmethod\n",
        "  def add_eos_bos(x): return [BOS] + x + [EOS]\n",
        "\n",
        "  #----------------------------------------------------\n",
        "  # Apply all the tokenising rules\n",
        "  # Pre-rules are applied before a sentence is tokenised into words\n",
        "  # Post-rules are applied after\n",
        "  #----------------------------------------------------\n",
        "  def apply_rules(self, sentence, pre):\n",
        "    rules = self.pre_rules if pre else self.post_rules\n",
        "    for rule in rules:\n",
        "      rule_method = getattr (self.__class__, rule, None)\n",
        "      assert(rule_method is not None)\n",
        "      sentence = rule_method(sentence)\n",
        "\n",
        "    return (sentence)\n",
        "\n",
        "  #----------------------------------------------------\n",
        "  # Break a sentence into word tokens\n",
        "  #----------------------------------------------------\n",
        "  def tokenise_sentence(self, args):\n",
        "    i, sentence_items = args\n",
        "    sentence_items = [self.apply_rules(sentence, pre=True) for sentence in sentence_items]\n",
        "    sentence_word_items = [[d.text for d in doc] for doc in self.tokenizer.pipe(sentence_items)]\n",
        "    sentence_word_items = [self.apply_rules(sentence, pre=False) for sentence in sentence_word_items]\n",
        "    return (sentence_word_items)\n",
        "\n",
        "  #----------------------------------------------------\n",
        "  # \n",
        "  #----------------------------------------------------\n",
        "  def SentenceToWord(self, items, convert_state, in_train, **kwargs):\n",
        "    toks = []\n",
        "    sentence_items = items\n",
        "    chunks = [sentence_items[i: i+self.chunksize] for i in (range(0, len(sentence_items), self.chunksize))]\n",
        "    toks = parallel(self.tokenise_sentence, chunks, max_workers=self.max_workers)\n",
        "    return sum(toks, [])\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList where each item is a list of text words corresponding to one sentence\n",
        "# eg. [[\"word1\", \"word2\", \"word3\"], [\"word4\", \"word5\", \"word6\", \"word7\"], ...]\n",
        "#----------------------------------------------------\n",
        "class SentenceWordItemList(ItemList):\n",
        "  def __init__(self, items, **kwargs):\n",
        "    self.max_vocab, self.min_freq = 60000, 2\n",
        "    self.spec_tok = def_spec_tok\n",
        "    super().__init__(items)\n",
        "\n",
        "  #----------------------------------------------------\n",
        "  # Numericalise the word tokens into word IDs using a vocab of words to IDs\n",
        "  # We build the vocab during training (ie. in_train is True) unless a pre-created vocab\n",
        "  # is passed in. We save the vocab in the 'convert_state', so we can refer to it later.\n",
        "  # During validation, we use the saved vocab that was built during training.\n",
        "  #----------------------------------------------------\n",
        "  def WordToWordId(self, items, convert_state, in_train, vocab_i2w=None, **kwargs):\n",
        "    sentence_word_items = items\n",
        "    if (in_train):\n",
        "      if (vocab_i2w is None):\n",
        "        # Build a vocab of unique words and save it for use during validation processing\n",
        "        freq = Counter(word for sentence in sentence_word_items for word in sentence)\n",
        "        vocab_i2w = [word for word,count in freq.most_common(self.max_vocab) if count >= self.min_freq]\n",
        "        for tok in reversed(self.spec_tok):\n",
        "          if tok in vocab_i2w: vocab_i2w.remove(tok)\n",
        "          vocab_i2w.insert(0, tok)\n",
        "      convert_state['vocab_i2w'] = vocab_i2w\n",
        "\n",
        "      # Build a mapping between word and IDs and save it. By using a defaultdict, any\n",
        "      # words which are not in the vocab don't get a Key Error but get an ID value of 0 which is UNK\n",
        "      vocab_w2i = collections.defaultdict(int, {word:id for id, word in enumerate(vocab_i2w)})\n",
        "      convert_state['vocab_w2i'] = vocab_w2i\n",
        "    else:\n",
        "      # Get the mapping built during training processing\n",
        "      vocab_w2i = convert_state['vocab_w2i']\n",
        "\n",
        "    # Use the mapping to get an ID for each word item\n",
        "    # Because vocab_w2i is a defaultdict, when the 'map' looks up any words which are not in the \n",
        "    # vocab, they will get added into vocab_w2i\n",
        "    wordid_items = [list(map(lambda x: vocab_w2i[x], sentence)) for sentence in sentence_word_items]\n",
        "\n",
        "    return (wordid_items)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList where each item is a list of word IDs corresponding to one sentence\n",
        "# eg. [[27, 9, 47], [16, 3, 14, 86], ...]\n",
        "#----------------------------------------------------\n",
        "class SentenceWordIdItemList(ItemList):\n",
        "  def __init__(self, items, vocab_i2w, **kwargs):\n",
        "    super().__init__(items)\n",
        "    self.vocab_i2w = vocab_i2w\n",
        "\n",
        "  def WordIdToStream(self, items, convert_state, in_train, **kwargs):\n",
        "    sentence_wordid_items = items\n",
        "    stream_items = [wordid for sentence in sentence_wordid_items for wordid in sentence]\n",
        "    return (stream_items)\n",
        "\n",
        "  def get_display_data(self, obj, idx):\n",
        "    sentence = obj\n",
        "    words = [self.vocab_i2w[wordid] for wordid in sentence]\n",
        "    sentence = ' '.join(words)\n",
        "    return sentence\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList which is a contiguous stream of word IDs\n",
        "# eg. [27, 9, 47, 16, 3, 14, 86, ...]\n",
        "#----------------------------------------------------\n",
        "class StreamWordIdItemList(ItemList):\n",
        "  def __init__(self, items, vocab_i2w, **kwargs):\n",
        "    super().__init__(items)\n",
        "    self.vocab_i2w = vocab_i2w\n",
        "\n",
        "  def get_display_data(self, obj, idx):\n",
        "    # Normally, with a single 'idx' for a Stream item list we would be given a single word id 'obj'\n",
        "    # so we convert it to a list to be able to iterate over it.\n",
        "    #\n",
        "    # On the other hand, if a range of slice 'idx' values was used, we would be given a\n",
        "    # list 'obj' already ready for iteration, so no conversion is necessary here. Also, in the \n",
        "    # case when the Stream item list is contained within a FastaiLMDataset, the 'obj' \n",
        "    # is a tensor of a chunk of text, which is also ready for iteration. \n",
        "    text = obj\n",
        "    if (isinstance(text, int)): text = list(text)\n",
        "\n",
        "    words = [self.vocab_i2w[wordid] for wordid in text]\n",
        "    sentence = ' '.join(words)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEiswEGrzQTh"
      },
      "source": [
        "### Audio Item List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9BWY6mL__Pw"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMTZ_LQ65ZGF"
      },
      "source": [
        "#export\n",
        "\n",
        "from nb_audio import AudioUtil\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Subclass of File Name Item List for Audio Files\n",
        "# Doesn't do much by itself, most of the file-specific functionality is in the parent\n",
        "# class and the audio-specific functionality is contained in AudioItemList\n",
        "#----------------------------------------------------\n",
        "class AudioFileItemList(FileNameItemList):\n",
        "  def FileToAudio(self, items, convert_state, in_train, **kwargs):\n",
        "    audiofile_items = items\n",
        "    return (audiofile_items)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList for Audio. Each item is an audio file, which is dynamically loaded. It also\n",
        "# implements several transforms which can then be applied in sequence to the loaded audio data.\n",
        "#----------------------------------------------------\n",
        "class AudioItemList(ItemList):\n",
        "  def __init__(self, items, **kwargs):\n",
        "    super().__init__(items, **kwargs)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Load the file as audio data\n",
        "  # ----------------------------\n",
        "  def _get_object(self, item):\n",
        "    return AudioUtil.open(item)\n",
        "\n",
        "  # ----------------------------\n",
        "  # \n",
        "  # ----------------------------\n",
        "  def get_display_data(self, obj, idx):\n",
        "    return obj\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform to pad or trim the audio to a fixed length (in milliseconds), since \n",
        "  # the model needs all tensors to be the same size\n",
        "  # ----------------------------\n",
        "  def pad_trim(self, aud, max_ms):\n",
        "    return AudioUtil.pad_trim(aud, max_ms)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform to shift the audio signal. Can be used for Augmentation of the\n",
        "  # raw audio.\n",
        "  # ----------------------------\n",
        "  def signal_shift(self, aud, max_shift_pct=.6):\n",
        "    return AudioUtil.signal_shift(aud, max_shift_pct)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform to generate a Spectrogram\n",
        "  # ----------------------------\n",
        "  def spectro_gram(self, aud, spectro_type='mel', n_mels=64, n_fft=1024, hop_len=None):\n",
        "    return AudioUtil.spectro_gram(aud, spectro_type, n_mels, n_fft, hop_len)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform to randomly mask some frequencies or time steps of the Spectrogram.\n",
        "  # Can be used for Augmentation of the processed Spectrogram rather than the raw\n",
        "  # audio.\n",
        "  # ----------------------------\n",
        "  def spectro_augment(self, spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
        "    return AudioUtil.spectro_augment(spec, max_mask_pct, n_freq_masks, n_time_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrarhQ1cd_mT"
      },
      "source": [
        "### Other ItemLists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3UvjZS9eHG4"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList for label class names\n",
        "# eg. [\"class1\", \"class2\", ...]\n",
        "#----------------------------------------------------\n",
        "class ClassNameItemList(ItemList):\n",
        "\n",
        "  # ----------------------------\n",
        "  # Convert class names to class IDs\n",
        "  # During preparation of training data we build a list of unique class names and a mapping from names to IDs\n",
        "  # During preparation of validation data we reuse the mapping built for training\n",
        "  # ----------------------------\n",
        "  def NameToId(self, items, convert_state, in_train, **kwargs):\n",
        "    if (in_train):\n",
        "      # Build a sorted list of unique class names and save it for use during validation processing\n",
        "      class_names = list(set(items))\n",
        "      class_names.sort()\n",
        "      convert_state['class_vocab_i2n'] = class_names\n",
        "\n",
        "      # Build a mapping between class name and IDs and save it\n",
        "      class_vocab_n2i = {name:id for id, name in enumerate(class_names)}\n",
        "      convert_state['class_vocab_n2i'] = class_vocab_n2i\n",
        "    else:\n",
        "      # Get the mapping built during training processing\n",
        "      class_vocab_n2i = convert_state['class_vocab_n2i']\n",
        "\n",
        "    # Use the mapping to get an ID for each class name item\n",
        "    classid_items = list(map(lambda x: class_vocab_n2i[x], items))\n",
        "    return (classid_items)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList for label class IDs\n",
        "# eg. [1, 2, ...]\n",
        "#----------------------------------------------------\n",
        "class ClassIdItemList(ItemList):\n",
        "  def __init__(self, items, class_vocab_i2n, **kwargs):\n",
        "    super().__init__(items)\n",
        "    self.class_vocab_i2n = class_vocab_i2n\n",
        "\n",
        "  def get_display_data(self, obj, idx):\n",
        "    id = obj\n",
        "    name = self.class_vocab_i2n[id]\n",
        "    return name\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList for a Pandas Dataframe. Each row is an item. Since the actual data \n",
        "# is stored in the dataframe, the items themselves are just a range of numbers \n",
        "# from 0 to 'dataframe_length - 1'.\n",
        "#----------------------------------------------------\n",
        "class DfItemList(ItemList):\n",
        "  # ----------------------------\n",
        "  # Initialisation is called in several cases - after loading from file, after\n",
        "  # splitting into training, validation and test sub-lists, after extracting into\n",
        "  # 'x' and 'y' sub-lists and potentially after converting from one type to another.\n",
        "  # \n",
        "  # In these different cases, the 'true' source of data is either:\n",
        "  #   a) The populated 'df' OR\n",
        "  #   b) A list of items. That list is used to select a subset of the 'df'\n",
        "  # We have to handle these different cases and create a 'df' and list of 'items' \n",
        "  # that are consistent with each other.\n",
        "  # ----------------------------\n",
        "  def __init__(self, items, df, **kwargs):\n",
        "    # If present, 'items' should never be longer than the Dataframe\n",
        "    len_df = len(df.index)\n",
        "    assert (not (items and (len(items) > len_df)))\n",
        "\n",
        "    if (items and (len(items) < len_df)):\n",
        "      # Items contains the indexes of a subset of the Dataframe. This happens when we\n",
        "      # split the ItemList. Create a new Dataframe with those indexes\n",
        "      df = df.iloc[items, :].reset_index(drop=True)\n",
        "      len_df = len(df.index)\n",
        "\n",
        "    self.df = df\n",
        "    # Items should be a range of consecutive numbers from 0 to len_df - 1 \n",
        "    items = list(range(len_df))\n",
        "    super().__init__(items)\n",
        "\n",
        "    df_mem = df.memory_usage().sum() / 1024**2\n",
        "    items_mem = sys.getsizeof(items) / 1024**2\n",
        "    print (f'DfItemList init with {len_df} items, {df_mem:.2f} df memory, {items_mem:.2f} items memory')\n",
        "\n",
        "    self._copy_new.append('df')\n",
        "\n",
        "  # ----------------------------\n",
        "  # Read a CSV file into a Dataframe\n",
        "  # ----------------------------\n",
        "  @classmethod\n",
        "  def open_csv(cls, csv_path):\n",
        "    df = pd.read_csv(csv_path, low_memory=False)\n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # To get the actual data object, we use the item as an index into the\n",
        "  # dataframe's rows. Therefore item #n corresponds to dataframe row #n.\n",
        "  # ----------------------------\n",
        "  def _get_object(self, i):\n",
        "    return self.df.iloc[i]\n",
        "\n",
        "  # ----------------------------\n",
        "  # Extract data as a Python list using a column name\n",
        "  # It is used to extract data out of a DataFrame into another target class\n",
        "  # !!!!!!!!! Should be a list of columns\n",
        "  # ----------------------------\n",
        "  def extract_colval(self, col, **kwargs):\n",
        "    y_list = self.df[col].tolist()\n",
        "    return (y_list)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Select a list of column names, or all columns except a list of column names\n",
        "  # It is used to select data but the target type remains a DataFrame\n",
        "  # ----------------------------\n",
        "  def extract_select(self, cols=None, cols_exclude=None, extra_args={}, **kwargs):\n",
        "    assert(not((cols is None) and (cols_exclude is None)))\n",
        "\n",
        "    if (cols_exclude):\n",
        "      cols = list(set(self.df.columns) - set(cols_exclude))\n",
        "\n",
        "    extra_args['df'] = self.df[cols].copy()\n",
        "\n",
        "    items = None\n",
        "    return (items)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# \n",
        "#----------------------------------------------------\n",
        "class DummyItemList(ItemList):\n",
        "  def __init__(self, items, **kwargs):\n",
        "    super().__init__(items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJVi6nX9i_S1"
      },
      "source": [
        "### Tabular Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tWTOeD4lUJg"
      },
      "source": [
        "#### Tabular ItemList"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYrJjugJASL7"
      },
      "source": [
        "#export\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#----------------------------------------------------\n",
        "# ItemList for Tabular data held in a Pandas Dataframe. Each row is an item.\n",
        "# Optionally, we could be given additional Dataframes with data whose rows have \n",
        "# a 1:1 or a 1:many relationship with the main Dataframe.\n",
        "#----------------------------------------------------\n",
        "class TabularItemList(DfItemList):\n",
        "  def __init__(self, items, df, related_dfs=None, fd={}, **kwargs):\n",
        "    super().__init__(items, df, **kwargs)\n",
        "    self.related_dfs = related_dfs\n",
        "\n",
        "    self.fd = fd\n",
        "    self._copy_new.append('fd')\n",
        "\n",
        "    if (related_dfs is not None):\n",
        "      self._save_related(df, related_dfs)\n",
        "      self._copy_new.append('related_dfs')\n",
        "\n",
        "  # ----------------------------\n",
        "  # Read a CSV file into a Dataframe\n",
        "  # ----------------------------\n",
        "  @classmethod\n",
        "  def open_csv(cls, csv_path):\n",
        "    df = super().open_csv(csv_path)\n",
        "    df = cls.reduce_mem(df, csv_path)\n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Select the type of columns ( either categorical + continuous, or target \n",
        "  # columns) to be extracted.\n",
        "  # It is used to select data but the target type remains a DataFrame\n",
        "  # ----------------------------\n",
        "  def extract_type(self, cols_type, extra_args={}, **kwargs):\n",
        "    assert(cols_type in ['cat_cont', 'tgt'])\n",
        "\n",
        "    cols = None\n",
        "    if (cols_type == 'cat_cont'):\n",
        "      cat_cols, cont_cols = self._categorical_cols(), self._continuous_cols()\n",
        "      cols = cat_cols + cont_cols\n",
        "      fd = {'nominal': cat_cols, 'continuous': cont_cols}\n",
        "    elif (cols_type == 'tgt'):\n",
        "      cols = self._target_cols()\n",
        "      fd = {'target' : cols}\n",
        "\n",
        "    extra_args['df'] = self.df[cols].copy()\n",
        "    extra_args['fd'] = fd\n",
        "\n",
        "    items = None\n",
        "    return (items)\n",
        "\n",
        "  # ----------------------------\n",
        "  # To get the actual data object, we use the item as an index into the\n",
        "  # numpy array's rows. Therefore item #i corresponds to array row #i.\n",
        "  # ----------------------------\n",
        "  def _get_object(self, i):\n",
        "    return self.arr[i]\n",
        "\n",
        "  #----------------------------------------------------\n",
        "  # Define which columns are:\n",
        "  #   Categorical - Nominal or Ordinal\n",
        "  #   Numeric - Continuous or Discrete\n",
        "  #   Target - ie. the dependent feature\n",
        "  #\n",
        "  # The given 'features_dict' is appended to any existing features_dict\n",
        "  #\n",
        "  # features_dict = {\n",
        "  #    'key'     : key_columns,\n",
        "  #    'primary' : primary_features,\n",
        "  #    'nominal' : nominal_features,\n",
        "  #    'ordinal' : ordinal_features,\n",
        "  #    'continuous' : numeric_continuous_features,\n",
        "  #    'discrete' : numeric_discrete_features,\n",
        "  #    'target' : target_feature}\n",
        "  #----------------------------------------------------\n",
        "  def add_features_dict(self, features_dict, df_name=None, mds=None):\n",
        "    fd = self._get_features_dict(df_name)\n",
        "\n",
        "    # If a metadata summary has been given, get the categorical and continuous\n",
        "    # columns from it\n",
        "    if (mds is not None):\n",
        "      # Mark the key and target columns in the mds so they get excluded from the \n",
        "      # categorical and continuous lists\n",
        "\n",
        "      if ('key' in features_dict):\n",
        "        key_cols = features_dict['key']\n",
        "        mds.loc[key_cols, 'col_type'] = 'key'\n",
        "    \n",
        "      if ('target' in features_dict):\n",
        "        target_cols = features_dict['target']\n",
        "        mds.loc[target_cols, 'col_type'] = 'target'\n",
        "\n",
        "      if ('continuous' not in features_dict):\n",
        "        features_dict['nominal'] = mds[mds['col_type'] == 'categorical'].index.to_list()\n",
        "\n",
        "      if ('continuous' not in features_dict):\n",
        "        features_dict['continuous'] = mds[mds['col_type'] == 'continuous'].index.to_list()\n",
        "\n",
        "    # Now update our features dict\n",
        "    fd.update(features_dict)\n",
        "\n",
        "  def _get_features_dict(self, df_name=None):\n",
        "    if (df_name is None):\n",
        "      return self.fd\n",
        "    else:\n",
        "      return getattr(self, f'{df_name}_fd', None)\n",
        "\n",
        "  def _get_features_dict_cols(self, col_type, df_name=None):\n",
        "    fd = self._get_features_dict(df_name)\n",
        "    if (col_type in fd.keys()):\n",
        "      return fd[col_type]\n",
        "    else:\n",
        "      return []\n",
        "\n",
        "  def _key_cols(self):\n",
        "    return self._get_features_dict_cols('key')\n",
        "\n",
        "  def _primary_cols(self):\n",
        "    return self._get_features_dict_cols('primary')\n",
        "\n",
        "  def _numeric_cols(self):\n",
        "    return self._get_features_dict_cols('continuous') + self._get_features_dict_cols('discrete')\n",
        "\n",
        "  def _continuous_cols(self):\n",
        "    return self._get_features_dict_cols('continuous')\n",
        "\n",
        "  def _discrete_cols(self):\n",
        "    return self._get_features_dict_cols('discrete')\n",
        "\n",
        "  def _categorical_cols(self):\n",
        "    return self._get_features_dict_cols('nominal') + self._get_features_dict_cols('ordinal')\n",
        "\n",
        "  def _target_cols(self):\n",
        "    return self._get_features_dict_cols('target')\n",
        "\n",
        "  # ----------------------------\n",
        "  # Save the ItemList for test data for later use during splitting\n",
        "  # ----------------------------\n",
        "  def add_test_rows(self, test_rows):\n",
        "    self.test_rows = test_rows\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split the rows into train and validation data sequentially, while the test data\n",
        "  # that we saved previously can be returned unchanged\n",
        "  # ----------------------------\n",
        "  def split_sequential(self, test_data=False, **kwargs):\n",
        "    train_rows, valid_rows, test_rows = super().split_sequential(**kwargs)\n",
        "    if ('test_ratio' not in kwargs):\n",
        "      test_rows = self._split_test(test_data)\n",
        "    return (train_rows, valid_rows, test_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split the rows into train and validation data based on row indexes, while the test data\n",
        "  # that we saved previously can be returned unchanged\n",
        "  # ----------------------------\n",
        "  def split_idxs(self, test_data=False, **kwargs):\n",
        "    train_rows, valid_rows, _ = super().split_idxs(**kwargs)\n",
        "    if ('test_ratio' not in kwargs):\n",
        "      test_rows = self._split_test(test_data)\n",
        "    return (train_rows, valid_rows, test_rows)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Return the test data that was loaded previously\n",
        "  # ----------------------------\n",
        "  def _split_test(self, test_data=False):\n",
        "    if (test_data):\n",
        "      test_rows = getattr(self, 'test_rows', None)\n",
        "    else:\n",
        "      test_rows = None\n",
        "    return test_rows\n",
        "\n",
        "  # ----------------------------\n",
        "  # Save each related df in its own attribute on the ItemList\n",
        "  # ----------------------------\n",
        "  def _save_related(self, df, related_dfs):\n",
        "    # Save each related df in its named field\n",
        "    for name, rdf in related_dfs.items():\n",
        "      setattr(self, f'{name}_df', rdf)\n",
        "      setattr(self, f'{name}_fd', {})\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get the main df or a related df\n",
        "  # ----------------------------\n",
        "  def _get_df(self, df_name=None):\n",
        "    if (df_name is None):\n",
        "      # Use main df\n",
        "      df = self.df\n",
        "    else:\n",
        "      # Use related df\n",
        "      df = getattr(self, f'{df_name}_df', None)\n",
        "    \n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Set the main df or a related df\n",
        "  # ----------------------------\n",
        "  def _set_df(self, res_df, df_name=None):\n",
        "    if (res_df is None): return\n",
        "\n",
        "    if (df_name is None):\n",
        "      # Update the main df\n",
        "      self.df = res_df\n",
        "    else:\n",
        "      # Update the related df\n",
        "      setattr(self, f'{df_name}_df', res_df)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Flatten the columns of a multi-level column index, that gets created as a\n",
        "  # result of a crosstab, pivot_table or groupby aggregation.\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def _flatten_col_names(df):\n",
        "    if (df.columns.nlevels > 1):\n",
        "      df.columns = ['_'.join(col) for col in df.columns.values]\n",
        "\n",
        "  # ----------------------------\n",
        "  # Decorator for all data processor functions.\n",
        "  #\n",
        "  # All the processor functions are kept simple and take an explicit 'df' argument \n",
        "  # and may return a 'df' as well. They don't contain logic to figure out which df\n",
        "  # to use from amongst the ItemList's main df and all the related dfs. Similarly\n",
        "  # if they return a result df, they don't contain logic to figure out which of the\n",
        "  # ItemList's dfs should be updated.\n",
        "  #\n",
        "  # That logic is encapsulated within this decorator. It has three modes. The first\n",
        "  # two modes are the only ones that can be used when it is invoked externally. The\n",
        "  # third mode is only used internally when one processor invokes another processor \n",
        "  # method within the class:\n",
        "  #   1. The simple case, where no 'df' is passed in. Decorator uses the ItemList's main df. \n",
        "  #   2. A 'df_name' is passed in. Decorator uses the corresponding related df.\n",
        "  #   3. A 'df' is passed in when invoked internally. The Decorator then bypasses all\n",
        "  #      its logic, and just invokes the function and returns its result normally.\n",
        "  # \n",
        "  # It also takes some flags, so that the behaviour can be different for the converter\n",
        "  # data processors and for the EDA display utilities.\n",
        "  #   1. save_res - if True, the result df is saved and replaces the df used above\n",
        "  #   2. use_fd - if True, the feature dict for the df is also passed in the kwargs\n",
        "  # ----------------------------\n",
        "  def df_process(save_res=True, use_fd=False):\n",
        "    def name_to_df(proc):\n",
        "      # Note that the method above cannot take a 'self' argument. However the 'self' \n",
        "      # is available within the method and is passed to the 'pair' wrapper function.\n",
        "\n",
        "      def wrap(self, df=None, df_name=None, *args, **kwargs):\n",
        "        if (df is not None):\n",
        "          # Bypass all decorator logic\n",
        "          return proc(self, df, *args, **kwargs)\n",
        "        \n",
        "        # items, convert_state, in_train, df_name\n",
        "\n",
        "        extra_dfs = {}\n",
        "\n",
        "        # Get the df to process\n",
        "        df = self._get_df(df_name)\n",
        "        # Pass the feature dictionary as well\n",
        "        if (use_fd):\n",
        "          fd = self._get_features_dict(df_name)\n",
        "          extra_dfs['fd'] = fd\n",
        "\n",
        "        # Check all other keyword arguments for anything named 'xxx_df_name' and\n",
        "        # if so, find the corresponding related df, and add an argument with the \n",
        "        # prefix {'xxx': <related_df>}\n",
        "        for kwarg, kwargv in kwargs.items():\n",
        "          if (kwarg.endswith('df_name')):\n",
        "            kwarg_pre = kwarg.partition('df_name')[0]\n",
        "            extra_dfs[f'{kwarg_pre}df'] = self._get_df(kwargv)\n",
        "            if (use_fd):\n",
        "              extra_dfs[f'{kwarg_pre}fd'] = self._get_features_dict(kwargv)\n",
        "\n",
        "        # We added extra arguments to be passed in\n",
        "        if (extra_dfs): kwargs.update(extra_dfs)\n",
        "\n",
        "        # Call the processor function\n",
        "        res_df = proc(self, df, *args, **kwargs)\n",
        "\n",
        "        # Save the result df if it is returned\n",
        "        if (save_res):\n",
        "          self._set_df(res_df, df_name)\n",
        "        else:\n",
        "          return res_df\n",
        "\n",
        "        # return items\n",
        "\n",
        "      return wrap\n",
        "    return name_to_df\n",
        "\n",
        "  # PREPARE Utilities for pre-processing\n",
        "  # ===============================================================\n",
        "\n",
        "  # ----------------------------\n",
        "  # Convert a numeric column that represents True/False with 1s and 0s into a\n",
        "  # bool column\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def num_01_to_bool(self, df, col):\n",
        "    df[col] = df[col] != 0\n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Sort by columns\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def sort_by(self, df, sort_cols, ascending):\n",
        "    df = df.sort_values(sort_cols, ascending=ascending)\n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Remove a column. If 'col_ys' is True, remove any columns with names ending\n",
        "  # with '_y', as they were duplicates after merging\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def remove_col(self, df, cols, col_ys=True):\n",
        "    # Find columns ending with '_y'\n",
        "    if (col_ys):\n",
        "      for col in df.columns:\n",
        "        if col.endswith('_y'):\n",
        "          cols.append(col)\n",
        "\n",
        "    # Now remove all the required columns\n",
        "    df.drop(columns=cols, inplace = True)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Rename a column\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def rename_col(self, df, col, new_name):\n",
        "    df = df.rename(columns={col: new_name})\n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Remap old category values to new values\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def remap_val(self, df, col, remap_dict):\n",
        "    df[col] = df[col].map(remap_dict)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Clone the index (containing row IDs) and add it as a new column\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def add_index_col(self, df, idx_col):\n",
        "    df.index = df.index.set_names([idx_col])\n",
        "    df = df.reset_index()\n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Do a Left Outer Join on the given 'left_on' and 'right_on' fields\n",
        "  # 'suffixes' describes the naming convention for duplicate fields. \n",
        "  # We leave the duplicate field names on the left untouched, and append a \"_y\" to those on the right\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def join_df(self, left_df, right_df, left_on, right_on=None, how='left', suffix='_y', right_key=None, **kwargs):\n",
        "    if right_on is None: right_on = left_on\n",
        "    join_df = left_df.merge(right_df, how=how, left_on=left_on, right_on=right_on, \n",
        "                        suffixes=(\"\", suffix))\n",
        "\n",
        "    if (right_key is None): right_key = right_df.columns[0]\n",
        "    assert(len(join_df[join_df[right_key].isnull()]) == 0)\n",
        "\n",
        "    return join_df\n",
        "    \n",
        "  # ----------------------------\n",
        "  # Aggregate columns. Group by 'group_cols' and then aggregate the columns in 'agg_cols_fn'\n",
        "  # using the given functions\n",
        "  # \n",
        "  # NB: rollup() provides the same functionality, and is the preferred way as \n",
        "  # it uses pivot_tables rather than groupby.\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def _rollup_groupby(self, df, group_cols, agg_cols_fn):\n",
        "    # Expand the types of the aggregate columns so that the aggregates don't overflow\n",
        "    # NB: !!!!!! we are changing the original dataframe. should we do it on a copy instead?\n",
        "    agg_cols = list(agg_cols_fn.keys())\n",
        "    df[agg_cols] = df[agg_cols].astype('float64')\n",
        "\n",
        "    # Group by 'group_cols'\n",
        "    gd = df.groupby(group_cols)\n",
        "\n",
        "    # Do the aggregates\n",
        "    adf = gd.agg(agg_cols_fn)\n",
        "\n",
        "    # The columns are now a two-level index, with the first level being the original\n",
        "    # column name and the second level being the aggregation function name.\n",
        "    # Rename the columns by flattening the index to a single level and appending \n",
        "    # the column name to the aggregation function name.\n",
        "    self._flatten_col_names(adf)\n",
        "\n",
        "    adf = adf.reset_index()\n",
        "    return adf\n",
        "  \n",
        "  # ----------------------------\n",
        "  # Helper function to rollup categorical columns\n",
        "  # NB: _roll_cat() is the preferred way as it is much faster and uses get_dummies \n",
        "  # rather than looping with crosstab\n",
        "  # ----------------------------\n",
        "  def _roll_cat_crosstab(self, df, index, cat_cols):\n",
        "    concat_dfs = []\n",
        "    # We want separate rolled up counts for each categorical column. Doing this in\n",
        "    # a single pivot_table() (or crosstab()) call by passing the list of categorical\n",
        "    # columns results in a deep multi-level column index with counts for all possible\n",
        "    # combinations of those column values, which is not what we want. So we get counts\n",
        "    # for one column at a time and concatenate them all into a single result df.\n",
        "    for col in cat_cols:\n",
        "      # Use crosstab since pivot_table doesn't support the normalise feature, \n",
        "      # which gives us percentage counts, by row\n",
        "      indexes = [df[idx] for idx in index]\n",
        "      df_count = pd.crosstab(indexes, df[col], normalize='index')\n",
        "      df_count.columns = [f'{df_count.columns.name}_{val}' for val in df_count.columns.values]\n",
        "\n",
        "      # Add to our list of count dfs\n",
        "      concat_dfs.append(df_count)\n",
        "\n",
        "    # Concatenate all our count dfs column-wise\n",
        "    df_cat = pd.concat(concat_dfs, axis=1, verify_integrity=True)\n",
        "    return df_cat\n",
        "\n",
        "  # ----------------------------\n",
        "  # Helper function to rollup categorical columns \n",
        "  # ----------------------------\n",
        "  def _roll_cat(self, df, index, cat_cols, nan_as_category=False):\n",
        "    # Create one column per categorical value, of each categorical column\n",
        "    original_columns = list(df.columns)\n",
        "    df = pd.get_dummies(df, columns=cat_cols, dummy_na=nan_as_category)\n",
        "\n",
        "    # Just use .mean() instead of agg_cat\n",
        "    # Use df_cat throughout not 'df'. Can get rid of prior columns. Then no\n",
        "    # need for new_columns.\n",
        "    # Also use _flatten_col_names\n",
        "\n",
        "    new_columns = [c for c in df.columns if c not in original_columns]\n",
        "    agg_cat = {}\n",
        "    for cat in new_columns:\n",
        "        agg_cat[cat] = ['mean']\n",
        "    df_cat = df.groupby(index).agg(agg_cat)\n",
        "    #df_cat.columns = ['_'.join(col) for col in df_cat.columns.values]\n",
        "    df_cat.columns = new_columns\n",
        "\n",
        "    return df_cat\n",
        "\n",
        "  # ----------------------------\n",
        "  # Helper function to rollup continuous columns \n",
        "  # ----------------------------\n",
        "  def _roll_cont(self, df, index, group_cols, agg_cont):\n",
        "    # Probably due to a Pandas bug, columns of type 'float16' result in a \n",
        "    # bizarre 'No Matching Signature' error. Converting to 'float32' gets rid\n",
        "    # of the error\n",
        "    for col in agg_cont.keys():\n",
        "      if (df[col].dtype == 'float16'):\n",
        "        df[col] = df[col].astype('float32')\n",
        "\n",
        "    # Aggregate the continuous columns, optionally grouping by some categorical\n",
        "    # columns. Then flatten the multi-index column names\n",
        "    df_cont = df.pivot_table(index=index, columns=group_cols, aggfunc=agg_cont)\n",
        "    self._flatten_col_names(df_cont)\n",
        "    return df_cont\n",
        "\n",
        "  # ----------------------------\n",
        "  # Rollup a related df by aggregating the categorical and continuous columns \n",
        "  # so that it can be joined with the main df\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def rollup(self, df, index, group_cols, cat_cols, agg_cont):\n",
        "    # Rollup categorical and continuous columns separately\n",
        "    concat_dfs = []\n",
        "    if (cat_cols):\n",
        "      df_cat = self._roll_cat(df, index, cat_cols)\n",
        "      concat_dfs.append(df_cat)\n",
        "    if (agg_cont):\n",
        "      df_cont = self._roll_cont(df, index, group_cols, agg_cont)\n",
        "      concat_dfs.append(df_cont)\n",
        "\n",
        "    # and then concatenate them column-wise\n",
        "    rdf = pd.concat(concat_dfs, axis=1, verify_integrity=True)\n",
        "    return rdf\n",
        "\n",
        "  # ----------------------------\n",
        "  # Convert a field to Datetime\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def to_date(self, df, col, **kwargs):\n",
        "    fld_dtype = df[col].dtype\n",
        "      \n",
        "    # If it is a custom datatime datatype, change the type to the standard datetime\n",
        "    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n",
        "      print ('Found DatetimeTZDtype ', col)\n",
        "      fld_dtype = np.datetime64\n",
        "\n",
        "    # If the column is a Pandas categorical type, then Numpy's np.issubdtype() throws an exception.\n",
        "    # Also, for categorical types, for some reason in some cases, to_datetime() doesn't convert \n",
        "    # to datetime correctly. So we convert categorical to object beforehand to prevent these errors.\n",
        "    if (pd.api.types.is_categorical_dtype(fld_dtype)):\n",
        "      df[col] = df[col].astype('object')\n",
        "\n",
        "    # If it is not already a datetime datatype, convert it to datetime\n",
        "    if not np.issubdtype(fld_dtype, np.datetime64):\n",
        "      df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Extract separate feature columns for all relevant attributes of DateTime fields\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def extract_date(self, df, col, drop=True, time=False, attr=None, **kwargs):\n",
        "    # First make sure the field is converted to Datetime\n",
        "    self.to_date(df=df, col=col)\n",
        "\n",
        "    fld = df[col]\n",
        "\n",
        "    # Get any prefix in the field name before the word 'Date'\n",
        "    # We will pre-pend this prefix to any new column names that we extract below\n",
        "    targ_pre = re.sub('[Dd]ate$', '', col)\n",
        "      \n",
        "    # List of attributes for which we want to extract separate columns\n",
        "    if (attr is None):\n",
        "      attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
        "              'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
        "    # Include attributes for time in the list\n",
        "    if time: attr = attr + ['Hour', 'Minute', 'Second']\n",
        "      \n",
        "    # Create a separate column for each attribute.\n",
        "    # Column name is created by concatentating the 'prefix' and the 'attribute'\n",
        "    # All datetime columns have \".dt\" from which we extract all the required attributes\n",
        "    for n in attr: df[targ_pre + n] = getattr(fld.dt, n.lower())\n",
        "      \n",
        "    # Create an Elapsed column\n",
        "    df[targ_pre + 'Elapsed'] = fld.astype(np.int64) // 10 ** 9\n",
        "      \n",
        "    # Drop the original date column if needed\n",
        "    if drop: df.drop(col, axis=1, inplace=True)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Calculate days since some event\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def time_since(self, df, sort_col, date_col, col, prefix):\n",
        "    secs_per_day = np.timedelta64(1, 'D')\n",
        "    last_event_date = np.datetime64()\n",
        "    last_sort_val = 0\n",
        "    res = []\n",
        "\n",
        "    for s,v,d in zip(df[sort_col].values, df[col].values, df[date_col].values):\n",
        "      # Reset counters at next sort value\n",
        "      if s != last_sort_val:\n",
        "        last_event_date = np.datetime64()\n",
        "        last_event_date = None\n",
        "        last_sort_val = s\n",
        "\n",
        "      # Event occurs\n",
        "      if v: last_event_date = d\n",
        "\n",
        "      # Number of days since last occurrence of event\n",
        "      if (last_event_date is None):\n",
        "        # Fill 0 until the first event occurs\n",
        "        res.append(int(0))\n",
        "      else:\n",
        "        res.append(((d - last_event_date).astype('timedelta64[D]') / secs_per_day))\n",
        "\n",
        "    df[prefix + col] = res\n",
        "\n",
        "  # ----------------------------\n",
        "  # Aggregate columns over rolling time windows.\n",
        "  # Compute rolling time windows on 'date_col' of duration 'window_sz'. Optionally\n",
        "  # also group by 'group_cols' if given. Then aggregate 'agg_cols'\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def rolling_window(self, df, date_col, group_cols, window_sz, agg_cols, agg_fn='sum', ascending=True, suffix=None):\n",
        "    # Sort by the 'date_col'\n",
        "    gd = df.set_index(date_col).sort_index(ascending=ascending)\n",
        "\n",
        "    # Group by 'group_col's if given\n",
        "    if (group_cols is not None):\n",
        "      gd = gd.groupby(group_cols)\n",
        " \n",
        "    # Compute rolling windows\n",
        "    gr = gd.rolling(window_sz, min_periods=1)\n",
        "\n",
        "    # Do the aggregates\n",
        "    adf = gr[agg_cols].agg(agg_fn)\n",
        "    adf = adf.reset_index()\n",
        "\n",
        "    # 'adf' now contains only the aggregates but not the original 'agg_cols' columns.\n",
        "    # So merge the aggregates into the original 'df'\n",
        "    join_cols = group_cols if (group_cols is not None) else []\n",
        "    join_cols.append(date_col)\n",
        "    suffix = suffix if (suffix is not None) else '_' + agg_fn\n",
        "    df = df.merge(adf, how='left', left_on=join_cols, right_on=join_cols, suffixes=('', suffix))\n",
        "\n",
        "    return df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Split a text value using a separator into separate text columns. \n",
        "  # The names of the new columns created is passed as a 'new_cols' list \n",
        "  #    eg. ['new_col_name_2', None, 'new_col_name_2', ...]. Any new column which \n",
        "  # we don't want can be passed as a None.\n",
        "  # Optionally, the original text column can be dropped.\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def text_split(self, df, col, sep, new_cols, drop_col=True, **kwargs):\n",
        "    # Split the string, and expand each part as a separate column\n",
        "    ndf = df[col].str.split(sep, expand=True)\n",
        "\n",
        "    # Assign names for each new column created, or exclude it if it is None\n",
        "    for i, new_col in enumerate(new_cols):\n",
        "      if (new_col is not None):\n",
        "        df[new_col] = ndf[i]\n",
        "\n",
        "    # Drop the original column if required\n",
        "    if (drop_col):\n",
        "      df.drop(columns=[col], inplace = True)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Replace a text value by another value\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def text_replace(self, df, col, old_val, new_val, **kwargs):\n",
        "    df.loc[df[col] == old_val, col] = new_val\n",
        "\n",
        "  # ----------------------------\n",
        "  # Concatenate multiple text values\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def text_concat(self, df, new_col, concat_cols, space=' ', **kwargs):\n",
        "    assert(isinstance(concat_cols, list) and (len(concat_cols) >= 2))\n",
        "    df[new_col] = df[concat_cols[0]] + space + df[concat_cols[1]]\n",
        "    for concat_col in concat_cols[2:]:\n",
        "      df[new_col] = df[new_col] + space + df[concat_col]\n",
        "\n",
        "  # -----------------------------------------------------\n",
        "  # Convert a numeric column to a categorical column based on ranges defined by bins\n",
        "  # -----------------------------------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def bin_num (self, df, col, bin_col, bins, labels):\n",
        "    df[bin_col] = pd.cut(df[col], bins=bins, labels=labels)\n",
        "\n",
        "  # PROCESSORs\n",
        "  # ===============================================================\n",
        "\n",
        "  # ----------------------------\n",
        "  # Convert all categorical variables to numeric category codes, using the \n",
        "  # Pandas Categorical data type. Categorical variables get converted to\n",
        "  # Categorical columns at the beginning before Splitting. That way all the\n",
        "  # category values are based on all data rows, not just the training rows.\n",
        "  # If this isn't done, then values which occur only in the validation rows\n",
        "  # would be given an 'unknown' category code.\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def to_cat(self, df, convert_state, in_train, cols=None, ordinal_vals={}, **kwargs):\n",
        "    cols = cols if (cols is not None) else self._categorical_cols()\n",
        "\n",
        "    for col in cols:\n",
        "      if (in_train):\n",
        "        if not is_categorical_dtype(df[col]):\n",
        "          # Convert to Categorical data type if it isn't already. All categorical\n",
        "          # variables have already been converted at the beginning itself, so it is\n",
        "          # only the '_na' variables that were newly added that get converted here.\n",
        "          df[col] = df[col].astype('category')\n",
        "\n",
        "        # Set the category value using the given order, if the values are \n",
        "        # ordinal ie. they have an intrinsic order\n",
        "        if (col in ordinal_vals):\n",
        "          vals = ordinal_vals[col]\n",
        "          df[col].cat.set_categories(vals, ordered=True, inplace=True)\n",
        "\n",
        "        # Save away the category names for use with validation data\n",
        "        convert_state[f'Category_{col}'] = df[col].cat.categories\n",
        "\n",
        "        # Get the category codes for training data. Pandas category codes start \n",
        "        # from 0, and -1 is used for NaN values. So we add 1 to all codes which means \n",
        "        # that NaN get a 0 value, and all other categories get shifted by 1.\n",
        "        df[col] = df[col].cat.codes + 1\n",
        "      else:\n",
        "        # For validation and test data, use the same categories that were saved for\n",
        "        # training data. Set the ordered flag if the column is ordinal.\n",
        "        ordered = (col in ordinal_vals)\n",
        "        cat_col = pd.Categorical(df[col], categories=convert_state[f'Category_{col}'], ordered=ordered)\n",
        "        df[col] = cat_col.codes + 1\n",
        "\n",
        "  # ----------------------------\n",
        "  # Replace any '+- inf' values by NaN\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def remove_inf(self, df, **kwargs):\n",
        "    continuous_cols = self._continuous_cols()\n",
        "    df[continuous_cols] = df[continuous_cols].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Check columns for count of missing values. Optionally add a flag column for each\n",
        "  # column with missing values\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def check_missing(self, df, convert_state, in_train, cols=None, missing_thresh=0.5, add_flag=False, **kwargs):\n",
        "    if (cols is None):\n",
        "      cols = self._continuous_cols()\n",
        "\n",
        "    # %age of missing values > threshold\n",
        "    missing_pct = df[cols].isna().sum() / len(df)\n",
        "    high_missing = missing_pct[missing_pct > missing_thresh].to_dict()\n",
        "    for col, pct in high_missing.items():\n",
        "      print (f'Column {col} has {pct} percent missing rows. In Train: {in_train}')\n",
        "\n",
        "    # Check if flag columns need to be added\n",
        "    if (not add_flag):\n",
        "      return\n",
        "\n",
        "    if (in_train):\n",
        "      # For training, find all columns with missing data and save the list\n",
        "      add_cols = missing_pct[missing_pct > 0].index.tolist()\n",
        "      convert_state[f'NAColumns'] = add_cols\n",
        "    else:\n",
        "      # For validation use the list of missing columns saved from training \n",
        "      add_cols = convert_state[f'NAColumns']\n",
        "\n",
        "    # Add an extra column with a flag indicating that a value was \n",
        "    # missing, before being filled. Add those extra columns to the list of\n",
        "    # categorical columns.\n",
        "    for col in add_cols:\n",
        "      na_col = col + '_na'\n",
        "      df[na_col] = df[col].isna()\n",
        "      self.fd['nominal'].append(na_col)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Helper function to impute values for missing column values\n",
        "  # ----------------------------\n",
        "  def _impute_values(self, df, cols, strategy, values=None):\n",
        "    imp_df = df[cols]\n",
        "\n",
        "    # Compute the mean, median or mode depending on the strategy. If strategy\n",
        "    # is 'constant', the values are passed in so there is nothing to compute\n",
        "    if (strategy == 'mode'):\n",
        "      # If multiple values have the same count, mode() returns multiple rows.\n",
        "      # So .iloc[0] picks the first row\n",
        "      fill_dict = imp_df.mode().iloc[0].to_dict()\n",
        "    elif (strategy == 'median'):\n",
        "      fill_dict = imp_df.median().to_dict()\n",
        "    elif (strategy == 'mean'):\n",
        "      fill_dict = imp_df.mean().to_dict()\n",
        "    elif (strategy == 'constant'):\n",
        "      fill_dict = {k:v for k, v in zip(cols, values)}\n",
        "\n",
        "    # Use the computed (or passed in) values to fill all missing NaNs\n",
        "    df.fillna(fill_dict, inplace=True)\n",
        "\n",
        "    # Check that there are no missing values left\n",
        "    assert(df[cols].isna().sum().sum() == 0)\n",
        "    return list(fill_dict.values())\n",
        "\n",
        "  # ----------------------------\n",
        "  # Fill in missing column values\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def fill_missing(self, df, convert_state, in_train, cols=None, strategy='median', fill_values=None, **kwargs):\n",
        "    assert(strategy in ['mean', 'median', 'mode', 'constant'])\n",
        "    assert(not((strategy == 'constant') and (fill_values is None)))\n",
        "\n",
        "    fill_cat = False    # For now, don't fill categorical columns\n",
        "    if (in_train):\n",
        "      if (cols is None):\n",
        "        continuous_cols = self._continuous_cols()\n",
        "        continuous_values = self._impute_values(df, continuous_cols, strategy, fill_values)\n",
        "        cols, values = continuous_cols, continuous_values\n",
        "\n",
        "        if (fill_cat):\n",
        "          # Get the categorical columns and impute the mode values for them\n",
        "          categorical_cols = self._categorical_cols()\n",
        "          categorical_values = self._impute_values(df, categorical_cols, 'mode')\n",
        "          cols += categorical_cols\n",
        "          values += categorical_values\n",
        "      else:\n",
        "        values = self._impute_values(df, cols, strategy, fill_values)\n",
        "\n",
        "      # Save the values that were used to fill missing NaNs, so that we can use\n",
        "      # them for validation data \n",
        "      for col, val in zip(cols, values):\n",
        "        convert_state[f'Missing_{col}'] = val\n",
        "    \n",
        "    else:\n",
        "      # For validation data, fill with the same values that were used for the \n",
        "      # training data\n",
        "\n",
        "      if (cols is None):\n",
        "        cols = self._continuous_cols()\n",
        "        if (fill_cat):\n",
        "          cols += self._categorical_cols()\n",
        "\n",
        "      fill_values = [convert_state[f'Missing_{col}'] for col in cols]\n",
        "      _ = self._impute_values(df, cols, strategy='constant', values=fill_values)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Normalisation\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def normalise(self, df, convert_state, in_train, cols=None, **kwargs):\n",
        "\n",
        "    # If no columns are given, normalise all continuous columns\n",
        "    if (cols is None):\n",
        "      cols = self._continuous_cols()\n",
        "\n",
        "    for col in cols:\n",
        "      if (in_train):\n",
        "        # Calculate the mean and std for training data and save it for use\n",
        "        # during validation.\n",
        "        # Since we have earlier optimised the DataFrame memory by changing columns \n",
        "        # to the lightest data type that encompasses the min-max range, but that \n",
        "        # could cause the mean and std to overflow. So we temporarily treat them\n",
        "        # as doubles for the mean/std calculation by using .astype('float64')\n",
        "        convert_state[f'Mean_{col}'] = df[col].astype('float64').mean()\n",
        "        convert_state[f'Std_{col}'] = df[col].astype('float64').std()\n",
        "\n",
        "      # Use the saved mean/std to normalise for both training data and\n",
        "      # validation data\n",
        "      col_mean = convert_state[f'Mean_{col}']\n",
        "      col_std = convert_state[f'Std_{col}']\n",
        "      assert (np.isfinite(col_mean) and np.isfinite(col_std))\n",
        "\n",
        "      df[col] = (df[col].astype('float64') - col_mean) / (1e-7 + col_std)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Return a numpy array of all the column values for the i-th row\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=True, use_fd=False)\n",
        "  def to_np(self, df, **kwargs):\n",
        "    cat_cols, cont_cols = self._categorical_cols(), self._continuous_cols()\n",
        "    all_cols = cat_cols + cont_cols + self._target_cols()\n",
        "\n",
        "    # Check that there are no NaNs\n",
        "    assert(df[cont_cols].isna().sum().sum() == 0)\n",
        "    #assert(np.isfinite(df[cont_cols]).all().all())\n",
        "\n",
        "    # List of columns values for all columns. Make sure that all the\n",
        "    # categorical columns are first, followed by all the continuous columns\n",
        "    vals = [df[col].values for col in all_cols if col in df.columns]\n",
        "\n",
        "    # Convert list to numpy array\n",
        "    arr = np.stack(vals, axis=1)\n",
        "\n",
        "    # Convert to float if it isn't already\n",
        "    if (not np.issubdtype(arr.dtype, float)):\n",
        "      arr = arr.astype('float32')\n",
        "\n",
        "    self.arr = arr\n",
        "\n",
        "  # ----------------------------\n",
        "  # Compare two Dataframes for equality, for debugging purposes\n",
        "  # ----------------------------\n",
        "  def compare_df(self, df1, df2, name=''):\n",
        "    # Compare the two dfs column-wise\n",
        "    diff_df = (df1 == df2).all(0)\n",
        "    # Get a list of columns which are different\n",
        "    diff_cols = diff_df[~diff_df].index.tolist()\n",
        "\n",
        "    # Since NaN cannot be compared with another NaN, columns which\n",
        "    # have NaN values are marked as being different. So we filter\n",
        "    # out columns which differ only in the NaN values\n",
        "    unmatched_cols=[]\n",
        "    for col in diff_cols:\n",
        "      # Compare the non-NaN values in both columns\n",
        "      if (df1[df1[col].notna()][col] == df2[df2[col].notna()][col]).all(0):\n",
        "        # Columns are same\n",
        "        pass\n",
        "      else:\n",
        "        # Columns are different\n",
        "        unmatched_cols.append(col)\n",
        "\n",
        "    # Get the rows which are different in the unmatched columns\n",
        "    if (unmatched_cols):\n",
        "      um_df1, um_df2 = self._unmatched_rows(df1, df2, unmatched_cols)\n",
        "      print (f'Unmatched {name}', unmatched_cols)\n",
        "    else:\n",
        "      um_df1, um_df2 = None, None\n",
        "      print (f'Matched {name}')\n",
        "\n",
        "    return [(unmatched_cols, um_df1, um_df2)]\n",
        "\n",
        "  # ----------------------------\n",
        "  # Supporting function, returns the unmatched rows between two Dataframes\n",
        "  # ----------------------------\n",
        "  def _unmatched_rows(self, df1, df2, unmatched_cols):\n",
        "    unmatch_mask = (df1[unmatched_cols] != df2[unmatched_cols]).any(1)\n",
        "    um_df1 = df1[unmatch_mask][unmatched_cols]\n",
        "    um_df2 = df2[unmatch_mask][unmatched_cols]\n",
        "    print ('Num unmatched', len(um_df1))\n",
        "    return (um_df1, um_df2)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Compare two pairs of np arrays (ie. X1 to X2, and Y1 to Y2) for equality, for \n",
        "  # debugging purposes\n",
        "  # ----------------------------\n",
        "  def compare_arr(self, x_arr1, x_arr2, y_arr1, y_arr2, n_cat):\n",
        "    # Compare shapes\n",
        "    assert(x_arr1.shape == x_arr2.shape)\n",
        "    assert(y_arr1.shape == y_arr2.shape)\n",
        "\n",
        "    # Compare all categorical columns\n",
        "    diff_cat = (x_arr1[:, :n_cat] == x_arr2[:, :n_cat]).all(0)\n",
        "    assert(diff_cat.all())\n",
        "\n",
        "    # Compare all continuous columns\n",
        "    diff_cont = (x_arr1[:, n_cat:] == x_arr2[:, n_cat:]).all(0)\n",
        "    assert(diff_cont.all())\n",
        "\n",
        "    # Compare the target columns\n",
        "    diff_tgt = (y_arr1 == y_arr2).all()\n",
        "    assert(diff_tgt.all())\n",
        "\n",
        "  # EDA and Metadata functionality\n",
        "  # ===============================================================\n",
        "  \n",
        "  # What we want\n",
        "  # - Names of all columns with their pandas data types\n",
        "  # - Logical type of each column ie. Categorical Ordinal, Categorical Nominal, Continuous, Target, Primary Key\n",
        "  # - Number of rows\n",
        "  # - Categorical columns distribution - number of unique values, list of unique values with their counts\n",
        "  # - Continuous columns distribution - histogram or density curve\n",
        "  # - For each column - number and %age of missing values\n",
        "  # - describe() stats\n",
        "  #\n",
        "  # - Bivariate distribution of each column with the Target column\n",
        "  #\n",
        "  # - For time series - multi-index on time and primary key. Then plot time on X-axis and\n",
        "  #    Target value on y-axis, with separate lines for each value of the primary key. Any\n",
        "  #    event occurrences can also be marked on the timeline.\n",
        "\n",
        "  # ----------------------------\n",
        "  # Make a first guess about the feature type of each column.\n",
        "  # We are given 'mds' which is the output of df.describe()\n",
        "  # ----------------------------\n",
        "  def _feature_types(self, mds):\n",
        "    # Initialise all columns to None\n",
        "    mds['col_type'] = None\n",
        "\n",
        "    # Mark columns, where > 30% rows are missing, as secondary\n",
        "    miss_mask = (mds['col_type'].isna()) & (mds['% miss'] > 30)\n",
        "    mds.loc[miss_mask, 'col_type'] = 'secondary'\n",
        "\n",
        "    # Mark columns, with finite unique values, as categorical\n",
        "    if ('unique' in mds.columns):\n",
        "      cat_mask = (mds['col_type'].isna()) & mds.loc[:, 'unique'].notnull()\n",
        "      mds.loc[cat_mask, 'col_type'] = 'categorical'\n",
        "\n",
        "    # Mark columns, where > 50% rows have the same mode value, as secondary\n",
        "    sec_mask = (mds['col_type'].isna()) & (mds['mode %'] > 50)\n",
        "    mds.loc[sec_mask, 'col_type'] = 'secondary'\n",
        "\n",
        "    # Mark columns, which are either ints or floats, as numeric\n",
        "    num_mask = (mds['col_type'].isna()) & (mds['dtype'].astype('str').str.startswith('float') | mds['dtype'].astype('str').str.startswith('int'))\n",
        "    mds.loc[num_mask, 'col_type'] = 'continuous'\n",
        "\n",
        "  # ----------------------------\n",
        "  # Summarise metadata about all the columns in the df\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=False, use_fd=False)\n",
        "  def metadata_summary (self, df):\n",
        "    # Each DataFrame column is a column in the describe() output. \n",
        "    mds = df.describe(include = 'all')\n",
        "    df_len = len(df)\n",
        "\n",
        "    # Datatypes and total counts\n",
        "    mds.loc['dtype'] = df.dtypes\n",
        "    mds.loc['size'] = df_len\n",
        "\n",
        "    # Most common value and % occurrence of that value\n",
        "    for col in mds.columns:\n",
        "      vc = df[col].value_counts()\n",
        "      mds.loc['mode', col] = vc.idxmax()\n",
        "      mds.loc['mode %', col] = round(vc.max() / df_len, 3) * 100\n",
        "\n",
        "    # Count and % of missing values\n",
        "    # !!!!!! Different ways of calculating, keep only one\n",
        "    mds.loc['missing'] = len(df.index) - df.count()\n",
        "    mds.loc['% count'] = df.isnull().mean() * 100\n",
        "    mds.loc['% missing'] = df.isnull().sum()/df_len*100\n",
        "    mds.loc['% miss'] = df.isna().mean().round(4) * 100\n",
        "\n",
        "    # Each DataFrame column now has a row in the 'mds' summary\n",
        "    mds = mds.transpose()\n",
        "\n",
        "    # Fill in the feature types for continuous, categorical etc.\n",
        "    self._feature_types(mds)\n",
        "\n",
        "    # For some reason, some Dataframes don't have 'unique' or 'min', 'mean' etc in their describe output\n",
        "    # Include 'unique' in the output if present\n",
        "    unique_cols = []\n",
        "    if ('unique' in mds.columns):\n",
        "      unique_cols = ['unique']\n",
        "\n",
        "    # Include 'min', 'mean' etc if present\n",
        "    stat_cols = []\n",
        "    if ('mean' in mds.columns):\n",
        "      stat_cols = ['min', 'mean', '50%', 'max']\n",
        "\n",
        "    cols = ['dtype', 'size', 'missing', '% miss', 'mode', 'mode %', 'col_type'] + unique_cols + stat_cols\n",
        "    mds = mds[cols]\n",
        "    return mds\n",
        "\n",
        "  # ----------------------------\n",
        "  # Calculate the distribution of the 'Many' values in the right hand side of a\n",
        "  # One-Many (or One-One) Foreign Key relationship.\n",
        "  #\n",
        "  # For example, if the right-hand DataFrame has key columns as:\n",
        "  # [fk1, fk2, pk1, pk2, pk3] \n",
        "  #    where [fk1, fk2] are foreign keys to the left-hand DataFrame\n",
        "  # \n",
        "  # For each unique combination of [fk1, fk2], how many unique combinations of\n",
        "  # [pk1, pk2, pk3] are there? That is the fanout or cardinality of the 'Many' \n",
        "  # side of the relationship.\n",
        "  # For one combination of [fk1, fk2], we could have a fanout of 3, say. In other\n",
        "  #    words, there are 3 combinations of [pk1, pk2, pk3] for that [fk1, fk2]\n",
        "  # Similarly, other combinations of [fk1, fk2] would have other fanout values.\n",
        "  #\n",
        "  # Here, we calculate the distribution of those fanout values ie. how many \n",
        "  # occurrences of each fanout value do we have.\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=False, use_fd=True)\n",
        "  def fk_relations_fan(self, df, fd, levels=0, **kwargs):\n",
        "    key_cols = fd['key']\n",
        "\n",
        "    lev = levels + 1\n",
        "    lev_col = key_cols[:lev]\n",
        "    size_col = key_cols[lev]\n",
        "    df_fan = df.groupby(lev_col)[size_col].nunique().sort_values(ascending=False).reset_index()\n",
        "\n",
        "    return (df_fan, size_col)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Select some sample rows for exploratory analysis from two dfs which have a \n",
        "  # One-Many (or One-One) Foreign Key relationship. Since the data can be large\n",
        "  # we fetch only a subset where the 'Many' side of the relationship has a count\n",
        "  # of 'size_val'. Also since the foreign key could have multiple index columns,\n",
        "  # we specify the 'levels' at which we take the count.\n",
        "  #\n",
        "  # !!!!!! NB: We still need to handle the case where key_cols1 has multiple columns\n",
        "  # and so vals2_match is a tuple and gb2_match.values is a dataframe not a series\n",
        "  # ----------------------------\n",
        "  @df_process(save_res=False, use_fd=True)\n",
        "  def fk_relations(self, df, right_df, fd, right_fd, size_val=3, max_rows=10, levels=0, **kwargs):\n",
        "    left_df, left_fd = df, fd\n",
        "    df1, df2 = left_df, right_df\n",
        "    key_cols1 = left_fd['key']\n",
        "    key_cols2 = right_fd['key']\n",
        "    cols1 = left_fd['primary']\n",
        "    cols2 = right_fd['primary']\n",
        "\n",
        "    # Group at the given level, and get the size (ie. number of rows) of each group\n",
        "    lev = levels + 1\n",
        "    lev_col = key_cols2[:lev]\n",
        "    size_col = key_cols2[lev]\n",
        "    gb2 = df2.groupby(lev_col)[size_col].nunique().reset_index()\n",
        "\n",
        "    # Get the groups that have the given group size 'size_val'\n",
        "    # Then, truncate if we get more than 'max_rows' groups\n",
        "    # Then, get the key IDs of the matching groups\n",
        "    len_lev_col = len(lev_col)\n",
        "    gb2_match = gb2[gb2[size_col] == size_val].iloc[:max_rows, :len_lev_col]\n",
        "    if (len_lev_col > 1):\n",
        "      vals2_match = [tuple(val) for val in gb2_match.values]\n",
        "    else:\n",
        "      vals2_match = gb2_match.values[:, 0]\n",
        "\n",
        "    # Now use those matching key IDs to select a subset of rows from the dataframe\n",
        "    df2_sub = df2.set_index(lev_col).loc[vals2_match, key_cols2[lev:]+cols2].reset_index()\n",
        "\n",
        "    keys_intersection = list(set(key_cols1) & set(key_cols2))\n",
        "    df1_sub = df1[key_cols1+cols1]\n",
        "    merge_df = df1_sub.merge(df2_sub, how='inner', left_on=keys_intersection, right_on=keys_intersection, suffixes=('_l', '_r'))\n",
        "\n",
        "    # Select the columns we need\n",
        "    keys_union = key_cols1 + [key2 for key2 in key_cols2 if (key2 not in key_cols1)]\n",
        "    merge_df = merge_df.set_index(keys_union).sort_values(keys_union)\n",
        "    return merge_df\n",
        "\n",
        "  # ----------------------------\n",
        "  # Reduce memory usage by optimising the data types of each column\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def reduce_mem(df, name):\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    \n",
        "    len_df = len(df.index)\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != object:\n",
        "            # Find the min and max range of values in each column and then use\n",
        "            # the smallest data type that can fit those values.\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.uint8).min and c_max < np.iinfo(np.uint8).max:\n",
        "                    df[col] = df[col].astype(np.uint8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.uint16).min and c_max < np.iinfo(np.uint16).max:\n",
        "                    df[col] = df[col].astype(np.uint16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.uint32).min and c_max < np.iinfo(np.uint32).max:\n",
        "                    df[col] = df[col].astype(np.uint32)                    \n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "                elif c_min > np.iinfo(np.uint64).min and c_max < np.iinfo(np.uint64).max:\n",
        "                    df[col] = df[col].astype(np.uint64)\n",
        "\n",
        "            elif str(col_type)[:5] == 'float':\n",
        "                # Commented out because arithmetic with float16 results in\n",
        "                # non-deterministic results due to floating point precision.\n",
        "                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                #    df[col] = df[col].astype(np.float16)\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "        elif (False):\n",
        "            # Commented out because categorical columns causes huge performance issues\n",
        "            # with rollups - crosstab, pivot_tables\n",
        "            # If the unique values are below a threshold, convert to categorical\n",
        "            assert(False)\n",
        "            unique_val = df[col].nunique()\n",
        "            if ((unique_val < 500) and (unique_val / len_df < 0.1)):\n",
        "              df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    mem_reduction_pct = (100 * (start_mem - end_mem) / start_mem)\n",
        "    print(f'{name} memory reduced by {mem_reduction_pct:.1f}% from {start_mem:.2f} MB to {end_mem:.2f} MB')\n",
        "    \n",
        "    return df\n",
        "\n",
        "  # UNUSED METHODS\n",
        "  # They are alternate implementations for functionality with better implementations\n",
        "  # above. But they are left here as examples of different ways to achieve the\n",
        "  # functionality\n",
        "  # ===============================================================\n",
        "\n",
        "  # ----------------------------\n",
        "  # Alternate implementation for getting rolled up counts of categorical columns\n",
        "  # instead of using crosstab (but doesn't do normalise)\n",
        "  # ----------------------------\n",
        "  def WAIT_roll_cat_alternate(df):\n",
        "    df_count = df.pivot_table(index=index, columns=[col], values=count_col, aggfunc='count', fill_value=0)\n",
        "    col_headers = df_count.columns.levels[1]\n",
        "    df_count.columns = [f'{col_headers.name}_{val}' for val in col_headers.to_list()]\n",
        "\n",
        "  # ----------------------------\n",
        "  # Alternate implementation to flatten multi-level column index\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def WAIT_flatten_col_basic(df):\n",
        "      levels = df.columns.nlevels\n",
        "      if (levels == 2):\n",
        "        df.columns = df.columns.map('{0[0]}_{0[1]}'.format)\n",
        "      elif (levels == 3):\n",
        "        df.columns = df.columns.map('{0[0]}_{0[1]}_{0[2]}'.format)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Alternate implementation to flatten multi-level column index\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def WAIT_flatten_col_names(df):\n",
        "      col_names=[]\n",
        "      for col, agg_fn in df.columns:\n",
        "        col_names.append(f'{col}_{agg_fn}')\n",
        "      df.columns = col_names\n",
        "\n",
        "  # ----------------------------\n",
        "  # Alternate implementation to flatten multi-level column index\n",
        "  # ----------------------------\n",
        "  @staticmethod\n",
        "  def WAIT_flatten_col_alternate(df):\n",
        "      levels = df.columns.nlevels\n",
        "      if (levels > 1):\n",
        "        s = '_'.join([f'{{0[{i}]}}' for i in range(levels)])\n",
        "        df.columns = df.columns.map(s.format)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Transform - not used right now, only for testing\n",
        "  # ----------------------------\n",
        "  def test_xform(self, obj):\n",
        "    return obj\n",
        "\n",
        "  # ----------------------------\n",
        "  # Min-Max Scaling - not used right now\n",
        "  # ----------------------------\n",
        "  def minmax_scaling (df, cols):\n",
        "      # Get np array for scaling. \n",
        "      # Adjust to 2D if cols contains only a single element\n",
        "      x = df[cols].values\n",
        "      if (x.ndim == 1):\n",
        "          x = x.reshape(-1,1)\n",
        "      \n",
        "      min_max_scaler = preprocessing.MinMaxScaler(feature_range =(0, 1)) \n",
        "      x_after_min_max_scaler = min_max_scaler.fit_transform(x)\n",
        "      df[cols] = x_after_min_max_scaler\n",
        "\n",
        "  # ----------------------------\n",
        "  # Helper function to impute values for missing column values using sklearn's\n",
        "  # SimpleImputer. It is not used now because of extreme performance issues for\n",
        "  # strategy 'most_frequent' which computes the mode(). We now use Pandas methods\n",
        "  # to impute these values.\n",
        "  # ----------------------------\n",
        "  from sklearn.impute import SimpleImputer\n",
        "  def _sk_impute_values(self, df, cols, strategy, fill_value=None):\n",
        "    imp_df = df[cols]\n",
        "    imr = SimpleImputer(missing_values=np.nan, strategy=strategy, fill_value=fill_value)\n",
        "    imr = imr.fit(imp_df)\n",
        "    df[imp_df.columns] = imr.transform(imp_df.values)\n",
        "    return (imr.statistics_.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8sH3Uo6IRA1"
      },
      "source": [
        "#### Tabular Data Bundle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14j3BWrlUShB"
      },
      "source": [
        "#export\n",
        "\n",
        "class FeatureList():\n",
        "  def __init__(self):\n",
        "    self.fss = {}\n",
        "\n",
        "  def add_fs(self, fs):\n",
        "    self.fss[fs.fs_name] = fs\n",
        "\n",
        "  def get_cols(self, fs_names=None):\n",
        "    fs_names = fs_names if fs_names else self.fss.keys()\n",
        "\n",
        "    cat_cont = [self.fss[fs_name].get_cols() for fs_name in fs_names]\n",
        "    cats, conts = zip(*cat_cont)\n",
        "    \n",
        "    # Flatten the list of lists\n",
        "    cat_cols = [cat_col for cat in cats for cat_col in cat]\n",
        "    # Remove duplicates while preserving the order\n",
        "    # 'Seen' is a set of all items seen so far. As we go through every item we add it\n",
        "    # to the set if it isn't there already. \n",
        "    # seen.add() always returns true, so basically the condition says to select each\n",
        "    # item 'x' if it is not already in the 'seen' set.\n",
        "    # See https://stackoverflow.com/questions/45746312/one-liner-to-remove-duplicates-keep-ordering-of-list\n",
        "    seen = set()\n",
        "    cat_cols = [x for x in cat_cols if not (x in seen or seen.add(x))]\n",
        "\n",
        "    # Flatten the list of lists\n",
        "    cont_cols = [cont_col for cont in conts for cont_col in cont]\n",
        "    # Remove duplicates while preserving the order\n",
        "    seen = set()\n",
        "    cont_cols = [x for x in cont_cols if not (x in seen or seen.add(x))]\n",
        "\n",
        "    return (cat_cols, cont_cols)\n",
        "\n",
        "class FeatureSet():\n",
        "  def __init__(self, fs_name, cat_cols, cont_cols):\n",
        "    self.fs_name = fs_name\n",
        "    self.cat_cols, self.cont_cols = cat_cols, cont_cols\n",
        "\n",
        "  def get_cols(self):\n",
        "    return (self.cat_cols, self.cont_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP9D56IxYdtb"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Tabular data preparation pipeline\n",
        "#----------------------------------------------------\n",
        "class TabularDataBundle(DataBundle):\n",
        "  def __init__(self, csv_path, test_csv_path=None, related_csv_paths=None, prepare_fn=None, \n",
        "               test_data=False, missing_thresh=0.5, add_flag=False, ordinal_vals={}, **kwargs):\n",
        "    self.tgt_range = None\n",
        "\n",
        "    print ('--------- Tabular Home Credit DataBundle init', csv_path, test_csv_path, related_csv_paths)\n",
        "\n",
        "    # Load all rows from the given CSV file\n",
        "    # Split randomly based on a percentage ratio for training and validation\n",
        "    # 'x' items are taken from 'Reviews' column as text sentences and\n",
        "    # 'y' labels are taken from 'Sentiments' column as class name labels\n",
        "    # Convert the 'x' items from Sentences to Words to Word Ids\n",
        "    # Convert the 'y' items from Class Names to Class Ids\n",
        "\n",
        "    # load takes a processor function which CSVItemContainer calls after creating the Tabular List. This function will be on the DataBundle.\n",
        "    # This processor does all data cleaning, merging of related dfs, feature engineering etc. It ends with a Tabular List that has a single\n",
        "    # df only, with items matching the length, and with a feature dict. Until that point, the 'items' are not used at all during the\n",
        "    # processor func. After this point the Tabular List with df is fully ready for particpating in the rest of the Data Bundle process. So\n",
        "    # it can get split, and x/y extracted.\n",
        "    #\n",
        "    # The TabularDataBundle base class defines a generic processor func, which all child DataBundles inherit. The base function creates\n",
        "    # items on the ItemList which match the df length, and it also sets the feature dict including all columns created during feature\n",
        "    # engineering in the processor.\n",
        "    #\n",
        "    # Load also takes an optional 'test_csv' argument for loading the test data.\n",
        "    # The split function splits the train file into train and valid and takes the test file as is. So all the split functions take either \n",
        "    # a 'test_ratio' or a 'test_data' flag. The test data flag points to an attribute on the itemlist which contains the test data??\n",
        "    #\n",
        "    # The extract_colval is used to extract column data as a list, do some processing on it and convert it to another target class. In the\n",
        "    # case of Tabular, we'll use a select_col function which will select the non target and target columns and end with the same Tabular\n",
        "    # target class.\n",
        "    #\n",
        "    # The convert_x_params can then be quite generic and cover just the to_cat, fill_missing and normalise steps.\n",
        "    #\n",
        "    # The Tabular List methods are therefore groupable into three buckets:\n",
        "    #   1. Those needed for EDA (use feature dict)\n",
        "    #   2. Those needed for pre-processing (don't use feature dict)\n",
        "    #   3. Those needed for convert_ing -- these are the only ones which take convert_state, in_train etc. (use feature dict)\n",
        "    #\n",
        "    # The workflow then is like this:\n",
        "    #   1. Manually call load on the DataBundle without any processor\n",
        "    #   2. Then do the data exploration EDA steps - metadata, relation, distribution graphs - these are all read-only.\n",
        "    #   2a.  NB: After metadata, the feature dict can get set and gets used for the remaining EDA operations. This is just the initial feature dict, which\n",
        "    #       will change later after processing once all new features are added.\n",
        "    #   3. Now run the full-fledged DataBundle with a processor. Incrementally, during dev, you can keep adding more and more steps one by one to the processor.\n",
        "    #       After each step, you can do any output of sample(), or graphing that you want to. As you are iterating you can keep refining the feature dict\n",
        "    #       as you go, if you want to.\n",
        "    #   4. Once you are happy with all the processor steps and it is ready, then define the final feature dict, and you can then just continue running \n",
        "    #       the full-fledged DataBundle just as you did in #3 above and feed the DataLoader to the model.\n",
        "\n",
        "    load_params = {'source': CSVItemContainer, 'target_cls': TabularItemList, 'csv_path': csv_path, 'test_csv_path': test_csv_path, 'related_csv_paths': related_csv_paths, 'prepare_fn': prepare_fn}\n",
        "    split_params = {'split_procedure': 'split_sequential', 'train_ratio': 0.8, 'valid_ratio': 0.2, 'test_data': test_data}\n",
        "    extract_x_params = {'extract_procedure': 'extract_type', 'target_cls': TabularItemList, 'cols_type': 'cat_cont'}\n",
        "    extract_y_params = {'extract_procedure': 'extract_type', 'target_cls': TabularItemList, 'cols_type': 'tgt'}\n",
        "\n",
        "    convert_x_params = [\n",
        "         {'target_cls': TabularItemList, 'convert_procedure': 'remove_inf'},\n",
        "         {'target_cls': TabularItemList, 'convert_procedure': 'check_missing', 'missing_thresh': missing_thresh, 'add_flag': add_flag},\n",
        "         {'target_cls': TabularItemList, 'convert_procedure': 'fill_missing', 'cols': None, 'strategy': 'median'},\n",
        "         {'target_cls': TabularItemList, 'convert_procedure': 'normalise', 'cols': None},\n",
        "         {'target_cls': TabularItemList, 'convert_procedure': 'to_cat', 'cols': None, 'ordinal_vals': ordinal_vals},\n",
        "         {'target_cls': TabularItemList, 'convert_procedure': 'to_np'},\n",
        "    ]\n",
        "    convert_y_params = [\n",
        "         {'target_cls': TabularItemList, 'convert_procedure': 'to_np'}\n",
        "    ]\n",
        "    xform_x_params = [\n",
        "#        {'xform_procedure': 'test_xform'}, \n",
        "    ]\n",
        "    dl_params = (\n",
        "        {'bs': 64, 'sampler_fn': RandomSampler},        # for training\n",
        "        {'bs': 64, 'sampler_fn': SequentialSampler}     # for valid/test\n",
        "    )\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_texts'\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=xform_x_params, dl_params=dl_params)\n",
        "\n",
        "  # ----------------------------\n",
        "  # This is the final step of the 'Prepare' processing. By this time, all data\n",
        "  # cleaning and feature generation is done, and all related dfs have been merged\n",
        "  # into the main df. We are given the final set of continuous and categorical\n",
        "  # variables. So here, the final ItemList is re-created with only the main\n",
        "  # df and with just the columns that are required. The Feature Dict is also defined\n",
        "  # based on the given variables.\n",
        "  # ----------------------------\n",
        "  def prepare(self, il, fl, tgt_cols):\n",
        "    # The df is now fully prepared after all the data processing and wrangling.\n",
        "\n",
        "    cat_cols, cont_cols = fl.get_cols()\n",
        "\n",
        "    # Convert categorical variables to Pandas Categorical data type. This ensures\n",
        "    # that the category values are based on both training and validation rows before \n",
        "    # splitting.\n",
        "    il.df[cat_cols] = il.df[cat_cols].astype('category')\n",
        "\n",
        "    cols_all = cat_cols + cont_cols + tgt_cols\n",
        "    il.df = il.df[cols_all].reset_index(drop=True)\n",
        "\n",
        "    features_dict = {\n",
        "        'nominal'     : cat_cols,\n",
        "        'continuous'  : cont_cols,\n",
        "        'target'      : tgt_cols,\n",
        "    }\n",
        "    il.add_features_dict(features_dict, df_name=None, mds=None)\n",
        "\n",
        "    # Reset the ItemList by re-creating it with the finished df and without the \n",
        "    # related dfs.\n",
        "    il = il.__class__(None, il.df)\n",
        "    return il\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get count of continuous columns and category names for categorical columns, as\n",
        "  # required to build the model\n",
        "  # ----------------------------\n",
        "  def col_szs(self):\n",
        "    continuous_cols = self.train_ds.x._continuous_cols()\n",
        "    n_cont = len(continuous_cols)\n",
        "\n",
        "    cat_cols = self.train_ds.x._categorical_cols()\n",
        "    cat_szs = [len(self.convert_state_x[f'Category_{cat_col}']) + 1 for cat_col in cat_cols]\n",
        "\n",
        "    n_tgt = len(self.train_ds.y._target_cols())\n",
        "\n",
        "    return n_cont, cat_szs, n_tgt, self.tgt_range\n",
        "\n",
        "  # ----------------------------\n",
        "  # Display metadata about each column in the DataFrame\n",
        "  # ----------------------------\n",
        "  def display_metadata(self, il, df_name=None):\n",
        "    mds = il.metadata_summary(df_name=df_name)\n",
        "    DisplayData.display_summary(mds)\n",
        "    return mds\n",
        "\n",
        "  # ----------------------------\n",
        "  # Show one-many relationship between two DataFrames and display sample rows \n",
        "  # ----------------------------\n",
        "  def display_relation(self, il, right_df_name, df_name=None, size_val=3, max_rows=10, levels=0):\n",
        "    fan_df, x = il.fk_relations_fan(df_name=right_df_name, levels=0)\n",
        "\n",
        "    merge_df = il.fk_relations(df_name=df_name, right_df_name=right_df_name, size_val=size_val, max_rows=max_rows, levels=levels)\n",
        "    DisplayData.display_rel(fan_df, x, merge_df)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Compare two DataBundles for equality, for debugging purposes. \n",
        "  # ----------------------------\n",
        "  def compare_db(self, db1, db2):\n",
        "    all_il1 = getattr(db1, 'all_rows', None)\n",
        "    all_il2 = getattr(db1, 'all_rows', None)\n",
        "    if (all_il1 and all_il2):\n",
        "      # Compare 'load' df if present\n",
        "      all_df1 = all_il1.df\n",
        "      all_df2 = all_il2.df\n",
        "      all_unmatched = all_il1.compare_df(all_df1, all_df2, name='All Rows')\n",
        "      return all_unmatched\n",
        "    else:\n",
        "      # Otherwise compare 'post_load' df and arr\n",
        "      tr_unmatcheds = self._compare_ds(db1.train_ds, db2.train_ds, name='Training')\n",
        "      val_unmatcheds = self._compare_ds(db1.valid_ds, db2.valid_ds, name='Valid')\n",
        "      return (tr_unmatcheds + val_unmatcheds)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Compare two Datasets for equality, for debugging purposes\n",
        "  # ----------------------------\n",
        "  def _compare_ds(self, ds1, ds2, name):\n",
        "    x_il1, y_il1, x_il2, y_il2 = ds1.x, ds1.y, ds2.x, ds2.y\n",
        "    x_df1, y_df1, x_arr1, y_arr1 = x_il1.df, y_il1.df, x_il1.arr, y_il1.arr\n",
        "    x_df2, y_df2, x_arr2, y_arr2 = x_il2.df, y_il2.df, x_il2.arr, y_il2.arr\n",
        "    \n",
        "    assert(x_il1._categorical_cols() == x_il2._categorical_cols())\n",
        "    assert(x_il1._continuous_cols() == x_il2._continuous_cols())\n",
        "    n_cat = len(x_il1._categorical_cols())\n",
        "\n",
        "    # Compare training df\n",
        "    x_unmatched = x_il1.compare_df(x_df1, x_df2, name=f'{name} X')\n",
        "    y_unmatched = y_il1.compare_df(y_df1, y_df2, name=f'{name} Y')\n",
        "\n",
        "    # Compare training arr\n",
        "    x_il1.compare_arr(x_arr1, x_arr2, y_arr1, y_arr2, n_cat)\n",
        "\n",
        "    return (x_unmatched + y_unmatched)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Helper function for making subsets which should be overridden by the subclass\n",
        "  # to define parameters for related dfs\n",
        "  # ----------------------------\n",
        "  @classmethod\n",
        "  def subset_related(cls):\n",
        "    idx_related = None\n",
        "    merge_fn = None\n",
        "    return (idx_related, merge_fn)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Create smaller subsets of the main and test dfs, along with matching rows from\n",
        "  # all the related dfs\n",
        "  #\n",
        "  # TODO: !!!!!!!!!! Currently, this assumes that there is a single merge_fn which merges\n",
        "  # all related files together at once. However such a function may not exist in most cases because\n",
        "  # most of the time, related files will get aggregated and rolled up into the main file. Also\n",
        "  # it may not be possible to merge all related files into the main file at once. Some related\n",
        "  # files may be hierarchically related at a second or deeper level, and will get merged into \n",
        "  # another related file first using a different primary key, and then that file gets merged \n",
        "  # to the main file eg. in the Home Credit application, bureau_balance first gets merged into\n",
        "  # bureau, when then gets merged into the main file.\n",
        "  #\n",
        "  # So basically, the approach we need to use here is for the merge to happen one related file\n",
        "  # at a time. So each related file might have its own separate merge function. So one\n",
        "  # related file gets merged and a subset gets created for it, then you loop to the next one\n",
        "  # and so on. Note that it could be more than one file as well, like in the hierarchical \n",
        "  # example above, you might merge two related files into the main file together. The real point\n",
        "  # is that it will not be a merge_all.\n",
        "  # ----------------------------\n",
        "  def make_subset(self, idx_related, merge_fn, subset_dir, subset_sz=100):\n",
        "    il = self.process(steps=['load'])\n",
        "\n",
        "    main_path = self.load_params['csv_path']\n",
        "    test_path = self.load_params['test_csv_path']\n",
        "\n",
        "    # Keep only the first 'subset_sz' of main df and test df, if it is present\n",
        "    # Save them to subset files\n",
        "    subset_dir.mkdir(exist_ok=True)\n",
        "    il.df = il.df.iloc[:subset_sz]\n",
        "    il.df.to_csv(subset_dir / main_path.name, index=False)\n",
        "    if (test_path):\n",
        "      test_df = il.test_rows.df.iloc[:subset_sz]\n",
        "      test_df.to_csv(subset_dir / test_path.name, index=False)\n",
        "      il.df = il.df.append(test_df)\n",
        "\n",
        "    # Now find the rows from all the related dfs which would get joined to\n",
        "    # the subset rows\n",
        "\n",
        "    if (not idx_related):\n",
        "      return\n",
        "\n",
        "    # Add the index as its own column named 'idx_<abbr>' so that we\n",
        "    # can track each row number from each related df\n",
        "    for df_name, idx_name in idx_related.items():\n",
        "      il.add_index_col(df_name=df_name, idx_col='idx_' + idx_name)\n",
        "\n",
        "    # Join all the dfs into a single df\n",
        "    merge_fn(il)\n",
        "\n",
        "    # Since the index columns are also merged, they can be used to identify which row\n",
        "    # numbers from the original related dfs were joined together. Note that if the \n",
        "    # same df was joined more than once, the index column will occur multiple times, each\n",
        "    # with a different suffix.\n",
        "\n",
        "    # Create an index df by selecting just the index columns from the merged df\n",
        "    idx_names = [col for col in il.df.columns if col.startswith('idx_')]\n",
        "    idx_df = il.df[idx_names].copy()\n",
        "\n",
        "    # Now discard the merged df and reload the original dfs once again\n",
        "    il = self.process(steps=['load'])\n",
        "\n",
        "    # Use the index df to pull out the corresponding row numbers from each original df\n",
        "    # Save those rows to the subset file\n",
        "    for df_name, idx_name in idx_related.items():\n",
        "      df = il._get_df(df_name)\n",
        "      # Get row ids from all the index columns for this original df, since the index column might \n",
        "      # occur multiple times\n",
        "      idxs = [idx_df[col].tolist() for col in idx_df.columns if col.startswith('idx_' + idx_name)]\n",
        "      # Flatten the list of sublists, and remove duplicates using set()\n",
        "      idxs = set([idx for sublist in idxs for idx in sublist])\n",
        "      # Use the row ids to get the corresponding rows from the df\n",
        "      df = df.loc[idxs]\n",
        "      # Save the subset file\n",
        "      df.to_csv(subset_dir / (df_name + '.csv'), index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTa8Tm8rchii"
      },
      "source": [
        "# -------------------------\n",
        "# WORKFLOW\n",
        "# -------------------------\n",
        "# Download data\n",
        "# Main File only\n",
        "  # EDA - MDS (TabularBaseDB.load)\n",
        "  # EDA - Distrib\n",
        "  # Cleaning - Prepare (MyTabularDB.load)\n",
        "  # Subset - 10 batches\n",
        "  # Convert - Continuous columns only - check Fill Missing and Normalise\n",
        "  # Run Model - DTR, Recorder, Metrics, Loss\n",
        "  # Convert - Add Categorical columns - check To Cat\n",
        "  # Feature Engg - Prepare\n",
        "  # Subset - 100 batches\n",
        "# Related - one file at a time - 100 batches\n",
        "  # EDA - MDS, Distrib, Related\n",
        "  # Cleaning\n",
        "  # Subset - add Related\n",
        "  # Rollup - Prepare\n",
        "  # Run Model\n",
        "# Feature Engg - Prepare\n",
        "# Subset - 1000 batches\n",
        "\n",
        "# -------------------------\n",
        "# DATABUNDLE PROCESS and MODEL\n",
        "# -------------------------\n",
        "# Load and Prepare (Save/Load Pickle)\n",
        "# Process - Split, Extract and Convert (Save/Load Pickle)\n",
        "# Model (Save/Load Weights)\n",
        "\n",
        "# -------------------------\n",
        "# COMPARE\n",
        "# -------------------------\n",
        "# Compare Prepared data (df)\n",
        "# Compare Processed data (train dfx/dfy and val dfx/dfy)\n",
        "# Compare Np Array data (np array train x/y and val x/y)\n",
        "# Compare Model results (results df)\n",
        "\n",
        "# -------------------------\n",
        "# OBJECTS\n",
        "# -------------------------\n",
        "# Tabular ItemList\n",
        "# Tabular DataBundle\n",
        "# Arch - model, hook layer groups, discrim lr param groups, load/save weights\n",
        "# App - pre-process data, load data, create arch, run train, run predict\n",
        "# Trainer - runs training loop\n",
        "# Optimiser (updates weights using gradients and hyperparam) and HyperParam (varies hyperparams in two dimensions across param groups (discrim) and across batches (scheduler))\n",
        "# Metrics - compute metrics\n",
        "# Hooks and Debug Callback - probes for capturing internal layer values\n",
        "# Debug Tracker - collects all Debug/Hook values, Metrics, HyperParams.\n",
        "# Reporter/Presenter/Recorder - outputs to DF, Tensorboard and Progress. DF outputs to Seaborn/Matplotlib\n",
        "\n",
        "   # The workflow then is like this:\n",
        "    #   1. Manually call load on the DataBundle without any processor\n",
        "    #   2. Then do the data exploration EDA steps - metadata, relation, distribution graphs - these are all read-only.\n",
        "    #   2a.  NB: After metadata, the feature dict can get set and gets used for the remaining EDA operations. This is just the initial feature dict, which\n",
        "    #       will change later after processing once all new features are added.\n",
        "    #   3. Now run the full-fledged DataBundle with a processor. Incrementally, during dev, you can keep adding more and more steps one by one to the processor.\n",
        "    #       After each step, you can do any output of sample(), or graphing that you want to. As you are iterating you can keep refining the feature dict\n",
        "    #       as you go, if you want to.\n",
        "    #   4. Once you are happy with all the processor steps and it is ready, then define the final feature dict, and you can then just continue running \n",
        "    #       the full-fledged DataBundle just as you did in #3 above and feed the DataLoader to the model.\n",
        "\n",
        "      # Normal Workflow - Break down the problem into smaller pieces\n",
        "     # Run with base-data ie only 'train.csv' columns with no related.\n",
        "     # Subset with say 10 batches - ie. 640 rows. Or with 10000 rows?\n",
        "     # Test with only continuous columns first\n",
        "     # Then add categorical columns\n",
        "     # Test that fill missing and normalise give us correct results\n",
        "     # Then add rollup of related, but only one table at first. Then slowly add more tables.\n",
        "     # Then make a subset of Small (10/100 batches), then Mid (1000) and Full(10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ymOtKzCFKMe"
      },
      "source": [
        "#### NpItemList"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKNPIfKTFMkB"
      },
      "source": [
        "#----------------------------------------------------\n",
        "# ItemList for a Numpy array. Each row is an item. Since the actual data \n",
        "# is stored in the array, the items themselves are just a range of numbers \n",
        "# from 0 to 'array_length - 1'.\n",
        "#----------------------------------------------------\n",
        "class NpItemList(ItemList):\n",
        "  # ----------------------------\n",
        "  # ----------------------------\n",
        "  def __init__(self, items, arr, **kwargs):\n",
        "    # If present, 'items' should be the same length as the array\n",
        "    assert (len(items) == arr.shape[0])\n",
        "    self.arr = arr\n",
        "    super().__init__(items)\n",
        "    self._copy_new.append('arr')\n",
        "\n",
        "  # ----------------------------\n",
        "  # To get the actual data object, we use the item as an index into the\n",
        "  # numpy array's rows. Therefore item #i corresponds to array row #i.\n",
        "  # ----------------------------\n",
        "  def _get_object(self, i):\n",
        "    return self.arr[i]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgulSb12lmi1"
      },
      "source": [
        "#### To Be Sorted - EDA and other Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7ae29a6c8db9faae2d7f3f350646d69be0b32013",
        "id": "c_TPuXQ1qzI8"
      },
      "source": [
        "# ------- THESE ARE NOT USED RIGHT NOW -----------------\n",
        "\n",
        "# CONVERT THE DATA\n",
        "# ===============================================================\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Label encode categorical columns\n",
        "# -----------------------------------------------------\n",
        "def encode_cat (df, cat_col, enc_col):\n",
        "    gle = LabelEncoder()\n",
        "    # Fill missing values with a dummy string, otherwise fit_transform throws an error\n",
        "    #cat_labels = gle.fit_transform(df[cat_col].fillna('Empty'))\n",
        "    cat_labels = gle.fit_transform(df[cat_col])\n",
        "    #cat_mappings = {index: label for index, label in enumerate(gle.classes_)}\n",
        "    df[enc_col] = cat_labels\n",
        "    \n",
        "# -----------------------------------------------------\n",
        "# One-hot encode categorical columns\n",
        "# -----------------------------------------------------\n",
        "def encode_cat_onehot (df, cat_col):\n",
        "    # Drop the first encoded value to avoid the \"dummy variable trap\"\n",
        "    # NB: You can also use sklearn's OneHotEncoder() but that returns a numpy array\n",
        "    # which will have to be converted into a dataframe\n",
        "    onehot_df = pd.get_dummies(df[cat_col], drop_first=True)\n",
        "    #onehot_df = pd.get_dummies(df[cat_col].fillna('Empty'), drop_first=True)\n",
        "    return (pd.concat([df, onehot_df], axis=1))\n",
        "\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Get a list of all the unique values for a feature (usually categorical)\n",
        "# -----------------------------------------------------\n",
        "def get_unique_values (df, target):\n",
        "      return df[target].unique()\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Get a count of unique values for 'Object' columns\n",
        "# -----------------------------------------------------\n",
        "def get_unique_counts (df):\n",
        "      return (df.select_dtypes('object').apply(pd.Series.nunique, axis = 0))\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Print the most frequently occuring values of each categorical column\n",
        "# -----------------------------------------------------\n",
        "def count_freq (df, cat_cols):\n",
        "      for col in cat_cols:\n",
        "          print (df[col].value_counts().nlargest())\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Visualise basic stats of the data\n",
        "# -----------------------------------------------------\n",
        "def data_stats (self):\n",
        "    df = self.df\n",
        "    p = Plotter()\n",
        "    \n",
        "    # Get all the datatypes\n",
        "    dd = self.get_dtypes (df)\n",
        "    p.create_subplot ('count', dd, 'Column Type')\n",
        "    \n",
        "    # Unique value counts\n",
        "    ud = self.get_unique_counts(df)\n",
        "    p.create_subplot ('series', ud, 'dummy')\n",
        "\n",
        "    # Missing value counts\n",
        "    miss_val = self.missing_values_summary(df)\n",
        "    cols = self.get_feature_names(miss_val)\n",
        "    p.plot_grid ('multicol', miss_val, cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "84ceb1fcdda1af1ccb6c407fea7c6c9aac7b5620",
        "id": "5W-UCh_nqzIJ"
      },
      "source": [
        "# VISUALISATION PLOTTING\n",
        "# ===============================================================\n",
        "\n",
        "def plot_corr(df):\n",
        "    # Heatmap of correlations\n",
        "    plt.figure(figsize = (8, 6))\n",
        "    sns.heatmap(df.corr(), vmin = -0.25, annot = True, vmax = 0.6, cmap = plt.cm.RdYlBu_r, fmt='.2f', linewidths=.05)\n",
        "    plt.title('Correlation Heatmap');\n",
        "\n",
        "# BIVARIATE VISUALISATION\n",
        "# ===============================================================\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Plots bar plots between a feature and a target pair\n",
        "# -----------------------------------------------------\n",
        "def plot_pair_target (df, features, target):\n",
        "    grid_size = 4\n",
        "    fig = plt.figure(figsize = (15, 15))\n",
        "    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    for i, feature in enumerate(features):\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "        sns.barplot(x = feature, y = target, data=df, ax = ax)\n",
        "        ax.set_xlabel(\"Values\")\n",
        "        ax.set_ylabel(\"Counts\") \n",
        "        ax.set_title('{} Totals'.format(feature), fontsize=16)\n",
        "\n",
        "# -----------------------------------------------------\n",
        "# Plot Stacked histograms of each feature vs target\n",
        "# -----------------------------------------------------\n",
        "def plot_stackhist(il, df, features, target):\n",
        "    grid_size = 4\n",
        "    fig = plt.figure(figsize = (20, 20))\n",
        "    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    # Get list of unique values of the target\n",
        "    target_values = il.get_unique_values (df, target)\n",
        "        \n",
        "    for i, feature in enumerate(features):\n",
        "        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n",
        "        # For each value of the target\n",
        "        dist = [df[df[target]==val][feature] for val in target_values]\n",
        "        ax.hist(x = dist, stacked=True, label = target_values)\n",
        "        ax.set_xlabel(\"Values\")\n",
        "        ax.set_ylabel(\"Counts\") \n",
        "        ax.set_title('{} Distribution'.format(feature), fontsize=16)\n",
        "        ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqWrvFiifAdd"
      },
      "source": [
        "### Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eugju1zmfEVV"
      },
      "source": [
        "#----------------------------------------------------\n",
        "# Image Classification data preparation pipeline\n",
        "#----------------------------------------------------\n",
        "class ImageClassificationDataBundle(DataBundle):\n",
        "  def __init__(self, folder_path):\n",
        "    print ('--------- Image Classification DataBundle init', folder_path)\n",
        "\n",
        "    # Get list of image file extensions\n",
        "    # Load all files with image extensions from the given folder path\n",
        "    # Split based on training and validation sub-folders in the top-level folder path\n",
        "    # Loaded files can be directly used as 'x' items, no extraction necessary\n",
        "    # Use the parent folder name of each file as the class name for the 'y' labels. Done through a custom extraction function.\n",
        "    # Convert the 'x' items from Image Files to Images\n",
        "    # Convert the 'y' items from Class Names to Class Ids\n",
        "    # At runtime, dynamically read an Image and apply some image processing steps. Finally\n",
        "    # convert to tensors of floats\n",
        "\n",
        "    image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))\n",
        "    ' '.join(image_extensions)\n",
        "    load_params = {'source': FolderItemContainer, 'target_cls': ImageFileItemList, 'folder_path': folder_path, 'extensions': image_extensions}\n",
        "    split_params = {'split_procedure': 'split_path', 'train_folder': 'train', 'valid_folder': 'val'}\n",
        "    extract_x_params = {'extract_procedure': None, 'target_cls': None}\n",
        "    y_custom_fn=partial(FileNameItemList.get_ancestor, up_level=1)\n",
        "    extract_y_params = {'extract_procedure': 'extract_custom', 'target_cls': ClassNameItemList, 'custom_fn': y_custom_fn}\n",
        "    convert_x_params = [\n",
        "        {'target_cls': ImageItemList, 'convert_procedure': 'FileToImage'}, \n",
        "    ]\n",
        "    convert_y_params = [\n",
        "        {'target_cls': ClassIdItemList, 'convert_procedure': 'NameToId'} \n",
        "    ]\n",
        "    xform_x_params = [\n",
        "        {'xform_procedure': 'make_rgb'}, \n",
        "        {'xform_procedure': 'resize', 'size': 128},\n",
        "        {'xform_procedure': 'to_byte_tensor'},        \n",
        "        {'xform_procedure': 'to_float_tensor'},\n",
        "        {'xform_procedure': 'normalise'},\n",
        "        # Example only {'xform_procedure': 'Custom', 'custom_fn': func}        \n",
        "    ]\n",
        "    # These dl_params are not really required for this DataBundle. They are there as test cases for different param combinations\n",
        "    dl_params = (\n",
        "        {'bs': 6, 'sampler_fn': RandomSampler},     # for training\n",
        "        {'bs': 6, 'sampler_fn': SequentialSampler}  # for valid/test\n",
        "    )\n",
        "    self.post_proc_params = [\n",
        "        {'proc_procedure': 'set_mean_std', 'mean': [0.5, 0.5, 0.5], 'std':[0.5, 0.5, 0.5]}, \n",
        "    ]\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_images', 'figsize': (20, 5),\n",
        "        'xyz_procedures': ('image', 'label', 'label')\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=xform_x_params, dl_params=dl_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8WVkY_yfMen"
      },
      "source": [
        "indb = ImageClassificationDataBundle(path_imagenette)\n",
        "indb.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi025HxkLjHC"
      },
      "source": [
        "# Image Classification class names\n",
        "indb.valid_ds.y.class_vocab_i2n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjq7lWi2XdW9"
      },
      "source": [
        "indb.display_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fURC6mrKNxPB"
      },
      "source": [
        "inex_idxs=[10, 11, 12, 13, 14, 15, 16, 20, 21, 22, 23, 24, 42, 43, 44]\n",
        "indb.display_batch(idxs=inex_idxs, num_cols=5, figsize=(25, 15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCrxbFDISDxm"
      },
      "source": [
        "indb.train_ds.x.add_xform([\n",
        "  {'xform_procedure': 'make_rgb'},\n",
        "  {'xform_procedure': 'resize', 'size': 128},\n",
        "  {'xform_procedure': 'random_resized_crop', 'crop_size': 128},\n",
        "  {'xform_procedure': 'flip_rotate'},\n",
        "  {'xform_procedure': 'perspective_warp', 'crop_size': 128},\n",
        "  {'xform_procedure': 'aug', 'aug_name': 'Random Brightness Contrast'},\n",
        "  {'xform_procedure': 'aug', 'aug_name': 'RGB Shift'},\n",
        "  {'xform_procedure': 'to_byte_tensor'},\n",
        "  {'xform_procedure': 'to_float_tensor'},\n",
        "  {'xform_procedure': 'normalise'},\n",
        "])\n",
        "indb.train_ds.x.xforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CEeB9M_Q-YM"
      },
      "source": [
        "indb.display_batch(idxs=inex_idxs, num_cols=5, figsize=(25, 15))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lw5dyPiM83v"
      },
      "source": [
        "assert(indb.train_ds.x.pair_type is None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCnFzscpHB1_"
      },
      "source": [
        "indb = ImageClassificationDataBundle(path_imagenette)\n",
        "indb.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PU1Eh6iEnD8"
      },
      "source": [
        "num_objs = 4\n",
        "x_objs = [torch.rand((3, 128, 128)) for _ in range(num_objs)]\n",
        "y_objs = [torch.randint(5, ()) for _ in range(num_objs)]\n",
        "z_objs = [torch.randint(5, ()) for _ in range(num_objs)]\n",
        "\n",
        "indb.display_results(x_objs, y_objs, z_objs, idxs=[0, 1, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXMvvOobffLy"
      },
      "source": [
        "### Text Classification CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmDnbgCEfsSd"
      },
      "source": [
        "#----------------------------------------------------\n",
        "# Text Classification from CSV data preparation pipeline\n",
        "#----------------------------------------------------\n",
        "class TextClassificationCSVDataBundle(DataBundle):\n",
        "  def __init__(self, csv_path):\n",
        "    print ('--------- Text Classification DataBundle init', csv_path)\n",
        "\n",
        "    # Load all rows from the given CSV file\n",
        "    # Split randomly based on a percentage ratio for training and validation\n",
        "    # 'x' items are taken from 'Reviews' column as text sentences and\n",
        "    # 'y' labels are taken from 'Sentiments' column as class name labels\n",
        "    # Convert the 'x' items from Sentences to Words to Word Ids\n",
        "    # Convert the 'y' items from Class Names to Class Ids\n",
        "\n",
        "    load_params = {'source': CSVItemContainer, 'target_cls': DfItemList, 'csv_path': csv_path}\n",
        "    split_params = {'split_procedure': 'split_random', 'train_ratio': 0.8, 'valid_ratio': 0.2}\n",
        "    extract_x_params = {'extract_procedure': 'extract_colval', 'target_cls': SentenceItemList, 'col': 'reviews'}\n",
        "    extract_y_params = {'extract_procedure': 'extract_colval', 'target_cls': ClassNameItemList, 'col': 'sentiments'}\n",
        "    convert_x_params = [\n",
        "        {'target_cls': SentenceWordItemList, 'convert_procedure': 'SentenceToWord'},\n",
        "        {'target_cls': SentenceWordIdItemList, 'convert_procedure': 'WordToWordId'}\n",
        "    ]\n",
        "    convert_y_params = [\n",
        "        {'target_cls': ClassIdItemList, 'convert_procedure': 'NameToId'} \n",
        "    ]\n",
        "    # These dl_params are not really required for this DataBundle. They are there as test cases for different param combinations\n",
        "    dl_params = (\n",
        "        {'bs': 2, 'shuffle': True},     # for training\n",
        "        {'bs': 4, 'shuffle': False}     # for valid/test\n",
        "    )\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_texts'\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, dl_params=dl_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utgY1am9fkV7"
      },
      "source": [
        "tcdb = TextClassificationCSVDataBundle('tc.csv')\n",
        "tcdb.process()\n",
        "tcdb.all_rows[:2], len(tcdb.all_rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1DUxf7_MJoo"
      },
      "source": [
        "# CSV Text Classification vocab and class names\n",
        "tcdb.train_ds.x.vocab_i2w, tcdb.train_ds.y.class_vocab_i2n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdmZhfpcyKkI"
      },
      "source": [
        "tcdb.display_batch(idxs=[0, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTYFzFfHf7n7"
      },
      "source": [
        "### Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLvOzolWgCf_"
      },
      "source": [
        "#----------------------------------------------------\n",
        "# Language Model from folder data preparation pipeline\n",
        "#----------------------------------------------------\n",
        "class LanguageModelFolderDataBundle(DataBundle):\n",
        "  def __init__(self, folder_path, ds_params):\n",
        "    print ('--------- IMDB Language Model DataBundle init', folder_path, ds_params)\n",
        "\n",
        "    # Load all files with text extensions from specific sub-folders within the given folder path\n",
        "    # Split randomly based on a percentage ratio for training and validation\n",
        "    # 'x' items are taken by reading the entire document from each file as a single text sentence and\n",
        "    # 'y' labels are dummy values which will be ignored. Actual Language Model 'y' labels will be generated during training.\n",
        "    # Convert the 'x' items from Sentences to Words to Word Ids\n",
        "    # No conversion needed for the dummy 'y' items\n",
        "\n",
        "    # !!!!!!!!!!!! Split random ratios kept temporarily low to speed up text processing\n",
        "\n",
        "    txt_extensions = '.txt'\n",
        "    lm_pre_rules = [\"fixup_text\", \"replace_rep\", \"replace_wrep\", \"spec_add_spaces\", \"rm_useless_spaces\", \"sub_br\"]\n",
        "    load_params = {'source': FolderItemContainer, 'target_cls': TextFileItemList, 'folder_path': folder_path, 'include_subfolders': ['train', 'test', 'unsup'], 'extensions': txt_extensions}\n",
        "    split_params = {'split_procedure': 'split_random', 'train_ratio': 0.03, 'valid_ratio': 0.02}\n",
        "    extract_x_params = {'extract_procedure': 'extract_doc', 'target_cls': SentenceItemList, 'pre_rules': lm_pre_rules}\n",
        "    extract_y_params = {'extract_procedure': 'extract_dummy', 'target_cls': DummyItemList}\n",
        "    convert_x_params = [\n",
        "        {'target_cls': SentenceWordItemList, 'convert_procedure': 'SentenceToWord'},\n",
        "        {'target_cls': SentenceWordIdItemList, 'convert_procedure': 'WordToWordId'},\n",
        "        {'target_cls': StreamWordIdItemList, 'convert_procedure': 'WordIdToStream'}\n",
        "    ]\n",
        "    convert_y_params = [\n",
        "    ]\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_texts'\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, ds_params=ds_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJLhwxlff-Uj"
      },
      "source": [
        "lmds_params = {'target_ds': FastaiLMDataset, 'bs': 64, 'bptt': 70}\n",
        "lmdb = LanguageModelFolderDataBundle(path_imdb, lmds_params)\n",
        "lmdb.process()\n",
        "lm_vocab = lmdb.convert_state_x['vocab_i2w']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0sMxm8x0x0C"
      },
      "source": [
        "lmdb.display_batch(idxs=[0, 2, 3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IazGtlaHgKbl"
      },
      "source": [
        "### Text Classification Folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L4VY3NfpBUo"
      },
      "source": [
        "#----------------------------------------------------\n",
        "# Text Classification from Folder data preparation pipeline\n",
        "# The processing is almost identical to the Language Model from Folder, except that we leave\n",
        "# the words from each sentence separate rather than create a continuous stream. And that the\n",
        "# y labels are taken from folder names rather than dummy values \n",
        "#----------------------------------------------------\n",
        "class TextClassificationFolderDataBundle(DataBundle):\n",
        "  def __init__(self, folder_path, bs, vocab_i2w=None):\n",
        "    print ('--------- IMDB Classification DataBundle init', folder_path)\n",
        "\n",
        "    # Take data only from 'train' and 'test' sub-folders\n",
        "    # Load all files with text extensions from specific sub-folders within the given folder path\n",
        "    # Split randomly based on a percentage ratio for training and validation\n",
        "    # 'x' items are taken by reading the entire document from each file as a single text sentence and\n",
        "    # 'y' labels are taken from folder names.\n",
        "    # Convert the 'x' items from Sentences to Words to Word Ids\n",
        "    # \n",
        "\n",
        "    # !!!!!!!!!!!! Split random ratios kept temporarily low to speed up text processing\n",
        "\n",
        "    txt_extensions = '.txt'\n",
        "    lm_pre_rules = [\"fixup_text\", \"replace_rep\", \"replace_wrep\", \"spec_add_spaces\", \"rm_useless_spaces\", \"sub_br\"]\n",
        "    load_params = {'source': FolderItemContainer, 'target_cls': TextFileItemList, 'folder_path': folder_path, 'include_subfolders': ['train', 'test'], 'extensions': txt_extensions}\n",
        "    split_params = {'split_procedure': 'split_random', 'train_ratio': 0.03, 'valid_ratio': 0.02}\n",
        "    extract_x_params = {'extract_procedure': 'extract_doc', 'target_cls': SentenceItemList, 'pre_rules': lm_pre_rules}\n",
        "\n",
        "    y_custom_fn=partial(FileNameItemList.get_ancestor, up_level=1)\n",
        "    extract_y_params = {'extract_procedure': 'extract_custom', 'target_cls': ClassNameItemList, 'custom_fn': y_custom_fn}\n",
        "\n",
        "    convert_x_params = [\n",
        "        {'target_cls': SentenceWordItemList, 'convert_procedure': 'SentenceToWord'},\n",
        "        {'target_cls': SentenceWordIdItemList, 'convert_procedure': 'WordToWordId', 'vocab_i2w': vocab_i2w}\n",
        "    ]\n",
        "    convert_y_params = [\n",
        "        {'target_cls': ClassIdItemList, 'convert_procedure': 'NameToId'} \n",
        "    ]\n",
        "    # We use different samples for training and validation. Also, the custom sampler functions need to take a sort-key-function\n",
        "    # as an extra argument. The key function needs the 'data' argument pre-bound using a partial.\n",
        "    len_key_fn = lambda i,data: len(data[i])\n",
        "    dl_params = (\n",
        "        {'bs': bs, 'sampler_fn': SortishSampler, 'key_fn': len_key_fn, 'collate_fn': pad_collate},    # for training\n",
        "        {'bs': bs, 'sampler_fn': SortSampler, 'key_fn': len_key_fn, 'collate_fn': pad_collate}        # for valid/test\n",
        "    )\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_texts'\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, dl_params=dl_params)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Since we use Sorting Samplers which require a key function (which also requires a partial wrapper)\n",
        "  # we have to override the parent DataBundle with a custom get_sampler() \n",
        "  # ----------------------------\n",
        "  def get_sampler(self, ds, in_train, bs, sampler_fn, key_fn, **kwargs):\n",
        "    key=partial(key_fn, data=ds.x)\n",
        "    # The two sampler functions take different arguments. Since we know the sampler\n",
        "    # functions we can hardcode their names. The sampler_fn that is passed in is the\n",
        "    # same, but we ignore it.\n",
        "    if (in_train):\n",
        "      sampler = SortishSampler(ds.x, key=key, bs=bs)\n",
        "    else:\n",
        "      sampler = SortSampler(ds.x, key=key)\n",
        "    return sampler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtVUaP649ugf"
      },
      "source": [
        "tlmdb = TextClassificationFolderDataBundle(path_imdb, bs=4, vocab_i2w=lm_vocab)\n",
        "tlmdb.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHRSYw8NMd5S"
      },
      "source": [
        "# IMDB Text Classification vocab\n",
        "tlmdb.train_ds.x.vocab_i2w[30:40]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS9SwUCWbY5Y"
      },
      "source": [
        "### Image Segmentation Image with Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfN3DSO3DZK_"
      },
      "source": [
        "g_data_path = Path(gd_path) / 'data'\n",
        "subset_masks_path = g_data_path/'train_masks_subset'\n",
        "subset_imgs_path = g_data_path/'train_subset'\n",
        "\n",
        "masks_path = subset_masks_path\n",
        "imgs_path = subset_imgs_path\n",
        "masks_file_path = subset_imgs_path/'train_masks.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s95O1BhkMn3"
      },
      "source": [
        "def mask_name_fn(row):\n",
        "  col = 'img'\n",
        "  mn = f'{row[col][:-4]}_mask.gif'\n",
        "  return mn\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Image Segmentation preparation pipeline\n",
        "#----------------------------------------------------\n",
        "class ImageSegmentationDataBundle(DataBundle):\n",
        "  def __init__(self, csv_path, img_folder_path, mask_folder_path):\n",
        "    print ('--------- Image Segmentation DataBundle init', csv_path, img_folder_path, mask_folder_path)\n",
        "\n",
        "    # Load all rows from the given CSV file\n",
        "    # Split sequentially. based on a percentage ratio for training and validation. \n",
        "    #   We do this sequentially rather than randomly because each car is in a set of 16 \n",
        "    #   images taken from different angles. And for a particular car we don't want some\n",
        "    #   of those images to be in the training set and some in the validation set. Otherwise\n",
        "    #   it will validate on the same car on which it got trained and will give artificially\n",
        "    #   good validation results. To address this we make sure that the entire set of images\n",
        "    #   for a particular car is entirely in training or entirely in validation. Since the\n",
        "    #   items in our source CSV file are sorted and grouped by car, the set of images for a particular\n",
        "    #   car appear sequentially in that file. So when we split sequentially we are ensuring that the\n",
        "    #   first 'n' car sets are all in training and the next 'm' car sets are all in validation.\n",
        "    # 'x' items are taken from 'img' column as image file names and\n",
        "    # 'y' labels are taken from 'img' column and then transformed into mask file names\n",
        "    # Convert the 'x' items from Image Files to Images\n",
        "    # Convert the 'y' items from Image Files to Images\n",
        "    # At runtime, dynamically read an Image and apply some image processing steps. Finally\n",
        "    # convert to tensors of floats\n",
        "\n",
        "    load_params = {'source': CSVItemContainer, 'target_cls': DfItemList, 'csv_path': csv_path}\n",
        "    split_params = {'split_procedure': 'split_sequential', 'train_ratio': 0.8, 'valid_ratio': 0.2}\n",
        "    extract_x_params = {'extract_procedure': 'extract_colval', 'target_cls': ImageFileItemList, 'col': 'img', 'folder_path': img_folder_path}\n",
        "    extract_y_params = {'extract_procedure': 'extract_custom', 'target_cls': ImageFileItemList, 'folder_path': mask_folder_path, 'custom_fn': mask_name_fn}\n",
        "    convert_x_params = [\n",
        "        {'target_cls': ImageItemList, 'convert_procedure': 'FileToImage', 'pair_type': 'mask'}, \n",
        "    ]\n",
        "    convert_y_params = [\n",
        "        {'target_cls': ImageItemList, 'convert_procedure': 'FileToImage'}, \n",
        "    ]\n",
        "    xform_x_params = [\n",
        "        {'xform_procedure': 'make_rgb'}, \n",
        "        {'xform_procedure': 'resize', 'size': 100},\n",
        "        {'xform_procedure': 'perspective_warp', 'crop_size': 100},\n",
        "        {'xform_procedure': 'aug', 'aug_name': 'Horizontal Flip'},\n",
        "        {'xform_procedure': 'aug', 'aug_name': 'Rotate'},\n",
        "        {'xform_procedure': 'aug', 'aug_name': 'Random Brightness Contrast'},\n",
        "        {'xform_procedure': 'to_byte_tensor'},\n",
        "        {'xform_procedure': 'to_float_tensor'},\n",
        "        {'xform_procedure': 'normalise'},\n",
        "        #{'xform_procedure': 'resize', 'size': 128},\n",
        "        #{'xform_procedure': 'to_byte_tensor'},        \n",
        "        #{'xform_procedure': 'to_float_tensor'}\n",
        "    ]\n",
        "    ds_params = {'target_ds': ILPairedDataset}\n",
        "    dl_params = (\n",
        "        {'bs': 6, 'sampler_fn': RandomSampler},     # for training\n",
        "        {'bs': 6, 'sampler_fn': SequentialSampler}  # for valid/test\n",
        "    )\n",
        "    self.post_proc_params = [\n",
        "        {'proc_procedure': 'set_mean_std', 'mean': [0.485, 0.456, 0.406], 'std':[0.229, 0.224, 0.225]}, \n",
        "    ]\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_images', 'figsize': (20, 5),\n",
        "        'xyz_procedures': ('image', 'mask', 'label')\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=xform_x_params, ds_params=ds_params, dl_params=dl_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPspLf6pciEE"
      },
      "source": [
        "isdb = ImageSegmentationDataBundle(masks_file_path, imgs_path, masks_path)\n",
        "isdb.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kng7BU2oc5Rq"
      },
      "source": [
        "isdb.display_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPMf2ZkpNcJV"
      },
      "source": [
        "assert(isdb.train_ds.x.pair_type == 'mask')\n",
        "assert(isdb.valid_ds.x.pair_type == 'mask')\n",
        "assert(isdb.train_ds.y.pair_type is None)\n",
        "assert(isdb.valid_ds.y.pair_type is None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-MFmsw8m7DM"
      },
      "source": [
        "### Cycle GAN Tuple Image List"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcamRSQZm_pF"
      },
      "source": [
        "#export\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Image Cycle GAN data preparation pipeline\n",
        "# In general, it is very similar to an Image Classification pipeline except that it provides pairs of images (ie. a tuple)\n",
        "# for each data row instead of a single image\n",
        "#----------------------------------------------------\n",
        "class ImageCycleGanDataBundle(DataBundle):\n",
        "  def __init__(self, folder_path, multi_path):\n",
        "    print ('--------- Image Cycle GAN DataBundle init', folder_path)\n",
        "\n",
        "    # Get list of image file extensions\n",
        "    # Under the given folder path, image files in each sub-folder in 'multi_path' are loaded into separate sub-lists.\n",
        "    # No Split is required as we don't need a validation set\n",
        "    # Loaded files can be directly used as 'x' items, no extraction necessary\n",
        "    # 'y' labels are dummy values which will be ignored. No labels are needed during training.\n",
        "    # Convert the 'x' items from Image Files to tuples of Images. A tuple contains one image from each sub-list.\n",
        "    # No conversion needed for the dummy 'y' items\n",
        "    # At runtime, dynamically read an Image and apply some image processing steps. Finally\n",
        "    # convert to tensors of floats. These image processing steps are identical to the ones for single\n",
        "    # images, except that they operate on tuples of images.\n",
        "\n",
        "    image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))\n",
        "    ' '.join(image_extensions)\n",
        "    load_params = {'source': FolderMultiItemContainer, 'target_cls': FileMultiItemList, 'folder_path': folder_path, 'extensions': image_extensions, 'multi_path': multi_path}\n",
        "    split_params = {'split_procedure': None}\n",
        "    extract_x_params = {'extract_procedure': None, 'target_cls': None}\n",
        "    extract_y_params = {'extract_procedure': 'extract_dummy', 'target_cls': DummyItemList}\n",
        "    convert_x_params = [\n",
        "        {'target_cls': ImageTupleItemList, 'convert_procedure': 'FileMultiToImageTuple'}, \n",
        "    ]\n",
        "    convert_y_params = [\n",
        "    ]\n",
        "    xform_x_params = [\n",
        "        {'xform_procedure': 'tuple_make_rgb'}, \n",
        "        {'xform_procedure': 'tuple_resize', 'size': 128},\n",
        "        {'xform_procedure': 'tuple_to_byte_tensor'},        \n",
        "        {'xform_procedure': 'tuple_to_float_tensor'}\n",
        "     ]\n",
        "    # These dl_params are not really required for this DataBundle. They are there as test cases for different param combinations\n",
        "    dl_params = (\n",
        "        {'bs': 1, 'sampler_fn': RandomSampler},     # for training\n",
        "        {'bs': 1, 'sampler_fn': SequentialSampler}  # for valid/test\n",
        "    )\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_images', 'figsize': (20, 5),\n",
        "        'xyz_procedures': ('image', 'label', 'image')\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=xform_x_params, dl_params=dl_params)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# A Folder Multi-Item is similar to a Folder container, except that it stores files in different\n",
        "# sub-folders in separate sub-lists. 'multi_path' defines the list of sub-folders directly under the\n",
        "# root 'folder_path'. One sub-list is created for each 'multi_path' sub-folder with all the files in\n",
        "# that sub-folder.\n",
        "#----------------------------------------------------\n",
        "class FolderMultiItemContainer(ItemContainer):\n",
        "  def __init__(self, folder_path, multi_path, extensions=None):\n",
        "    # NB: For now, we support only two paths\n",
        "    assert (isinstance(multi_path, list) and (len(multi_path) == 2))\n",
        "    self.folder_path = Path(folder_path)\n",
        "    self.extensions = extensions\n",
        "    # 'A' and 'B' sub-folders\n",
        "    self.A_path, self.B_path = multi_path[0], multi_path[1]\n",
        "    super().__init__()\n",
        "\n",
        "  # ----------------------------\n",
        "  # Use the FolderItemContainer to load files in each sub-folder\n",
        "  # We create A and B sub-lists since we support only two sub-folders\n",
        "  # ----------------------------\n",
        "  def load(self, target_cls):\n",
        "    # Create 'A' and 'B' FolderItemContainers, one for each sub-folder\n",
        "    A_cntnr = FolderItemContainer(self.folder_path, include_subfolders=[self.A_path], extensions=self.extensions)\n",
        "    B_cntnr = FolderItemContainer(self.folder_path, include_subfolders=[self.B_path], extensions=self.extensions)\n",
        "\n",
        "    # Load the 'A' and 'B' sub-folders into 'A' and 'B' sub-lists\n",
        "    A_file_list = A_cntnr.load(ImageFileItemList)[:]\n",
        "    B_file_list = B_cntnr.load(ImageFileItemList)[:]\n",
        "\n",
        "    # !!!!!!!!!! Remove this now. 'A' and 'B' sub-lists\n",
        "    #A_file_list = A_cntnr.all_rows[:]\n",
        "    #B_file_list = B_cntnr.all_rows[:]\n",
        "\n",
        "    # Create the Item List with the list of files\n",
        "    all_rows = target_cls([A_file_list, B_file_list])\n",
        "    return all_rows\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Similar to a FileItemList except that it stores multiple separate lists of files\n",
        "# It doesn't do much, except a 'pass-through' conversion to an Image Tuple item list\n",
        "#----------------------------------------------------\n",
        "class FileMultiItemList(ItemList):\n",
        "  def FileMultiToImageTuple(self, items, convert_state, in_train, **kwargs):\n",
        "    filemulti_items = items\n",
        "    # 'filemulti_items' contains two sub-lists. Pass them through as is to the\n",
        "    # Image Tuple item list.\n",
        "    image_tuple_items = filemulti_items\n",
        "    return (image_tuple_items)\n",
        "\n",
        "  def extract_dummy(self, **kwargs):\n",
        "    y_list = [0] * len(self.items[0])\n",
        "    return (y_list)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Similar to an ImageItemList except that each item is a pair of images. It stores\n",
        "# two sub-lists of images, which are of different lengths. When asked for the i-th\n",
        "# item, it gets the i-th image from the first (ie. 'A') sub-list, and then fetches one\n",
        "# image at random from the second (ie. 'B') sub-list.\n",
        "#----------------------------------------------------\n",
        "class ImageTupleItemList(ImageItemList):\n",
        "  def __init__(self, image_tuple_items, **kwargs):\n",
        "    assert(len(image_tuple_items) == 2)\n",
        "    assert(isinstance(image_tuple_items[0], list) and isinstance(image_tuple_items[1], list))\n",
        "    # 'image_tuple_items' contains two sub-lists\n",
        "    items, self.B_items = image_tuple_items\n",
        "    super().__init__(items, **kwargs)\n",
        "\n",
        "  # ----------------------------\n",
        "  # For displaying items, we combine the pair of images into a single stacked image\n",
        "  # ----------------------------\n",
        "  def get_display_data(self, obj, idx):\n",
        "    img_tensor_tuple = obj\n",
        "    assert(isinstance(img_tensor_tuple[0], torch.Tensor))\n",
        "    # dim=1 will stack the two images vertically. To do it horizontally, use dim=2\n",
        "    merged = torch.cat(img_tensor_tuple, dim=1)\n",
        "    return (merged)\n",
        "\n",
        "  # ----------------------------\n",
        "  # Get a pair of images rather than a single image\n",
        "  # ----------------------------\n",
        "  def _get_object(self, item):\n",
        "    # Let the parent Image list load the image for an image file item.\n",
        "    obj = super()._get_object(item)\n",
        "\n",
        "    # Fetch another image file at random from the 'B' list and let the parent load the \n",
        "    # image for that as well.\n",
        "    B_obj = super()._get_object(self.B_items[random.randint(0, len(self.B_items)-1)])\n",
        "\n",
        "    # Return the pair\n",
        "    return ((obj, B_obj))\n",
        "\n",
        "  # ----------------------------\n",
        "  # Support all the same image xform transforms as an Image item list except that\n",
        "  # we operate on a pair of images. Rather than re-implementing them, we leverage the parent object's\n",
        "  # transforms by calling them twice for the pair of images.\n",
        "  #\n",
        "  # The xform methods on the child start with a prefix of 'tuple_' but are otherwise identical \n",
        "  # to the parent's. When any of the child's 'tuple_xform_*' methods is called, with a pair of\n",
        "  # images as input, __getattr_() gets invoked. The parent's method gets bound into a wrapper\n",
        "  # function which we return here.\n",
        "  #\n",
        "  # Python then calls the wrapper function passing it the pair of images as input. The wrapper\n",
        "  # in turn calls the parent's method for each of the images in the pair, and then returns\n",
        "  # the result\n",
        "  # ----------------------------\n",
        "  def __getattr__(self, attr):\n",
        "    # Strip off the 'tuple_' prefix to get the name of the parent method\n",
        "    # We don't invoke the parent's method here, but invoke it from the wrapper\n",
        "    # function instead. Since the wrapper is a nested function the 'super_method' \n",
        "    # gets bound into it.\n",
        "    super_attr = attr.partition('tuple_')[2]\n",
        "    super_method = getattr(super(), super_attr, None)\n",
        "    #\n",
        "    # Dispatching based on passed arguments is a two-step process:\n",
        "    #    __getattr__ returns a proxy wrapper method\n",
        "    #    python calls the proxy wrapper which then decides which real method to call\n",
        "    def _tuple_to_image_wrapper(tuple_obj, *args, **kwargs):\n",
        "      A_obj, B_obj = tuple_obj\n",
        "\n",
        "      # The parent's 'super_method' gets bound inside the wrapper\n",
        "      # Call the parent's method for the 'A' and 'B' images\n",
        "      A_res = super_method(A_obj, *args, **kwargs)\n",
        "      B_res = super_method(B_obj, *args, **kwargs)\n",
        "\n",
        "      # Return the result as a pair\n",
        "      return ((A_res, B_res))\n",
        "\n",
        "    return _tuple_to_image_wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0TaAYGO-FtR"
      },
      "source": [
        "icgdb = ImageCycleGanDataBundle(path_imagenette, multi_path=['train', 'val'])\n",
        "icgdb.process()\n",
        "\n",
        "icgdb.display_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNHYoIG6FQJm"
      },
      "source": [
        "icgdb.display_batch(idxs=[49])\n",
        "\n",
        "icg_x, icg_y = next(iter(icgdb.train_dl))\n",
        "type(icg_x), icg_x[0].shape, icg_x[1].shape, type(icg_y), icg_y.shape, icg_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MFbdjiCvV07"
      },
      "source": [
        "### Audio Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg16HX7zvV1Q"
      },
      "source": [
        "#export\n",
        "\n",
        "def re_labeler(fn, pat): return re.findall(pat, str(fn))[0]\n",
        "label_pat = r'/([mf]\\d+)_'\n",
        "speaker_labeler = partial(re_labeler, pat=label_pat)\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Audio Classification data preparation pipeline\n",
        "#----------------------------------------------------\n",
        "class AudioClassificationDataBundle(DataBundle):\n",
        "  def __init__(self, folder_path):\n",
        "    print ('--------- Audio Classification DataBundle init', folder_path)\n",
        "\n",
        "    # Get list of audio file extensions\n",
        "    # Load all files with audio extensions from the given folder path\n",
        "    # Split randomly based on a percentage ratio for training and validation\n",
        "    # Loaded files can be directly used as 'x' items, no extraction necessary\n",
        "    # Use a substring from the file name of each file as the class name for the 'y' labels. Done through a custom extraction function.\n",
        "    # Convert the 'x' items from Audio Files to Audio\n",
        "    # Convert the 'y' items from Class Names to Class Ids\n",
        "    # At runtime, dynamically read an Audio File and apply some pre-processing steps to create spectrograms\n",
        "\n",
        "    audio_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('audio/'))\n",
        "    ' '.join(audio_extensions)\n",
        "    load_params = {'source': FolderItemContainer, 'target_cls': AudioFileItemList, 'folder_path': folder_path, 'extensions': audio_extensions}\n",
        "    split_params = {'split_procedure': 'split_random', 'train_ratio': 0.8, 'valid_ratio': 0.2}\n",
        "    extract_x_params = {'extract_procedure': None, 'target_cls': None}\n",
        "    extract_y_params = {'extract_procedure': 'extract_custom', 'target_cls': ClassNameItemList, 'custom_fn': speaker_labeler}\n",
        "    convert_x_params = [\n",
        "        {'target_cls': AudioItemList, 'convert_procedure': 'FileToAudio'}, \n",
        "    ]\n",
        "    convert_y_params = [\n",
        "        {'target_cls': ClassIdItemList, 'convert_procedure': 'NameToId'} \n",
        "    ]\n",
        "    xform_x_params = [\n",
        "        {'xform_procedure': 'pad_trim', 'max_ms': 3000},\n",
        "        {'xform_procedure': 'signal_shift', 'max_shift_pct': 0.4},\n",
        "        {'xform_procedure': 'spectro_gram'},\n",
        "        {'xform_procedure': 'spectro_augment', 'max_mask_pct': 0.1, 'n_freq_masks': 2, 'n_time_masks': 2},\n",
        "    ]\n",
        "    # These dl_params are not really required for this DataBundle. They are there as test cases for different param combinations\n",
        "    dl_params = (\n",
        "        {'bs': 6, 'sampler_fn': RandomSampler},     # for training\n",
        "        {'bs': 6, 'sampler_fn': SequentialSampler}  # for valid/test\n",
        "    )\n",
        "    self.display_params = {\n",
        "        'layout_procedure': 'display_audio', 'figsize': (30, 6),\n",
        "    }\n",
        "    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=xform_x_params, dl_params=dl_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSMxUsK0zH3C"
      },
      "source": [
        "**Download data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5Uai_G_zGcJ"
      },
      "source": [
        "audio_path=Path.cwd()/'audio'\n",
        "dsid = \"ST-AEDS-20180100_1-OS\"\n",
        "!wget 'http://www.openslr.org/resources/45/{dsid}.tgz'\n",
        "\n",
        "import tarfile\n",
        "tf = tarfile.open(f'{dsid}.tgz')\n",
        "tf.extractall(audio_path)\n",
        "list(audio_path.iterdir())[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT8A6Bz2eY9o"
      },
      "source": [
        "acdb = AudioClassificationDataBundle(audio_path)\n",
        "acdb.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjFuLIXNoqRj"
      },
      "source": [
        "file_orig = acdb.train_ds.x.items[0]\n",
        "sample_orig = AudioUtil.open(file_orig)\n",
        "AudioUtil.play(sample_orig)\n",
        "\n",
        "sample_aud = acdb.train_ds.x[0]\n",
        "AudioUtil.show_spectro(sample_aud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3dY-lHCwNaE"
      },
      "source": [
        "# Audio Classification class names\n",
        "acdb.valid_ds.y.class_vocab_i2n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-_fDiO2wNaJ"
      },
      "source": [
        "acdb.display_batch()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f67GwyU4Xe0L"
      },
      "source": [
        "### Export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPRG233iuTeu"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/ketanhdoshi/ml/master/lib/nb_export.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYy95Q_juc1i"
      },
      "source": [
        "from nb_export import notebook2scriptSingle\n",
        "notebook2scriptSingle(gn_path + '/lib/data_lib.ipynb', gn_path + '/exp')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iim4HEAK-alg"
      },
      "source": [
        "### Old DfItemList code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9_Rjcob-Vf8"
      },
      "source": [
        "\n",
        "\n",
        "    len_df = len(df.index)\n",
        "    if (items is None):\n",
        "      # Items should be a range of consecutive numbers from 0 to len_df - 1 \n",
        "      items = list(range(len_df))\n",
        "    elif (len(items) == len_df):\n",
        "      # Most likely, items already contains the required numbers, but we re-create\n",
        "      # it just to be sure\n",
        "      items = list(range(len_df))\n",
        "    elif (len(items) < len_df):\n",
        "      # Items contains the indexes of a subset of the Dataframe. This happens when we\n",
        "      # split the ItemList. Create a new Dataframe with those indexes\n",
        "      df = df.iloc[items, :].reset_index(drop=True)\n",
        "      new_len_df = len(df.index)\n",
        "      items = list(range(new_len_df))\n",
        "    else:\n",
        "      # Items is longer than the Dataframe. This should never happen.\n",
        "      assert (False)\n",
        "\n",
        "\n",
        "  # ----------------------------\n",
        "  # Create a new dataframe with just the given subset of rows ie. 'items'\n",
        "  # ----------------------------\n",
        "  def WAIT_new(self, items):\n",
        "    if (items is None):\n",
        "      return (None)\n",
        "\n",
        "    # Make a new dataframe by selecting the given 'items' and create a new\n",
        "    # ItemList from it\n",
        "    new_df = self.df.iloc[items, :].reset_index(drop=True)\n",
        "\n",
        "    # Keyword arguments for initialisation using the attributes to be copied over \n",
        "    # to the cloned ItemList\n",
        "    copy_args = {k:getattr(self, k, None) for k in self._copy_new}\n",
        "    # Add the list of items to the keyword arguments\n",
        "    kwargs = {'df': new_df, **copy_args}\n",
        "    # Now create a new instance of my ItemList type using the keyword arguments\n",
        "    il = self.__class__(**kwargs)\n",
        "\n",
        "    return (il)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TOD2Kt_8AGO"
      },
      "source": [
        "### Old Test the image data bundle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G3ggHCK_bTF"
      },
      "source": [
        "indb = ImageClassificationDataBundle(path_imagenette)\n",
        "indb.process()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNFzxUbcyvat"
      },
      "source": [
        "my_item = indb.train_ds.x._get_item_for_split_wo_xform(img_idx)\n",
        "my_img = indb.train_ds.x._get_object(my_item)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzU9ejEq4Ona"
      },
      "source": [
        "img_idx=55\n",
        "foo=indb.train_ds.x[img_idx]\n",
        "foo_y=indb.train_ds.y[img_idx]\n",
        "print('class label is', indb.convert_state_y['class_vocab_i2n'][foo_y])\n",
        "foo.type(), foo.shape, indb.convert_state_y\n",
        "show_image(foo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux3TJZVWCAYq"
      },
      "source": [
        "### Old Show Item/Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeYqY1X_o5ri"
      },
      "source": [
        "\n",
        "  # ----------------------------\n",
        "  # Dynamically apply any defined transforms on an item and return the resulting object\n",
        "  # ----------------------------\n",
        "  def _WAIT_xform(self, item):\n",
        "    # Get the object for this item \n",
        "    obj = self._get_object(item)\n",
        "\n",
        "    # Apply transforms on that object. The output of each transform becomes the input\n",
        "    # for the next transform\n",
        "    if (self.xforms is not None):\n",
        "      for xform in self.xforms:\n",
        "        obj = self._do_xform(obj, **xform)\n",
        "\n",
        "    return obj\n",
        "\n",
        "  # ----------------------------\n",
        "  # Built-in dunder method to get an object or a list of objects from the ItemList, using\n",
        "  # standard indexing and slicing. So 'idx' can be either a single index, a range or a slice\n",
        "  # We get the data object, apply transforms on it and return the result\n",
        "  # ----------------------------\n",
        "  def WAIT__getitem__(self, idx):\n",
        "    # Get the item (or list of items) corresponding to the given indices\n",
        "    item = super().__getitem__(idx)\n",
        "\n",
        "    if isinstance(item,list):\n",
        "      # Return a list of objects after applying transforms on each one\n",
        "      return [self._xform(o) for o in item]\n",
        "    else:\n",
        "      # Return a single object after applying transforms on it\n",
        "      return self._xform(item)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuEVhir2CGAj"
      },
      "source": [
        "def show_image(im, figsize=(3,3)):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(im.permute(1,2,0))\n",
        "\n",
        "def show_batch(x, c=4, r=None, figsize=None):\n",
        "    n = len(x)\n",
        "    if r is None: r = int(math.ceil(n/c))\n",
        "    if figsize is None: figsize=(c*3,r*3)\n",
        "    fig,axes = plt.subplots(r,c, figsize=figsize)\n",
        "    for xi,ax in zip(x,axes.flat): show_image(xi, ax)\n",
        "\n",
        "def show_item(idx, obj_x, obj_y, figsize=(3,3), **kwargs):\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(obj_x.permute(1,2,0))\n",
        "\n",
        "def show_image(im, ax=None, figsize=(3,3)):\n",
        "    if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)\n",
        "    ax.axis('off')\n",
        "    ax.imshow(im.permute(1,2,0))\n",
        "\n",
        "def show_batch(x, c=10, r=None, figsize=None):\n",
        "    n = len(x)\n",
        "    if r is None: r = int(math.ceil(n/c))\n",
        "    if figsize is None: figsize=(c*3,r*3)\n",
        "    fig,axes = plt.subplots(r,c, figsize=figsize)\n",
        "    for xi,ax in zip(x,axes.flat): \n",
        "      show_image(xi, ax)\n",
        "\n",
        "def show_batch_transpose (my_img):\n",
        "  _, axs = plt.subplots(2, 4, figsize=(12, 6))\n",
        "  for i,ax in enumerate(axs.flatten()):\n",
        "    if i==0: ax.imshow(my_img)\n",
        "    else:    ax.imshow(my_img.transpose(i-1))\n",
        "    ax.axis('off')\n",
        "\n",
        "\n",
        "#foo_imgs, _ = next(iter(indb.train_dl))\n",
        "foo_img_tensors = indb.train_ds.x[:15]\n",
        "foo_img_labels = indb.train_ds.y[:15]\n",
        "if (False):\n",
        "  show_item(None, foo, None)\n",
        "  show_item(None, foo_imgs[0], None)\n",
        "  show_batch(foo_imgs)\n",
        "\n",
        "sd=ShowData()\n",
        "# Show images as Pytorch tensors, after applying transforms\n",
        "sd.show_images(foo_img_tensors, foo_img_labels)\n",
        "\n",
        "foo_img_files = indb.train_ds.x.items[:15]\n",
        "foo_imgs = [indb.train_ds.x._get_object(f) for f in foo_img_files]\n",
        "# Show images as PIL Image objects\n",
        "sd.show_images(foo_imgs, foo_img_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoL1QeLI7m9Z"
      },
      "source": [
        "### Create a DataLoader using a Sampler and Collate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb49mXk3NuR9"
      },
      "source": [
        "tcdb.convert_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E36NUIVOe1Gd"
      },
      "source": [
        "fdl = tcdb.train_dl\n",
        "idl = iter(fdl)\n",
        "fdx, fdy = next(idl)\n",
        "tcdb.train_ds.x, tcdb.train_ds.y, fdx, fdy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS2ZSQjRMxi8"
      },
      "source": [
        "fdl = DataLoader(tcdb.train_ds, batch_size=2)\n",
        "idl = iter(fdl)\n",
        "fdx, fdy = next(idl)\n",
        "tcdb.train_ds.x, tcdb.train_ds.y, fdx, fdy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VgRJYg8QKDs"
      },
      "source": [
        "fdl = DataLoader(tcdb.train_ds, batch_size=2)\n",
        "idl = iter(fdl)\n",
        "fdx, fdy = next(idl)\n",
        "tcdb.train_ds.x, tcdb.train_ds.y, fdx, fdy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNIkJ01dPD2o"
      },
      "source": [
        "ss = SortSampler(tcdb.train_ds.x, key=lambda t: len(tcdb.train_ds.x[t]))\n",
        "sdl = DataLoader(tcdb.train_ds, batch_size=2, sampler=ss)\n",
        "sidl = iter(sdl)\n",
        "sfdx, sfdy = next(sidl)\n",
        "sfdx, sfdy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-Cjn_p2ex1p"
      },
      "source": [
        "sos = SortishSampler(tlmdb.train_ds.x, key=lambda t: len(tlmdb.train_ds.x[t]), bs=10)\n",
        "sodl = DataLoader(tlmdb.train_ds, batch_size=10, sampler=sos, collate_fn=pad_collate)\n",
        "soidl = iter(sodl)\n",
        "sofdx, sofdy = next(soidl)\n",
        "sofdx, sofdy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehiCSQNC7O6m"
      },
      "source": [
        "#type(foo), type(indb.train_ds.x[img_idx])\n",
        "type(indb.train_ds.x.items[img_idx])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}