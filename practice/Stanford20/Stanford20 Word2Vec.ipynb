{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Stanford20 Word2Vec.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"tvczegV-2uZo","colab_type":"text"},"cell_type":"markdown","source":["## Implementation of Word2Vec based on Stanford CS20 [example](https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/04_word2vec_visualize.py) and Lecture [Notes](https://docs.google.com/document/d/1wqp8_-H06oE4zB9CHDwzTx5BfAOMM_6nnNJMSfkazkU/edit)\n","\n","It uses a decent sized corpus and covers several additional concepts in addition to Word2Vec:\n","*   Fetch data using tf.data.Dataset and a generator function\n","*   Define the Model as a Python class\n","*   Name Scoping\n","*   Saving and Restoring checkpoints\n","*  Visualising the learned word embeddings with t-SNE in Tensorboard\n","*   Logging Summary for visualising loss histograms in Tensorboard"]},{"metadata":{"id":"Z_AyBtK0WrgU","colab_type":"code","colab":{}},"cell_type":"code","source":["from collections import Counter\n","import random\n","import os\n","#import sys\n","import zipfile\n","import numpy as np\n","from tensorflow.contrib.tensorboard.plugins import projector\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kB0a28dW3fmc","colab_type":"code","colab":{}},"cell_type":"code","source":["# Model hyperparameters\n","VOCAB_SIZE = 50000\n","BATCH_SIZE = 128\n","EMBED_SIZE = 128            # dimension of the word embedding vectors\n","SKIP_WINDOW = 1             # the context window\n","NUM_SAMPLED = 64            # number of negative examples to sample\n","LEARNING_RATE = 1.0\n","NUM_TRAIN_STEPS = 100000\n","VISUAL_FLD = 'visualization'\n","SKIP_STEP = 5000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7lFy3N5v-Pbg","colab_type":"code","colab":{}},"cell_type":"code","source":["# These are not used since we are not downloading the data programmatically\n","# Parameters for downloading data\n","DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n","EXPECTED_BYTES = 31344016\n","NUM_VISUALIZE = 3000        # number of tokens to visualize"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dOND-8OKFP-D","colab_type":"text"},"cell_type":"markdown","source":["### Download the text corpus"]},{"metadata":{"id":"D0r6F8pn3_N5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"d54cd178-73f6-4159-c392-0de0a4d0a540","executionInfo":{"status":"ok","timestamp":1544679069283,"user_tz":-330,"elapsed":39846,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"cell_type":"code","source":["# Download the text corpus to train the word2vec. The text8 dataset is the first 100 MB of cleaned text of the English Wikipedia dump on Mar. 3, 2006 (whose link is no longer available)\n","!wget 'http://mattmahoney.net/dc/text8.zip'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2018-12-13 05:30:30--  http://mattmahoney.net/dc/text8.zip\n","Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n","Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 31344016 (30M) [application/zip]\n","Saving to: ‘text8.zip’\n","\n","text8.zip           100%[===================>]  29.89M   852KB/s    in 37s     \n","\n","2018-12-13 05:31:07 (836 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"1WGgNgThFTrU","colab_type":"text"},"cell_type":"markdown","source":["### Functions to load and parse the text corpus, and feed in the input word data for training"]},{"metadata":{"id":"SMmBt_I04Vph","colab_type":"code","colab":{}},"cell_type":"code","source":["def read_data(file_path):\n","    \"\"\" Read data into a list of tokens \n","    There should be 17,005,207 tokens\n","    \"\"\"\n","    with zipfile.ZipFile(file_path) as f:\n","        # Read text from the first file in the zipfile and split it into a list of words\n","        words = tf.compat.as_str(f.read(f.namelist()[0])).split() \n","    return words"],"execution_count":0,"outputs":[]},{"metadata":{"id":"21-cnsNM492p","colab_type":"code","colab":{}},"cell_type":"code","source":["def safe_mkdir(path):\n","    \"\"\" Create a directory if there isn't one already. \"\"\"\n","    try:\n","        os.mkdir(path)\n","    except OSError:\n","        pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"os6-4bS34ygO","colab_type":"code","colab":{}},"cell_type":"code","source":["def build_vocab(words, vocab_size, visual_fld):\n","    \"\"\" Build vocabulary of VOCAB_SIZE most frequent words and write it to\n","    visualization/vocab.tsv\n","    \"\"\"\n","    safe_mkdir(visual_fld)\n","    file = open(os.path.join(visual_fld, 'vocab.tsv'), 'w')\n","    \n","    # The first entry in the dictionary is 'Unknown'. \n","    dictionary = dict()\n","    count = [('UNK', -1)]\n","    index = 0\n","    # Add the VOCAB_SIZE most commonly occurring words to the 'count' list\n","    count.extend(Counter(words).most_common(vocab_size - 1))\n","    \n","    # Build the dictionary as (word, index) using the list in 'count'\n","    # Write the dictionary out to the file\n","    for word, _ in count:\n","        dictionary[word] = index\n","        index += 1\n","        file.write(word + '\\n')\n","    \n","    # Now build another inverted dictionary as (index, word)\n","    index_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n","    file.close()\n","    return dictionary, index_dictionary"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hz-dUfns5TSI","colab_type":"code","colab":{}},"cell_type":"code","source":["def convert_words_to_index(words, dictionary):\n","    \"\"\" Replace each word in the dataset with its index in the dictionary \"\"\"\n","    return [dictionary[word] if word in dictionary else 0 for word in words]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AsHKTu59BX57","colab_type":"code","colab":{}},"cell_type":"code","source":["def most_common_words(visual_fld, num_visualize):\n","    \"\"\" create a list of num_visualize most frequent words to visualize on TensorBoard.\n","    saved to visualization/vocab_[num_visualize].tsv\n","    \"\"\"\n","    words = open(os.path.join(visual_fld, 'vocab.tsv'), 'r').readlines()[:num_visualize]\n","    words = [word for word in words]\n","    file = open(os.path.join(visual_fld, 'vocab_' + str(num_visualize) + '.tsv'), 'w')\n","    for word in words:\n","        file.write(word)\n","    file.close()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tLN6pY9B5Wf-","colab_type":"code","colab":{}},"cell_type":"code","source":["def generate_sample(index_words, context_window_size):\n","    \"\"\" Form training pairs according to the skip-gram model. \"\"\"\n","    # Loop through the given list 'index_words', making each word the center word\n","    # one by one. For that center word, find some random target words before it and\n","    # some more random target words after it. All these target words occur within \n","    # 'context' distance from the center word\n","    # Since we do a yield, we are a generator function and will pass back one pair \n","    # of (center, target) at a time\n","    for index, center in enumerate(index_words):\n","        context = random.randint(1, context_window_size)\n","        # get a random target before the center word\n","        for target in index_words[max(0, index - context): index]:\n","            yield center, target\n","        # get a random target after the center wrod\n","        for target in index_words[index + 1: index + context + 1]:\n","            yield center, target"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VDq3YtG95cfy","colab_type":"code","colab":{}},"cell_type":"code","source":["def batch_gen(download_url, expected_byte, vocab_size, batch_size, \n","                skip_window, visual_fld):\n","    local_dest = 'text8.zip'\n","    #utils.download_one_file(download_url, local_dest, expected_byte)\n","    words = read_data(local_dest)\n","    dictionary, _ = build_vocab(words, vocab_size, visual_fld)\n","    index_words = convert_words_to_index(words, dictionary)\n","    del words           # to save memory\n","    single_gen = generate_sample(index_words, skip_window)\n","    \n","    while True:\n","        center_batch = np.zeros(batch_size, dtype=np.int32)\n","        target_batch = np.zeros([batch_size, 1])\n","        for index in range(batch_size):\n","            center_batch[index], target_batch[index] = next(single_gen)\n","        yield center_batch, target_batch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"d6EiribxFhQK","colab_type":"text"},"cell_type":"markdown","source":["### Python Class for the Word2Vec model - build the graph, fetch data, train and visualise "]},{"metadata":{"id":"1Jsw3FBK5-tG","colab_type":"code","colab":{}},"cell_type":"code","source":["class SkipGramModel:\n","    \"\"\" Build the graph for word2vec model \"\"\"\n","    def __init__(self, dataset, vocab_size, embed_size, batch_size, num_sampled, learning_rate):\n","        self.vocab_size = vocab_size\n","        self.embed_size = embed_size\n","        self.batch_size = batch_size\n","        self.num_sampled = num_sampled\n","        self.lr = learning_rate\n","        self.global_step = tf.get_variable('global_step', initializer=tf.constant(0), trainable=False)\n","        self.skip_step = SKIP_STEP\n","        self.dataset = dataset\n","\n","    def _import_data(self):\n","        \"\"\" Step 1: import data\n","        \"\"\"\n","        with tf.name_scope('data'):\n","            self.iterator = self.dataset.make_initializable_iterator()\n","            self.center_words, self.target_words = self.iterator.get_next()\n","\n","    def _create_embedding(self):\n","        \"\"\" Step 2 + 3: define weights and embedding lookup.\n","        In word2vec, it's actually the weights that we care about \n","        \"\"\"\n","        with tf.name_scope('embed'):\n","            self.embed_matrix = tf.get_variable('embed_matrix', \n","                                                shape=[self.vocab_size, self.embed_size],\n","                                                initializer=tf.random_uniform_initializer())\n","            self.embed = tf.nn.embedding_lookup(self.embed_matrix, self.center_words, name='embedding')\n","\n","    def _create_loss(self):\n","        \"\"\" Step 4: define the loss function \"\"\"\n","        with tf.name_scope('loss'):\n","            # construct variables for NCE loss\n","            nce_weight = tf.get_variable('nce_weight', \n","                        shape=[self.vocab_size, self.embed_size],\n","                        initializer=tf.truncated_normal_initializer(stddev=1.0 / (self.embed_size ** 0.5)))\n","            nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n","\n","            # define loss function to be NCE loss function\n","            self.loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, \n","                                                biases=nce_bias, \n","                                                labels=self.target_words, \n","                                                inputs=self.embed, \n","                                                num_sampled=self.num_sampled, \n","                                                num_classes=self.vocab_size), name='loss')\n","    def _create_optimizer(self):\n","        \"\"\" Step 5: define optimizer \"\"\"\n","        self.optimizer = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss, \n","                                                              global_step=self.global_step)\n","\n","    def _create_summaries(self):\n","        with tf.name_scope('summaries'):\n","            tf.summary.scalar('loss', self.loss)\n","            tf.summary.histogram('histogram loss', self.loss)\n","            # because you have several summaries, we should merge them all\n","            # into one op to make it easier to manage\n","            self.summary_op = tf.summary.merge_all()\n","\n","    def build_graph(self):\n","        \"\"\" Build the graph for our model \"\"\"\n","        self._import_data()\n","        self._create_embedding()\n","        self._create_loss()\n","        self._create_optimizer()\n","        self._create_summaries()\n","\n","    def train(self, num_train_steps):\n","        # Used to save (and restore) the runtime state of the model ie. values of the trained weights etc\n","        saver = tf.train.Saver() # defaults to saving all variables - in this case embed_matrix, nce_weight, nce_bias\n","\n","        initial_step = 0\n","        safe_mkdir('checkpoints')\n","        with tf.Session() as sess:\n","            sess.run(self.iterator.initializer)\n","            sess.run(tf.global_variables_initializer())\n","            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n","\n","            # if that checkpoint exists, restore from checkpoint\n","            if ckpt and ckpt.model_checkpoint_path:\n","                saver.restore(sess, ckpt.model_checkpoint_path)\n","\n","            total_loss = 0.0 # we use this to calculate late average loss in the last SKIP_STEP steps\n","            writer = tf.summary.FileWriter('graphs/word2vec/lr' + str(self.lr), sess.graph)\n","            initial_step = self.global_step.eval()\n","\n","            for index in range(initial_step, initial_step + num_train_steps):\n","                try:\n","                    loss_batch, _, summary = sess.run([self.loss, self.optimizer, self.summary_op])\n","                    writer.add_summary(summary, global_step=index)\n","                    total_loss += loss_batch\n","                    if (index + 1) % self.skip_step == 0:\n","                        print('Average loss at step {}: {:5.1f}'.format(index, total_loss / self.skip_step))\n","                        total_loss = 0.0\n","                        saver.save(sess, 'checkpoints/skip-gram', index)\n","                except tf.errors.OutOfRangeError:\n","                    sess.run(self.iterator.initializer)\n","            writer.close()\n","\n","    def visualize(self, visual_fld, num_visualize):\n","        \"\"\" run \"'tensorboard --logdir='visualization'\" to see the embeddings \"\"\"\n","        \n","        # create the list of num_variable most common words to visualize\n","        most_common_words(visual_fld, num_visualize)\n","\n","        saver = tf.train.Saver()\n","        with tf.Session() as sess:\n","            sess.run(tf.global_variables_initializer())\n","            ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n","\n","            # if that checkpoint exists, restore from checkpoint\n","            if ckpt and ckpt.model_checkpoint_path:\n","                saver.restore(sess, ckpt.model_checkpoint_path)\n","\n","            final_embed_matrix = sess.run(self.embed_matrix)\n","            \n","            # you have to store embeddings in a new variable\n","            embedding_var = tf.Variable(final_embed_matrix[:num_visualize], name='embedding')\n","            sess.run(embedding_var.initializer)\n","\n","            config = projector.ProjectorConfig()\n","            summary_writer = tf.summary.FileWriter(visual_fld)\n","\n","            # add embedding to the config file\n","            embedding = config.embeddings.add()\n","            embedding.tensor_name = embedding_var.name\n","            \n","            # link this tensor to its metadata file, in this case the first NUM_VISUALIZE words of vocab\n","            embedding.metadata_path = 'vocab_' + str(num_visualize) + '.tsv'\n","\n","            # saves a configuration file that TensorBoard will read during startup.\n","            projector.visualize_embeddings(summary_writer, config)\n","            saver_embed = tf.train.Saver([embedding_var])\n","            saver_embed.save(sess, os.path.join(visual_fld, 'model.ckpt'), 1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5WqT1CQpF3KX","colab_type":"text"},"cell_type":"markdown","source":["### Invoke the model for training"]},{"metadata":{"id":"0sskXNTs6L1T","colab_type":"code","colab":{}},"cell_type":"code","source":["# Generator function, which internally, downloads the data file, parses it, creates\n","# the vocabulary of word indexes and then iterates through it returning pairs of\n","# training words ie. (center, target) one batch at a time.\n","def gen():\n","    yield from batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n","                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)\n","\n","def main():\n","    dataset = tf.data.Dataset.from_generator(gen, \n","                                (tf.int32, tf.int32), \n","                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n","    model = SkipGramModel(dataset, VOCAB_SIZE, EMBED_SIZE, BATCH_SIZE, NUM_SAMPLED, LEARNING_RATE)\n","    model.build_graph()\n","    model.train(NUM_TRAIN_STEPS)\n","    model.visualize(VISUAL_FLD, NUM_VISUALIZE)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e-wf57O5_yEk","colab_type":"code","colab":{}},"cell_type":"code","source":["# Run the program\n","tf.reset_default_graph()\n","main()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9pztIUbVJGP2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"1177ce85-6f17-4ef4-b680-c56582ff391c","executionInfo":{"status":"ok","timestamp":1544680715002,"user_tz":-330,"elapsed":4031,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"cell_type":"code","source":["!ls -lR graphs"],"execution_count":25,"outputs":[{"output_type":"stream","text":["graphs:\n","total 4\n","drwxr-xr-x 3 root root 4096 Dec 13 05:32 word2vec\n","\n","graphs/word2vec:\n","total 4\n","drwxr-xr-x 2 root root 4096 Dec 13 05:33 lr1.0\n","\n","graphs/word2vec/lr1.0:\n","total 18032\n","-rw-r--r-- 1 root root   136499 Dec 13 05:32 events.out.tfevents.1544679134.63ddedfd3a16\n","-rw-r--r-- 1 root root 18322925 Dec 13 05:37 events.out.tfevents.1544679214.63ddedfd3a16\n"],"name":"stdout"}]},{"metadata":{"id":"cUsv03GhJ4kE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"301fd0cc-7ebc-401b-f3b6-3bcf602adf52","executionInfo":{"status":"ok","timestamp":1544680921245,"user_tz":-330,"elapsed":5800,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"cell_type":"code","source":["!ps -ef | grep tensor\n","!kill 677"],"execution_count":33,"outputs":[{"output_type":"stream","text":["root        1064      70  0 06:01 ?        00:00:00 /bin/bash -c ps -ef | grep tensor\n","root        1066    1064  0 06:01 ?        00:00:00 grep tensor\n","/bin/bash: line 0: kill: (677) - No such process\n"],"name":"stdout"}]},{"metadata":{"id":"zYyqW7vcERoL","colab_type":"text"},"cell_type":"markdown","source":["### Visualise with Tensorboard"]},{"metadata":{"id":"NoIFfI6CEUbD","colab_type":"code","colab":{}},"cell_type":"code","source":["# Set the LOGDIR correctly to use Tensorboard\n","#LOG_DIR = VISUAL_FLD\n","LOG_DIR = 'graphs/word2vec/lr1.0'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5UpUmLbTEarm","colab_type":"code","colab":{}},"cell_type":"code","source":["! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Md4Fa_OsEn3u","colab_type":"code","colab":{}},"cell_type":"code","source":["get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YbfgaOzwEsQH","colab_type":"code","colab":{}},"cell_type":"code","source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"c7vkhuYOEvYJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"789ce8b5-9280-4d7b-c299-f94d26bb531b","executionInfo":{"status":"ok","timestamp":1544680802487,"user_tz":-330,"elapsed":4003,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"cell_type":"code","source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":29,"outputs":[{"output_type":"stream","text":["http://f139247a.ngrok.io\n"],"name":"stdout"}]}]}