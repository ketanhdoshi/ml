{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Udacity Assignment 4.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["_5J7MsFhqgV1","_pD3wme0ISHw","i5x83GPyrBsG","e8IUczZYrDck","15-hfZKPrrsk","0U5zF2sor-WZ","xweRVEkswZkp","8VYiKSBbwCuk","s8Jws20qwquM","ugiuuFyG08pC","3nkKW0Qo-OH4","BlA48NOcL8GK","Ib7rRZc6L6AE"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"_5J7MsFhqgV1","colab_type":"text"},"cell_type":"markdown","source":["### Use Tensorflow to create a Convolutional Neural Network"]},{"metadata":{"id":"NLq6ZyeOqZkj","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import print_function\n","import os\n","import numpy as np\n","import pickle\n","import tensorflow as tf\n","\n","from six.moves import range"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_pD3wme0ISHw","colab_type":"text"},"cell_type":"markdown","source":["### Connect with Google Drive to mount the data"]},{"metadata":{"id":"mBULj593IQgy","colab_type":"code","outputId":"43d78fc4-d5af-415a-f578-28eca8043326","executionInfo":{"status":"ok","timestamp":1545632094474,"user_tz":-330,"elapsed":34984,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"metadata":{"id":"i5x83GPyrBsG","colab_type":"text"},"cell_type":"markdown","source":["### Load the data"]},{"metadata":{"id":"Oc9SOncQqxSu","colab_type":"code","outputId":"0caf4b6d-2683-4b71-db8e-1e3ed05b8a2d","executionInfo":{"status":"ok","timestamp":1545642042968,"user_tz":-330,"elapsed":976,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["# First reload the data we generated in Assignment 1\n","#pickle_file = '/content/gdrive/My Drive/test/notMNIST.pickle'\n","pickle_file = 'notMNISTtiny.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["Training set (100, 28, 28) (100,)\n","Validation set (20, 28, 28) (20,)\n","Test set (40, 28, 28) (40,)\n"],"name":"stdout"}]},{"metadata":{"id":"e8IUczZYrDck","colab_type":"text"},"cell_type":"markdown","source":["### Reformat the data for training"]},{"metadata":{"id":"8LwXSupqrFYx","colab_type":"code","outputId":"ec457750-9f5d-4f96-a55e-6a95420849a4","executionInfo":{"status":"ok","timestamp":1545642049475,"user_tz":-330,"elapsed":971,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"cell_type":"code","source":["image_size = 28\n","num_labels = 10\n","num_channels = 1 # grayscale\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape(\n","    (-1, image_size, image_size, num_channels)).astype(np.float32)\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Training set (100, 28, 28, 1) (100, 10)\n","Validation set (20, 28, 28, 1) (20, 10)\n","Test set (40, 28, 28, 1) (40, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"15-hfZKPrrsk","colab_type":"text"},"cell_type":"markdown","source":["### Define some functions to calculate accuracy, create a directory to reuse for all the tests below"]},{"metadata":{"id":"LX6BGpptrul7","colab_type":"code","colab":{}},"cell_type":"code","source":["def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bhQ5oNCpXQrb","colab_type":"code","colab":{}},"cell_type":"code","source":["def safe_mkdir(path):\n","    \"\"\" Create a directory if there isn't one already. \"\"\"\n","    try:\n","        os.mkdir(path)\n","    except OSError:\n","        pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0U5zF2sor-WZ","colab_type":"text"},"cell_type":"markdown","source":["### Create a basic Convolutional Network\n","Two convolutional layers with Relu and one Fully Connected layer"]},{"metadata":{"id":"YZinfw3HsC2z","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 128\n","patch_size = 5\n","depth = 16\n","num_hidden = 64\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  tf_train_dataset = tf.placeholder(\n","    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  layer1_weights = tf.Variable(tf.truncated_normal(\n","      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n","  layer1_biases = tf.Variable(tf.zeros([depth]))\n","  layer2_weights = tf.Variable(tf.truncated_normal(\n","      [patch_size, patch_size, depth, depth], stddev=0.1))\n","  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n","  layer3_weights = tf.Variable(tf.truncated_normal(\n","      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n","  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n","  layer4_weights = tf.Variable(tf.truncated_normal(\n","      [num_hidden, num_labels], stddev=0.1))\n","  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n","  \n","  # Model.\n","  def model(data):\n","    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n","    hidden = tf.nn.relu(conv + layer1_biases)\n","    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n","    hidden = tf.nn.relu(conv + layer2_biases)\n","    shape = hidden.get_shape().as_list()\n","    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n","    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n","    return tf.matmul(hidden, layer4_weights) + layer4_biases\n","  \n","  # Training computation.\n","  logits = model(tf_train_dataset)\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n","  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"xweRVEkswZkp","colab_type":"text"},"cell_type":"markdown","source":["### Create a Convolutional Network with Max Pooling\n","\n","Two convolutional layers with Relu and one Fully Connected layer"]},{"metadata":{"id":"UxEbMRbTwkEg","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 128\n","patch_size = 5\n","depth = 16\n","num_hidden = 64\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  tf_train_dataset = tf.placeholder(\n","    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  layer1_weights = tf.Variable(tf.truncated_normal(\n","      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n","  layer1_biases = tf.Variable(tf.zeros([depth]))\n","  layer2_weights = tf.Variable(tf.truncated_normal(\n","      [patch_size, patch_size, depth, depth], stddev=0.1))\n","  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n","  layer3_weights = tf.Variable(tf.truncated_normal(\n","      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n","  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n","  layer4_weights = tf.Variable(tf.truncated_normal(\n","      [num_hidden, num_labels], stddev=0.1))\n","  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n","  \n","  # Model.\n","  def model(data):\n","    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n","    hidden = tf.nn.relu(conv + layer1_biases)\n","    pool1 = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], \n","                            strides=[1, 2, 2, 1], padding='VALID')\n","    conv = tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME')\n","    hidden = tf.nn.relu(conv + layer2_biases)\n","    pool2 = tf.nn.max_pool(hidden, ksize=[1, 2, 2, 1], \n","                            strides=[1, 2, 2, 1], padding='VALID')\n","    shape = pool2.get_shape().as_list()\n","    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n","    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n","    return tf.matmul(hidden, layer4_weights) + layer4_biases\n","  \n","  # Training computation.\n","  logits = model(tf_train_dataset)\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n","  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8VYiKSBbwCuk","colab_type":"text"},"cell_type":"markdown","source":["### Run the model to train it - reused for the models above"]},{"metadata":{"id":"k06-fIZfvgHJ","colab_type":"code","outputId":"5af2e353-1750-41e5-a4ae-285b52fd290d","executionInfo":{"status":"ok","timestamp":1545285125647,"user_tz":-330,"elapsed":380095,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":408}},"cell_type":"code","source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 500 == 0):\n","      print('Minibatch loss at step %d: %f' % (step, l))\n","      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 3.791311\n","Minibatch accuracy: 10.2%\n","Validation accuracy: 10.0%\n","Minibatch loss at step 500: 0.467512\n","Minibatch accuracy: 84.4%\n","Validation accuracy: 84.5%\n","Minibatch loss at step 1000: 0.506753\n","Minibatch accuracy: 84.4%\n","Validation accuracy: 86.7%\n","Minibatch loss at step 1500: 0.495117\n","Minibatch accuracy: 82.8%\n","Validation accuracy: 87.6%\n","Minibatch loss at step 2000: 0.493518\n","Minibatch accuracy: 83.6%\n","Validation accuracy: 88.3%\n","Minibatch loss at step 2500: 0.355758\n","Minibatch accuracy: 88.3%\n","Validation accuracy: 88.7%\n","Minibatch loss at step 3000: 0.200726\n","Minibatch accuracy: 95.3%\n","Validation accuracy: 89.0%\n","Test accuracy: 94.3%\n"],"name":"stdout"}]},{"metadata":{"id":"s8Jws20qwquM","colab_type":"text"},"cell_type":"markdown","source":["### OBSOLETE"]},{"metadata":{"id":"EaHAOpVzPVJU","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Create a convolution + relu layer on the data\n","'''\n","def _conv_relu(data, filter_num, filter_size, stride, padding, scope_name):\n","      \n","      in_depth = data.shape[-1] # Last dimension of the data is the depth\n","      out_depth = filter_num\n","\n","      with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n","        weight = tf.get_variable('weight', \n","                     [filter_size, filter_size, in_depth, out_depth],\n","                     initializer=tf.truncated_normal_initializer(stddev=0.1))\n","        bias = tf.get_variable('bias', initializer=tf.zeros(out_depth))\n","        conv = tf.nn.conv2d(data, weight, strides=[1, stride, stride, 1], padding=padding)\n","      \n","      return tf.nn.relu(conv + bias, name=scope.name)        \n","    \n","'''\n","Create a maxpool layer\n","'''\n","def _maxpool(data, filter_size, stride, padding='VALID', scope_name='pool'):\n","      with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n","        pool = tf.nn.max_pool(data, \n","                            ksize=[1, filter_size, filter_size, 1], \n","                            strides=[1, stride, stride, 1],\n","                            padding=padding)\n","      return pool\n","\n","'''\n","Create a fully connected layer\n","'''\n","def _fully_connected(inputs, out_dim, scope_name='fc'):\n","      with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n","        in_dim = inputs.shape[-1]\n","        w = tf.get_variable('w', [in_dim, out_dim],\n","                            initializer=tf.truncated_normal_initializer(stddev=0.1))\n","        b = tf.get_variable('b', [out_dim],\n","                            initializer=tf.constant_initializer(0.0))\n","        out = tf.matmul(inputs, w) + b\n","        \n","      return out\n","\n","'''\n","Define an accuracy measure\n","'''\n","def _accuracy(predictions, labels):\n","      correct_preds = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n","      return 100 * tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n","    \n","class ConvNet(object):\n","    def __init__(self, filter_size, filter_num, num_hidden, image_size, num_labels, num_channels):\n","      self.filter_size = filter_size\n","      self.filter_num = filter_num\n","      self.num_hidden = num_hidden\n","      self.image_size = image_size\n","      self.num_labels = num_labels\n","      self.num_channels = num_channels\n","      self.gstep = tf.Variable(0, dtype=tf.int32, \n","                                trainable=False, name='global_step')\n","\n","    '''\n","    Prepare the input data\n","    '''\n","    def _get_data(self):\n","      # Input data.\n","      with tf.name_scope('data'):\n","        self.X = tf.placeholder(tf.float32, shape=(None, self.image_size, \n","                                         self.image_size, self.num_channels))\n","        self.y = tf.placeholder(tf.float32, shape=(None, self.num_labels))\n","    \n","    '''\n","    Create all the layers of the network\n","    '''\n","    def _inference(self):\n","      conv1 = _conv_relu(self.X, self.filter_num, self.filter_size, 1, 'SAME', 'conv1')\n","      pool1 = _maxpool(conv1, 2, 2, 'VALID', 'pool1')\n","      \n","      conv2 = _conv_relu(pool1, self.filter_num, self.filter_size, 1, 'SAME', 'conv2')\n","      pool2 = _maxpool(conv2, 2, 2, 'VALID', 'pool2')\n","      \n","      shape = pool2.get_shape().as_list()\n","      reshape = tf.reshape(pool2, [-1, shape[1] * shape[2] * shape[3]])\n","      \n","      fc1 = _fully_connected(reshape, self.num_hidden, 'fc1')\n","      self.logits = _fully_connected(fc1, self.num_labels, 'logits')\n","\n","    '''\n","    Define loss function - use softmax cross entropy with logits\n","    '''\n","    def _loss(self):\n","      with tf.name_scope('loss'):\n","        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits), name='loss')\n","      \n","    '''\n","    Define Optimiser - Use Gradient Descent to minimize cost\n","    '''\n","    def _optimize(self):\n","      self.optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(self.loss)\n","      \n","    '''\n","    Create summaries to write on TensorBoard\n","    '''      \n","    def _summary(self):\n","      with tf.name_scope('summaries'):\n","            tf.summary.scalar('loss', self.loss)\n","            tf.summary.scalar('accuracy', self.accuracy)\n","            tf.summary.histogram('histogram_loss', self.loss)\n","            self.summary_op = tf.summary.merge_all()\n","    \n","    '''\n","    Count the number of right predictions in a batch\n","    '''\n","    def _eval(self):\n","      with tf.name_scope('predict'):\n","        prediction = tf.nn.softmax(self.logits)\n","        self.accuracy = _accuracy (prediction, self.y)\n","\n","    def _train_one_epoch(self, session, saver, init, writer, epoch, gstep):\n","        batch_size = 4\n","        num_batches = 25\n","        \n","        for batch_no in range(num_batches):\n","          # Pick an offset within the training data, which has been randomized.\n","          # Note: we could use better randomization across epochs.\n","          offset = (batch_no * batch_size) % (train_labels.shape[0] - batch_size)\n","\n","          # Generate a minibatch\n","          batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n","          batch_labels = train_labels[offset:(offset + batch_size), :]\n","\n","          # Run the optimiser and predictions on the training data\n","          # Prepare a dictionary telling the session where to feed the minibatch.\n","          # The key of the dictionary is the placeholder node of the graph to be fed,\n","          # and the value is the numpy array to feed to it.\n","          feed_dict = {self.X : batch_data, self.y : batch_labels}\n","          _, l, train_accuracy, summaries = session.run(\n","            [self.optimizer, self.loss, self.accuracy, self.summary_op], feed_dict=feed_dict)\n","\n","          if (batch_no % 5 == 0):\n","            writer.add_summary(summaries, global_step=gstep)\n","            \n","            print('Minibatch loss at batch no %d: %f' % (batch_no, l))\n","            print('Minibatch accuracy: %.1f%%' % train_accuracy)\n","\n","            valid_feed_dict = {self.X : valid_dataset, self.y : valid_labels}\n","            valid_accuracy = session.run(self.accuracy, feed_dict=valid_feed_dict)\n","            print('Validation accuracy: %.1f%%' % valid_accuracy)\n","        \n","          gstep += 1\n","        \n","        # Save model state as of this global step number\n","        saver.save(session, 'checkpoints/convnet_mnist', gstep)\n","        \n","        return gstep\n","\n","\n","    def _eval_once(self, session, init, writer, epoch, gstep):\n","        test_feed_dict = {self.X : test_dataset, self.y : test_labels}\n","        test_accuracy, test_summaries = session.run([self.accuracy, self.summary_op], feed_dict=test_feed_dict)\n","        writer.add_summary(test_summaries, global_step=gstep)\n","        print('Test accuracy: %.1f%%' % test_accuracy)\n","\n","    \n","    def build(self):\n","        self._get_data()\n","        self._inference()\n","        self._loss()\n","        self._optimize()\n","        self._eval()\n","        self._summary()\n","\n","    def train(self, n_epochs):\n","      \n","      # Directory to store checkpoints\n","      CKPT_DIR = 'checkpoints'\n","      safe_mkdir(CKPT_DIR)\n","        \n","      # Save summary data for Tensorboard visualisation\n","      LOG_DIR = 'graphs'\n","      writer = tf.summary.FileWriter(LOG_DIR, graph)\n","      \n","      with tf.Session(graph=graph) as session:\n","        tf.global_variables_initializer().run()\n","        print('Initialized')\n","\n","        # Restore model state from prior checkpoint if it exists\n","        saver = tf.train.Saver()\n","        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n","        if ckpt and ckpt.model_checkpoint_path:\n","           print('Found checkpoint, Restoring ', ckpt.model_checkpoint_path)\n","           saver.restore(session, ckpt.model_checkpoint_path)\n","        \n","        # Global counter of number of training loops done so far\n","        gstep = self.gstep.eval()\n","        \n","        init = ''\n","        for epoch in range(n_epochs):\n","            gstep = self._train_one_epoch(session, saver, init, writer, epoch, gstep)\n","            self._eval_once(session, init, writer, epoch, gstep)\n","        \n","      writer.close()\n","\n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"do-UhSALQJKr","colab_type":"code","colab":{}},"cell_type":"code","source":["FILTER_SIZE = 5\n","FILTER_NUM = 16\n","NUM_HIDDEN = 64\n","IMAGE_SIZE = 28\n","NUM_LABELS = 10\n","NUM_CHANNELS = 1 # grayscale\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","   model = ConvNet(FILTER_SIZE, FILTER_NUM, NUM_HIDDEN, IMAGE_SIZE, NUM_LABELS, NUM_CHANNELS)\n","   model.build()\n","\n","model.train(n_epochs=3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4TcKRQSBeem_","colab_type":"code","outputId":"a33751b4-79f9-497e-e5d7-f91d0a26179f","executionInfo":{"status":"ok","timestamp":1545308518166,"user_tz":-330,"elapsed":3286,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"cell_type":"code","source":["!ls graphs"],"execution_count":0,"outputs":[{"output_type":"stream","text":["events.out.tfevents.1545307425.eea806322868\n","events.out.tfevents.1545307958.eea806322868\n","events.out.tfevents.1545308395.eea806322868\n","events.out.tfevents.1545308445.eea806322868\n"],"name":"stdout"}]},{"metadata":{"id":"ugiuuFyG08pC","colab_type":"text"},"cell_type":"markdown","source":["### Create an Improved Convolutional Layer with Max Pool - V1\n","\n","*   Re-use the complete graph for training, testing and validation data\n","*   Define a class for the Model\n","*   Define reusable methods to create each layer of the network\n","*   Variable Scoping\n","*   Use epochs during training\n","*   Save and restore intermediate state\n","*   Visualisation\n","*   Use tf.datasets\n","*   L2 Regularisation and Dropout\n","*   Add Learning Decay?"]},{"metadata":{"id":"UfyOSZtQ1Jb9","colab_type":"code","colab":{}},"cell_type":"code","source":["'''\n","Create a convolution + relu layer on the data\n","'''\n","def _conv_relu(data, filter_num, filter_size, stride, padding, scope_name):\n","      \n","      in_depth = data.shape[-1] # Last dimension of the data is the depth\n","      out_depth = filter_num\n","\n","      with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n","        weight = tf.get_variable('weight', \n","                     [filter_size, filter_size, in_depth, out_depth],\n","                     initializer=tf.truncated_normal_initializer(stddev=0.1))\n","        bias = tf.get_variable('bias', initializer=tf.zeros(out_depth))\n","        conv = tf.nn.conv2d(data, weight, strides=[1, stride, stride, 1], padding=padding)\n","      \n","      return tf.nn.relu(conv + bias, name=scope.name)        \n","    \n","'''\n","Create a maxpool layer\n","'''\n","def _maxpool(data, filter_size, stride, padding='VALID', scope_name='pool'):\n","      with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n","        pool = tf.nn.max_pool(data, \n","                            ksize=[1, filter_size, filter_size, 1], \n","                            strides=[1, stride, stride, 1],\n","                            padding=padding)\n","      return pool\n","\n","'''\n","Create a fully connected layer\n","'''\n","def _fully_connected(inputs, out_dim, activation, scope_name='fc'):\n","      with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n","        in_dim = inputs.shape[-1] # shape of the last dimension\n","        w = tf.get_variable('w', [in_dim, out_dim],\n","                            initializer=tf.truncated_normal_initializer(stddev=0.1))\n","        b = tf.get_variable('b', [out_dim],\n","                            initializer=tf.constant_initializer(0.0))\n","        out = tf.matmul(inputs, w) + b\n","        \n","        # Add the weight to a collection so regularisation loss can be applied\n","        # later\n","        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, w)\n","        \n","        # Call the activation function if passed in\n","        if activation:\n","           out = activation (out)\n","      \n","      return out\n","\n","'''\n","Define an accuracy measure\n","'''\n","def _accuracy(predictions, labels):\n","      # Predictions is a softmax with one column for the probabilities of each \n","      # label. Doing an argmax along axis=1 (ie. column) gives the index of the\n","      # column with the highest predicted probability. The rows are the samples.\n","      #\n","      # Similarly, labels is a one-hot encoding with one column for each label.\n","      # Doing an argmax along axis=1 (ie. column) gives the index of the column\n","      # with the correct expected value ie. the column with the non-zero value. The\n","      # rows are the samples.\n","      #\n","      # The tf.equal then compares the column index of the predicted probability \n","      # with the column index of the expected value for all samples in the batch, \n","      # resulting in a vector where all the correctly predicted samples are \n","      # True (or 1) and the incorrect samples are False (or 0)\n","      #\n","      # Then, doing a reduce_mean of that vector basically sums up all the \n","      # samples with a True (or 1) and divides by the total number of samples,\n","      # giving the % of correctly predicted samples, which is the accuracy.\n","      correct_preds = tf.equal(tf.argmax(predictions, 1), tf.argmax(labels, 1))\n","      return 100 * tf.reduce_mean(tf.cast(correct_preds, tf.float32))\n","    \n","class ConvNet(object):\n","    def __init__(self, filter_size, filter_num, num_hidden, image_size, num_labels, num_channels, batch_size, drop_rate, regul_rate):\n","      self.filter_size = filter_size\n","      self.filter_num = filter_num\n","      self.num_hidden = num_hidden\n","      self.image_size = image_size\n","      self.num_labels = num_labels\n","      self.num_channels = num_channels\n","      self.batch_size = batch_size\n","\n","      # Set the default value to 1.0. We use this value for all evaluation with\n","      # the test and validation data, so it doesn't need to passed in via feed_dict\n","      # Only during training we pass in a different DROP_RATE value via feed_dict\n","      self.keep_prob = tf.placeholder_with_default(1.0, shape=(), name='keep_prob')\n","      self.drop_rate = drop_rate\n","      \n","      # L2 regularisation rate\n","      self.beta_regul = tf.placeholder_with_default(regul_rate, shape=(), name='beta_regul')\n","      \n","      self.gstep = tf.Variable(1, dtype=tf.int32, \n","                                trainable=False, name='global_step')\n","\n","    '''\n","    Prepare the input data\n","    '''\n","    def _get_data(self):\n","      # Input data.\n","      with tf.name_scope('data'):\n","        # Set up the training dataset\n","        train_ds = tf.data.Dataset.from_tensor_slices((train_dataset, train_labels))\n","        #train_ds = train_ds.shuffle(10000) # if you want to shuffle your data\n","        train_ds = train_ds.batch(self.batch_size)\n","\n","        # Set up the validation and test datasets\n","        valid_ds = tf.data.Dataset.from_tensor_slices((valid_dataset, valid_labels))\n","        valid_ds = valid_ds.batch(valid_dataset.shape[0])   # Get all the validation data in a single batch\n","        test_ds = tf.data.Dataset.from_tensor_slices((test_dataset, test_labels))\n","        test_ds = test_ds.batch(test_dataset.shape[0])   # Get all the test data in a single batch\n","        \n","        # Feedable - Set up a single generic iterator, not tied to any dataset\n","        self.handle = tf.placeholder(tf.string, shape=[])\n","        iterator = tf.data.Iterator.from_string_handle(\n","            self.handle, train_ds.output_types, train_ds.output_shapes)\n","        \n","        # Use the generic iterator to get generic ops for the data and labels, which are still not\n","        # tied to any dataset. We will build the model using these generic ops\n","        self.X, self.y = iterator.get_next()\n","        \n","        # Now create three separate iterators for each of the three datasets\n","        self.train_iter = train_ds.make_initializable_iterator()\n","        self.valid_iter = valid_ds.make_initializable_iterator()\n","        self.test_iter = test_ds.make_initializable_iterator()\n","    \n","    '''\n","    Create all the layers of the network\n","    '''\n","    def _inference(self):\n","      # First convolution and max pool layer\n","      conv1 = _conv_relu(self.X, self.filter_num, self.filter_size, 1, 'SAME', 'conv1')\n","      pool1 = _maxpool(conv1, 2, 2, 'VALID', 'pool1')\n","      \n","      # Second convolution and max pool layer\n","      conv2 = _conv_relu(pool1, self.filter_num, self.filter_size, 1, 'SAME', 'conv2')\n","      pool2 = _maxpool(conv2, 2, 2, 'VALID', 'pool2')\n","      \n","      # Reshape the output of the convolution layers, by flattening it out\n","      # so that it can be fed to the fully connected layer\n","      shape = pool2.get_shape().as_list()\n","      reshape = tf.reshape(pool2, [-1, shape[1] * shape[2] * shape[3]])\n","      \n","      # First fully connected layer viz. hidden layer\n","      fc1 = _fully_connected(reshape, self.num_hidden, tf.nn.relu, 'fc1')\n","      \n","      # Add a dropout layer\n","      dropout = tf.nn.dropout (fc1, self.keep_prob, name='dropout')\n","      \n","      # Second fully connected layer viz. output layer\n","      self.logits = _fully_connected(dropout, self.num_labels, None, 'logits')\n","\n","    '''\n","    Define loss function - use softmax cross entropy with logits\n","    '''\n","    def _loss(self):\n","      with tf.name_scope('loss'):\n","        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y, logits=self.logits), name='loss')\n","        \n","        # Get the collection of weights marked earlier for regularisation, apply\n","        # L2 regularisation to them, and add it to the loss \n","        w_regul_collection = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n","        self.loss += self.beta_regul * tf.reduce_sum ([tf.nn.l2_loss(w) for w in w_regul_collection])   \n","      \n","    '''\n","    Define Optimiser - Use Gradient Descent to minimize cost\n","    '''\n","    def _optimize(self):\n","      self.optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(self.loss)\n","      \n","    '''\n","    Create summaries to write on TensorBoard\n","    '''      \n","    def _summary(self):\n","      with tf.name_scope('summaries'):\n","            tf.summary.scalar('loss', self.loss)\n","            tf.summary.scalar('accuracy', self.accuracy)\n","            tf.summary.histogram('histogram_loss', self.loss)\n","            self.summary_op = tf.summary.merge_all()\n","    \n","    '''\n","    Count the number of right predictions in a batch\n","    '''\n","    def _eval(self):\n","      with tf.name_scope('predict'):\n","        prediction = tf.nn.softmax(self.logits)\n","        self.accuracy = _accuracy (prediction, self.y)\n","\n","    def _train_one_epoch(self, session, saver, train_iter, valid_iter, writer, epoch, gstep):\n","        # Initialise the training data and get handles for the training and \n","        # validation to pass to the Feedable\n","        session.run(train_iter.initializer)\n","        train_handle = session.run(train_iter.string_handle())\n","        valid_handle = session.run(valid_iter.string_handle())\n","        \n","        # We don't need to pre-decide the number of mini-batches. We keep going\n","        # through the dataset, picking up one batch size at a time. When the\n","        # data is finished, we get an exception which allows us to exit the loop\n","        while True:\n","          try:\n","            # Run the optimiser and predictions on the training data\n","            # Set the drop rate since we're in the training phase (we will \n","            # not do this for validation and testing)\n","            _, l, train_accuracy, summaries = session.run(\n","              [self.optimizer, self.loss, self.accuracy, self.summary_op], feed_dict={self.handle: train_handle, self.keep_prob: self.drop_rate})\n","\n","            if (gstep % 5 == 0):\n","              writer.add_summary(summaries, global_step=gstep)\n","\n","              print('Minibatch loss at epoch %d / step %d: %f' % (epoch, gstep, l))\n","              print('Minibatch accuracy at epoch %d / step %d: %.1f%%' % (epoch, gstep, train_accuracy))\n","\n","              # We use a Feedable Iterator (instead of a Reinitialisable one) because\n","              # of the below logic, where we are evaluating the validation data in\n","              # the middle of an epoch. If we only evaluated it at the end of the\n","              # epoch (like we do for the test data), a Reinitialisable Iterator\n","              # would have been fine.\n","\n","              # Initialise the validation data\n","              session.run(valid_iter.initializer)\n","              valid_accuracy = session.run(self.accuracy, feed_dict={self.handle: valid_handle})\n","              print('Validation accuracy at epoch %d / step %d: %.1f%%' % (epoch, gstep, valid_accuracy))\n"," \n","            gstep += 1\n","\n","          except tf.errors.OutOfRangeError:\n","            # Data is over, so exit the loop\n","            break\n","          \n","        # Save model state as of this global step number\n","        saver.save(session, 'checkpoints/convnet_mnist', gstep)\n","        \n","        return gstep\n","\n","\n","    def _eval_once(self, session, test_iter, writer, epoch, gstep):\n","        # The `Iterator.string_handle()` method returns a tensor that can be evaluated\n","        # and used to feed the `handle` placeholder.\n","        test_handle = session.run(test_iter.string_handle())\n","        \n","        # Run one pass over the test data\n","        session.run(test_iter.initializer)\n","        test_accuracy, test_summaries = session.run([self.accuracy, self.summary_op], feed_dict={self.handle: test_handle})\n","        writer.add_summary(test_summaries, global_step=gstep)\n","        print('Test accuracy at epoch %d : %.1f%%' % (epoch, test_accuracy))\n","    \n","    def build(self):\n","        self._get_data()\n","        self._inference()\n","        self._loss()\n","        self._optimize()\n","        self._eval()\n","        self._summary()\n","\n","    def train(self, n_epochs):\n","      \n","      # Directory to store checkpoints\n","      CKPT_DIR = 'checkpoints'\n","      safe_mkdir(CKPT_DIR)\n","        \n","      # Save summary data for Tensorboard visualisation. Create separate graphs\n","      # for the training and test data\n","      LOG_DIR = 'graphs'\n","      train_writer = tf.summary.FileWriter(LOG_DIR + '/train', graph)\n","      test_writer = tf.summary.FileWriter(LOG_DIR + '/test', graph)\n","      \n","      with tf.Session(graph=graph) as session:\n","        tf.global_variables_initializer().run()\n","        print('Initialized')\n","\n","        # Restore model state from prior checkpoint if it exists\n","        saver = tf.train.Saver()\n","        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n","        if ckpt and ckpt.model_checkpoint_path:\n","           print('Found checkpoint, Restoring ', ckpt.model_checkpoint_path)\n","           saver.restore(session, ckpt.model_checkpoint_path)\n","        \n","        # Global counter of number of training loops done so far\n","        gstep = self.gstep.eval()\n","        \n","        for epoch in range(n_epochs):\n","            # Train one epoch with the training and validation data\n","            gstep = self._train_one_epoch(session, saver, self.train_iter, self.valid_iter, train_writer, epoch, gstep)\n","            \n","            # Then evaluate with the test data\n","            self._eval_once(session, self.test_iter, test_writer, epoch, gstep)\n","        \n","      train_writer.close()\n","      test_writer.close()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f__wgbVD1K7j","colab_type":"code","outputId":"cdcaa83d-1375-4556-fdc5-75b1533cdcad","executionInfo":{"status":"ok","timestamp":1545646725847,"user_tz":-330,"elapsed":2389,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":612}},"cell_type":"code","source":["FILTER_SIZE = 5\n","FILTER_NUM = 16\n","NUM_HIDDEN = 64\n","IMAGE_SIZE = 28\n","NUM_LABELS = 10\n","NUM_CHANNELS = 1 # grayscale\n","BATCH_SIZE = 4\n","DROP_RATE = 0.5\n","REGUL_RATE = 0.001 # Beta rate for regularisation\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","   model = ConvNet(FILTER_SIZE, FILTER_NUM, NUM_HIDDEN, IMAGE_SIZE, NUM_LABELS, NUM_CHANNELS, BATCH_SIZE, DROP_RATE, REGUL_RATE)\n","   model.build()\n","\n","model.train(n_epochs=2)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["Initialized\n","Found checkpoint, Restoring  checkpoints/convnet_mnist-51\n","INFO:tensorflow:Restoring parameters from checkpoints/convnet_mnist-51\n","Minibatch loss at epoch 0 / step 5: 0.238748\n","Minibatch accuracy at epoch 0 / step 5: 100.0%\n","Validation accuracy at epoch 0 / step 5: 75.0%\n","Minibatch loss at epoch 0 / step 10: 0.299449\n","Minibatch accuracy at epoch 0 / step 10: 100.0%\n","Validation accuracy at epoch 0 / step 10: 70.0%\n","Minibatch loss at epoch 0 / step 15: 0.225857\n","Minibatch accuracy at epoch 0 / step 15: 100.0%\n","Validation accuracy at epoch 0 / step 15: 70.0%\n","Minibatch loss at epoch 0 / step 20: 0.368412\n","Minibatch accuracy at epoch 0 / step 20: 100.0%\n","Validation accuracy at epoch 0 / step 20: 65.0%\n","Minibatch loss at epoch 0 / step 25: 0.238606\n","Minibatch accuracy at epoch 0 / step 25: 100.0%\n","Validation accuracy at epoch 0 / step 25: 65.0%\n","Test accuracy at epoch 0 : 95.0%\n","Minibatch loss at epoch 1 / step 30: 0.243519\n","Minibatch accuracy at epoch 1 / step 30: 100.0%\n","Validation accuracy at epoch 1 / step 30: 65.0%\n","Minibatch loss at epoch 1 / step 35: 0.684518\n","Minibatch accuracy at epoch 1 / step 35: 75.0%\n","Validation accuracy at epoch 1 / step 35: 70.0%\n","Minibatch loss at epoch 1 / step 40: 0.232064\n","Minibatch accuracy at epoch 1 / step 40: 100.0%\n","Validation accuracy at epoch 1 / step 40: 65.0%\n","Minibatch loss at epoch 1 / step 45: 0.274594\n","Minibatch accuracy at epoch 1 / step 45: 100.0%\n","Validation accuracy at epoch 1 / step 45: 65.0%\n","Minibatch loss at epoch 1 / step 50: 0.226519\n","Minibatch accuracy at epoch 1 / step 50: 100.0%\n","Validation accuracy at epoch 1 / step 50: 65.0%\n","Test accuracy at epoch 1 : 92.5%\n"],"name":"stdout"}]},{"metadata":{"id":"ClAiD4YR2EUo","colab_type":"code","colab":{}},"cell_type":"code","source":["!rm -r graphs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3nkKW0Qo-OH4","colab_type":"text"},"cell_type":"markdown","source":["### Results so far\n","\n"]},{"metadata":{"id":"tNhVaHf1-XS_","colab_type":"text"},"cell_type":"markdown","source":["Initialized\n","\n","Test accuracy at epoch 0 : 93.4%\n","\n","Test accuracy at epoch 1 : 94.2%\n","\n","Test accuracy at epoch 2 : 94.6%\n","\n","Test accuracy at epoch 3 : 94.9%\n","\n","Test accuracy at epoch 4 : 95.1%\n","\n","Test accuracy at epoch 5 : 95.2%"]},{"metadata":{"id":"BlA48NOcL8GK","colab_type":"text"},"cell_type":"markdown","source":["### Create an Improved Convolutional Layer with Max Pool - V2\n","\n","*   Use tf.layers"]},{"metadata":{"id":"Ib7rRZc6L6AE","colab_type":"text"},"cell_type":"markdown","source":["### Optionally set up Tensorboard and grok for Visualisation"]},{"metadata":{"id":"FZKV-NG0l7pR","colab_type":"code","colab":{}},"cell_type":"code","source":["# Set the LOGDIR correctly to use Tensorboard\n","LOG_DIR = 'graphs'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BxTmOkobl1_w","colab_type":"code","colab":{}},"cell_type":"code","source":["! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[]},{"metadata":{"id":"125oN8YVmETI","colab_type":"code","colab":{}},"cell_type":"code","source":["get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-2MzFSTemKSf","colab_type":"code","colab":{}},"cell_type":"code","source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OBJ1GXpMmOJ6","colab_type":"code","outputId":"651c1c40-b4ad-49c6-e036-dfc3bee642d4","executionInfo":{"status":"ok","timestamp":1545634728779,"user_tz":-330,"elapsed":11257,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":20,"outputs":[{"output_type":"stream","text":["https://39a5ed08.ngrok.io\n"],"name":"stdout"}]}]}