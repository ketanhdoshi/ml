{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF Function Examples.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "d7_jnO310H6c",
        "hlFEVLSGurvZ",
        "-RKzCJ6Le7hp",
        "CEJhAQ10fxNS",
        "tQ18nvg7V3Xn",
        "ZxlLyLlrWhpU",
        "E-hswCNGE-RF",
        "vGXv8z1dMbsa",
        "qdRDnu1GmOpP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanhdoshi/ml/blob/master/examples/TF_Function_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Bqf3DguNcy"
      },
      "source": [
        "## Simple examples of each the commonly use TF functions to understand the calculations they are doing under the covers\n",
        "\n",
        "\n",
        "Tensorflow functions:\n",
        "*  nn.conv2d()\n",
        "*  nn.l2_loss()\n",
        "*  nn.reduce_mean()\n",
        "*  nn.dropout()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1JFVLLVWKbG"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7_jnO310H6c"
      },
      "source": [
        "### [Example](https://www.dotnetperls.com/conv2d-tensorflow) to illustrate conv2d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtsmLxhg0HCK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "c2f86c26-4e2d-438a-b07a-1bdc67a39c68"
      },
      "source": [
        "# The input data (could be an image).\n",
        "temp = tf.constant([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], tf.float32)\n",
        "temp2 = tf.reshape(temp, [1, 3, 3, 2])\n",
        "\n",
        "# A filter (affects the convolution).\n",
        "#filter = tf.constant([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], tf.float32)\n",
        "filter = tf.constant([1, 1, 0, 0, 0, 0, 0, 0], tf.float32)\n",
        "filter2 = tf.reshape(filter, [2, 2, 2, 1])\n",
        "\n",
        "# Use convolution layer on 4D matrices.\n",
        "convolution = tf.nn.conv2d(temp2, filter2, [1, 1, 1, 1], padding=\"VALID\")\n",
        "\n",
        "# Initialize session.\n",
        "session = tf.Session()\n",
        "tf.global_variables_initializer()\n",
        "\n",
        "# Evaluate all our variables.\n",
        "print(\"INPUT\")\n",
        "print(session.run(temp2))\n",
        "print(\"first sample, first channel\")\n",
        "print(session.run(temp2[0, :, :, 0]))\n",
        "print(\"first sample, second channel\")\n",
        "print(session.run(temp2[0, :, :, 1]))\n",
        "print(\"FILTER\")\n",
        "print(session.run(filter2))\n",
        "print(\"filter first channel\")\n",
        "print(session.run(filter2[:, :, 0, :]))\n",
        "print(\"filter second channel\")\n",
        "print(session.run(filter2[:, :, 1, :]))\n",
        "print(\"CONVOLUTION\")\n",
        "print(session.run(convolution))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT\n",
            "[[[[1. 1.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]\n",
            "\n",
            "  [[0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]\n",
            "\n",
            "  [[0. 0.]\n",
            "   [0. 0.]\n",
            "   [0. 0.]]]]\n",
            "first sample, first channel\n",
            "[[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "first sample, second channel\n",
            "[[1. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "FILTER\n",
            "[[[[1.]\n",
            "   [1.]]\n",
            "\n",
            "  [[0.]\n",
            "   [0.]]]\n",
            "\n",
            "\n",
            " [[[0.]\n",
            "   [0.]]\n",
            "\n",
            "  [[0.]\n",
            "   [0.]]]]\n",
            "filter first channel\n",
            "[[[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]]]\n",
            "filter second channel\n",
            "[[[1.]\n",
            "  [0.]]\n",
            "\n",
            " [[0.]\n",
            "  [0.]]]\n",
            "CONVOLUTION\n",
            "[[[[2.]\n",
            "   [0.]]\n",
            "\n",
            "  [[0.]\n",
            "   [0.]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlFEVLSGurvZ"
      },
      "source": [
        "### Example to understand reshape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMPyz9IZWi3y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "f30f37c3-3d75-4e6b-f55a-fd65dc626e32"
      },
      "source": [
        "temp = tf.constant([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], tf.float32)\n",
        "temp2 = tf.reshape(temp, [1, 2, 3, 4])\n",
        "print(\"INPUT\")\n",
        "print(session.run(temp2))\n",
        "print(\"first sample, first channel\")\n",
        "print(session.run(temp2[0, :, :, 0]))\n",
        "print(\"first sample, second channel\")\n",
        "print(session.run(temp2[0, :, :, 1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT\n",
            "[[[[ 1.  2.  3.  4.]\n",
            "   [ 5.  6.  7.  8.]\n",
            "   [ 9. 10. 11. 12.]]\n",
            "\n",
            "  [[13. 14. 15. 16.]\n",
            "   [17. 18. 19. 20.]\n",
            "   [21. 22. 23. 24.]]]]\n",
            "first sample, first channel\n",
            "[[ 1.  5.  9.]\n",
            " [13. 17. 21.]]\n",
            "first sample, second channel\n",
            "[[ 2.  6. 10.]\n",
            " [14. 18. 22.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RKzCJ6Le7hp"
      },
      "source": [
        "### [Example](https://www.dotnetperls.com/stack-tensorflow) to understand stack and unstack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhzzpMDLfBRU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "9279417a-ff1b-4ac5-acfa-eaf6796c81ab"
      },
      "source": [
        "# Shape is (2, 3)\n",
        "t1 = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "t2 = tf.constant([[7, 8, 9], [10, 11, 12]])\n",
        "t3 = tf.constant([[13, 14, 15], [16, 17, 18]])\n",
        "\n",
        "\n",
        "tstack = tf.stack([t1, t2], axis=1) # Shape is (2, 2, 3)\n",
        "\n",
        "tstack2 = tf.stack([t1, t2], axis=2) # Shape is (2, 3, 2)\n",
        "\n",
        "tstack_triple = tf.stack([t1, t2, t3], axis=2) # Shape is (2, 3, 3)\n",
        "\n",
        "unstack0 = tf.unstack(tstack, axis=0)\n",
        "unstack1 = tf.unstack(tstack, axis=1)\n",
        "unstack2 = tf.unstack(tstack, axis=2)\n",
        "\n",
        "session = tf.Session()\n",
        "\n",
        "print(\"STACK Axis 1\")\n",
        "print(session.run(tstack))\n",
        "print(session.run(tf.shape(tstack)))\n",
        "\n",
        "print(\"STACK Axis 2\")\n",
        "print(session.run(tstack2))\n",
        "print(session.run(tf.shape(tstack2)))\n",
        "\n",
        "print(\"STACK Axis triple\")\n",
        "print(session.run(tstack_triple))\n",
        "print(session.run(tf.shape(tstack_triple)))\n",
        "\n",
        "print(\"UNSTACK Axis 0\")\n",
        "print(session.run(unstack0))\n",
        "print(session.run(tf.shape(unstack0)))\n",
        "\n",
        "print(\"UNSTACK Axis 1\")\n",
        "print(session.run(unstack1))\n",
        "print(session.run(tf.shape(unstack1)))\n",
        "\n",
        "print(\"UNSTACK Axis 2\")\n",
        "print(session.run(unstack2))\n",
        "print(session.run(tf.shape(unstack2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STACK Axis 1\n",
            "[[[ 1  2  3]\n",
            "  [ 7  8  9]]\n",
            "\n",
            " [[ 4  5  6]\n",
            "  [10 11 12]]]\n",
            "[2 2 3]\n",
            "STACK Axis 2\n",
            "[[[ 1  7]\n",
            "  [ 2  8]\n",
            "  [ 3  9]]\n",
            "\n",
            " [[ 4 10]\n",
            "  [ 5 11]\n",
            "  [ 6 12]]]\n",
            "[2 3 2]\n",
            "STACK Axis triple\n",
            "[[[ 1  7 13]\n",
            "  [ 2  8 14]\n",
            "  [ 3  9 15]]\n",
            "\n",
            " [[ 4 10 16]\n",
            "  [ 5 11 17]\n",
            "  [ 6 12 18]]]\n",
            "[2 3 3]\n",
            "UNSTACK Axis 0\n",
            "[array([[1, 2, 3],\n",
            "       [7, 8, 9]], dtype=int32), array([[ 4,  5,  6],\n",
            "       [10, 11, 12]], dtype=int32)]\n",
            "[2 2 3]\n",
            "UNSTACK Axis 1\n",
            "[array([[1, 2, 3],\n",
            "       [4, 5, 6]], dtype=int32), array([[ 7,  8,  9],\n",
            "       [10, 11, 12]], dtype=int32)]\n",
            "[2 2 3]\n",
            "UNSTACK Axis 2\n",
            "[array([[ 1,  7],\n",
            "       [ 4, 10]], dtype=int32), array([[ 2,  8],\n",
            "       [ 5, 11]], dtype=int32), array([[ 3,  9],\n",
            "       [ 6, 12]], dtype=int32)]\n",
            "[3 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEJhAQ10fxNS"
      },
      "source": [
        "### [Example](https://stackoverflow.com/questions/41534593/why-would-i-ever-use-tf-concat-instead-of-tf-stack) to understand concat\n",
        "\n",
        "And how it is different from stack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K106Mi_Uf0XE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "32396a66-0c35-4a49-8f9c-988be7768f44"
      },
      "source": [
        "t1 = tf.constant([[1, 2, 3]])\n",
        "t2 = tf.constant([[4, 5, 6]])\n",
        "\n",
        "# Shape is (1, 2, 3)\n",
        "tstack = tf.stack([t1, t2], axis=1)\n",
        "\n",
        "# Shape is (1, 6)\n",
        "tconcat = tf.concat([t1, t2], axis=1)\n",
        "\n",
        "session = tf.Session()\n",
        "\n",
        "print(\"T1\")\n",
        "print(session.run(t1))\n",
        "print(\"T2\")\n",
        "print(session.run(t2))\n",
        "print(\"STACK\")\n",
        "print(session.run(tstack))\n",
        "print(session.run(tf.shape(tstack)))\n",
        "print(\"CONCAT\")\n",
        "print(session.run(tconcat))\n",
        "print(session.run(tf.shape(tconcat)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "T1\n",
            "[[1 2 3]]\n",
            "T2\n",
            "[[4 5 6]]\n",
            "STACK\n",
            "[[[1 2 3]\n",
            "  [4 5 6]]]\n",
            "[1 2 3]\n",
            "CONCAT\n",
            "[[1 2 3 4 5 6]]\n",
            "[1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ18nvg7V3Xn"
      },
      "source": [
        "### [Example](https://www.dotnetperls.com/arg-max-tensorflow) to understand argmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U58toPhV_q1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e92928f-1e33-458f-a145-3287522ed4cd"
      },
      "source": [
        "values = [10, 0, 30, 40, 50, 60, 3, 1, 2]\n",
        "\n",
        "# Get the index of the minimum value (which is the value 0).\n",
        "# ... This occurs at index 1.\n",
        "min = tf.argmin(values, 0)\n",
        "\n",
        "# Find maximum.\n",
        "max = tf.argmax(values, 0)\n",
        "\n",
        "session = tf.Session()\n",
        "\n",
        "# Find minimum in vector.\n",
        "print(session.run(min))\n",
        "print(session.run(max))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxlLyLlrWhpU"
      },
      "source": [
        "### [Example](https://www.dotnetperls.com/reduce-sum-tensorflow) to understand reduce_sum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5n3tRJ01Wkie",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "e6f003f6-d81e-49ed-d6ee-f6eccfae6cef"
      },
      "source": [
        "a = tf.constant([[1, 3], [2, 0], [0, 1]])\n",
        "# Sum elements from different parts of the array.\n",
        "# ... With no second argument, everything is summed.\n",
        "#     Second argument indicates what axis to sum.\n",
        "b = tf.reduce_sum(a)\n",
        "c = tf.reduce_sum(a, 0)\n",
        "d = tf.reduce_sum(a, 1)\n",
        "\n",
        "# Just for looping.\n",
        "tensors = [b, c, d]\n",
        "\n",
        "# Loop over our tensors and run a Session on the graph.\n",
        "for tensor in tensors:\n",
        "    result = tf.Session().run(tensor)\n",
        "    print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n",
            "[3 4]\n",
            "[4 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-hswCNGE-RF"
      },
      "source": [
        "### [Example](https://www.dotnetperls.com/reduce-mean-tensorflow) to illustrate how tf.reduce_mean() works\n",
        "\n",
        "reduce_mean() takes an array of cost values and reduces it to a single scalar cost number by calculating the average of all the costs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UaFUa7_EzoO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "33770941-a811-490d-e4b3-839188e2d0a3"
      },
      "source": [
        "a = tf.constant([[100, 110], [10, 20], [1000, 1100]])\n",
        "\n",
        "# Use reduce_mean to compute the average (mean) across a dimension.\n",
        "b = tf.reduce_mean(a) # Average of all six values in the 2D array\n",
        "c = tf.reduce_mean(a, axis=0)  # Average of all the values in a column, one column at a time\n",
        "d = tf.reduce_mean(a, axis=1)  # Average of all the values in a row, one row at a time\n",
        "\n",
        "session = tf.Session()\n",
        "\n",
        "print(\"INPUT\")\n",
        "print(session.run(a))\n",
        "print(\"REDUCE MEAN\")\n",
        "print(session.run(b))\n",
        "print(\"REDUCE MEAN AXIS 0\")\n",
        "print(session.run(c))\n",
        "print(\"REDUCE MEAN AXIS 1\")\n",
        "print(session.run(d))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT\n",
            "[[ 100  110]\n",
            " [  10   20]\n",
            " [1000 1100]]\n",
            "REDUCE MEAN\n",
            "390\n",
            "REDUCE MEAN AXIS 0\n",
            "[370 410]\n",
            "REDUCE MEAN AXIS 1\n",
            "[ 105   15 1050]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGXv8z1dMbsa"
      },
      "source": [
        "### [Example](https://www.quora.com/How-does-one-implement-the-L2-loss-in-tensor-flow) to illustrate how tf.nn.l2_loss() works\n",
        "\n",
        "It just calculates the sum of squares of all individual elements in tensor irrespective of dimensions. Tensor can be of any dimension "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbxJj91-MpeS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "e2b45642-bbff-4dcd-bd9c-bfd8329fecf4"
      },
      "source": [
        "# Calculate l2_loss in different ways with the same set of numbers\n",
        "a = np.array ([1.,2.,-3.,1.,1.,1.])\n",
        "b = a.reshape (2, 3)\n",
        "\n",
        "w = tf.constant(a)\n",
        "l = tf.reduce_sum(tf.square(w)) / 2\n",
        "\n",
        "w1D = tf.constant(a)\n",
        "l1D = tf.nn.l2_loss(w1D)\n",
        "\n",
        "w2D = tf.constant(b)\n",
        "l2D = tf.nn.l2_loss(w2D)\n",
        "\n",
        "session = tf.Session()\n",
        "\n",
        "print(\"Half of Sum of squares\")\n",
        "print(session.run(l))\n",
        "\n",
        "print(\"L2_LOSS 1D\")\n",
        "print(session.run(l1D))\n",
        "\n",
        "print(\"L2_LOSS 2D\")\n",
        "print(session.run(l2D))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Half of Sum of squares\n",
            "8.5\n",
            "L2_LOSS 1D\n",
            "8.5\n",
            "L2_LOSS 2D\n",
            "8.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdRDnu1GmOpP"
      },
      "source": [
        "### [Example](https://www.dotnetperls.com/dropout-tensorflow) to illustrate how tf.nn.dropout() works\n",
        "\n",
        "It randomly sets some proportion of the input elements to 0, based on the keep_prob parameter. The values of the remaining elements are scaled up accordingly by a factor of 1/keep_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHDN7fdCpDWf"
      },
      "source": [
        "**Details on the keep_prob parameter** and how it affects which elements are selected, and how their values are scaled up\n",
        "\n",
        "If you have a large amount of neurons, like 10,000 in a layer, and the **keep_prob** is let's say, 0.3, then 3,000 is the expected value of the number of neurons kept. So it's more or less the same thing to say that a keep_prob of 0.3 means to keep the value of 3,000 randomly chosen ones of the 10,000 neurons. But not exactly, because the actual number might vary a bit from 3,000.\n",
        "\n",
        "The random decision to drop a neuron or not is recalculated for each invocation of the network, so **you will have a different set of neurons dropped on every iteration**. \n",
        "\n",
        "**Scaling** comes into the picture because if you drop a certain number of neurons, then the expected sum of the layer will be reduced. So the remaining ones are multiplied to feed forward the same magnitude of values as they would otherwise.\n",
        "\n",
        "Let's say the network had n neurons and we applied dropout rate 1/2\n",
        "\n",
        "Training phase, we would be left with n/2 neurons. So if you were expecting output x with all the neurons, now you will get on x/2. So for every batch, the network weights are trained according to this x/2\n",
        "\n",
        "Testing/Inference/Validation phase, we dont apply any dropout so the output is x. So, in this case, the output would be with x and not x/2, which would give you the incorrect result. So what you can do is scale it to x/2 during testing.\n",
        "\n",
        "Rather than the above scaling specific to Testing phase. What Tensorflow's dropout layer does is that whether it is with dropout or without (Training or testing), it scales the output so that the sum is constant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF2SdopUmr0p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5788e558-dbdc-4b8a-e620-ed1e3a28e45b"
      },
      "source": [
        "x = [1., 2., 1., 4., 1., 2.]\n",
        "\n",
        "# Compute dropout on our tensor with keep_prob of 0.5\n",
        "# So, randomly half of the elements will be set to 0 and\n",
        "# the values of the remaining elements will be doubled ie. set to 1/0.5 = 2\n",
        "result = tf.nn.dropout(x, 0.5)\n",
        "session = tf.Session()\n",
        "print(session.run(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 2. 8. 2. 4.]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}