{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow Examples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanhdoshi/ml/blob/master/examples/Tensorflow_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnN0kbiRZdQz"
      },
      "source": [
        "### Basic Tensorflow concepts\n",
        "\n",
        "Based on examples from this article - https://medium.com/learning-machine-learning/introduction-to-tensorflow-estimators-part-1-39f9eb666bc7\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TaMseb6ZOD3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "480ccfae-95b4-4fb1-9d0f-a08c9ab955bf"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2HIJLpAZYSd"
      },
      "source": [
        "Now let's add some constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRmrHmf0Zen7"
      },
      "source": [
        "a = tf.constant(5,name = \"a\")\n",
        "b = tf.constant(3, name = \"b\")\n",
        "result = tf.add(a,b,name='add_a_b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J5bO5joGnfi"
      },
      "source": [
        "And some variables and placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwxDC3DFGlr1"
      },
      "source": [
        "c = tf.Variable(3,dtype=tf.float32)\n",
        "d = tf.placeholder(dtype = tf.float32,shape=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4wLz3wkGPtq"
      },
      "source": [
        "Next we'll run it and see the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eckJUMyFGJxE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba2a8085-b5bd-4cae-85c8-7a17c49e0fe5"
      },
      "source": [
        "sess = tf.Session()\n",
        "sess.run(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC1cqYHoGyMh"
      },
      "source": [
        "Now let's initialise the variables and placeholders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2D7vjrWXG3pI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "463a1e0d-604e-4e1a-d68c-9e9a299fe036"
      },
      "source": [
        "sess.run(tf.global_variables_initializer())\n",
        "print(sess.run(c,feed_dict = {c:14}))\n",
        "print(sess.run(d,feed_dict = {d:[1,1,3,5]}))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14.0\n",
            "[1. 1. 3. 5.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHcqCxjgLySq"
      },
      "source": [
        "### Datasets and Iterators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM5CxeFzyp1w"
      },
      "source": [
        "This is a very simple example based on this article - https://kratzert.github.io/2017/06/15/example-of-tensorflows-new-input-pipeline.html\n",
        "\n",
        "It demonstrates how Datasets, Iterators and Sessions interact. It shows how to fetch data one element at a time from an iterator defined over a Dataset. Since it doesn't include the complications about models and input functions and so on, you can see how the data elements are fetched when the session is run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW1qZTZDxaWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f99effb9-643a-4e09-b569-f52584499415"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Toy data\n",
        "train_imgs = tf.constant(['train/img1.png', 'train/img2.png',\n",
        "                          'train/img3.png', 'train/img4.png',\n",
        "                          'train/img5.png', 'train/img6.png'])\n",
        "train_labels = tf.constant([0, 0, 0, 1, 1, 1])\n",
        "\n",
        "val_imgs = tf.constant(['val/img1.png', 'val/img2.png',\n",
        "                        'val/img3.png', 'val/img4.png'])\n",
        "val_labels = tf.constant([0, 0, 1, 1])\n",
        "\n",
        "# create TensorFlow Dataset objects\n",
        "tr_data = tf.data.Dataset.from_tensor_slices((train_imgs, train_labels))\n",
        "val_data = tf.data.Dataset.from_tensor_slices((val_imgs, val_labels))\n",
        "\n",
        "# create TensorFlow Iterator object of type Reinitialisable\n",
        "iterator = tf.data.Iterator.from_structure(tr_data.output_types,\n",
        "                                   tr_data.output_shapes)\n",
        "next_element = iterator.get_next()\n",
        "\n",
        "# create two initialization ops to switch between the datasets\n",
        "training_init_op = iterator.make_initializer(tr_data)\n",
        "validation_init_op = iterator.make_initializer(val_data)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # initialize the iterator on the training data\n",
        "    sess.run(training_init_op)\n",
        "\n",
        "    # get each element of the training dataset until the end is reached\n",
        "    while True:\n",
        "        try:\n",
        "            elem = sess.run(next_element)\n",
        "            print(elem)\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            print(\"End of training dataset.\")\n",
        "            break\n",
        "\n",
        "    # initialize the iterator on the validation data\n",
        "    sess.run(validation_init_op)\n",
        "\n",
        "    # get each element of the validation dataset until the end is reached\n",
        "    while True:\n",
        "        try:\n",
        "            elem = sess.run(next_element)\n",
        "            print(elem)\n",
        "        except tf.errors.OutOfRangeError:\n",
        "            print(\"End of training dataset.\")\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(b'train/img1.png', 0)\n",
            "(b'train/img2.png', 0)\n",
            "(b'train/img3.png', 0)\n",
            "(b'train/img4.png', 1)\n",
            "(b'train/img5.png', 1)\n",
            "(b'train/img6.png', 1)\n",
            "End of training dataset.\n",
            "(b'val/img1.png', 0)\n",
            "(b'val/img2.png', 0)\n",
            "(b'val/img3.png', 1)\n",
            "(b'val/img4.png', 1)\n",
            "End of training dataset.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyUE5006MBg3"
      },
      "source": [
        "### Another Datasets, Iterators, Sessions Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bye8fP64CvMV"
      },
      "source": [
        "This example is based on the article - https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html and the related code which is https://github.com/tensorflow/models/blob/master/samples/outreach/blogs/blog_estimators_dataset.py\n",
        "\n",
        "It shows how to use Datasets and how they work with Iterators and Sessions to fetch one Element of the data at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o-zX9rs-dtp"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRPu2LpEDGhN"
      },
      "source": [
        "**Upload the data file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ECjkFxy_Eq5",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "122fc8ff-1aef-4266-9de8-f937e1d7b2e2"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-01f1fa1b-35d6-4fa3-ae79-831de4224d11\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-01f1fa1b-35d6-4fa3-ae79-831de4224d11\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving iris_training.csv to iris_training.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRds42UT_JTz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "880ba44e-a1e5-46ac-8d94-7047b7e5d3f0"
      },
      "source": [
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "User uploaded file \"iris_training.csv\" with length 2194 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atKBcDoKDJlU"
      },
      "source": [
        "**Training Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Qs3hb_2_S6x"
      },
      "source": [
        "# The CSV features in our training & test data\n",
        "feature_names = [\n",
        "    'SepalLength',\n",
        "    'SepalWidth',\n",
        "    'PetalLength',\n",
        "    'PetalWidth']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAr58yHBDOga"
      },
      "source": [
        "**Input Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90XUkLrB_ZiC"
      },
      "source": [
        "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
        "    def decode_csv(line):\n",
        "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
        "        label = parsed_line[-1]  # Last element is the label\n",
        "        del parsed_line[-1]  # Delete last element\n",
        "        features = parsed_line  # Everything but last elements are the features\n",
        "        d = dict(zip(feature_names, features)), label\n",
        "        return d\n",
        "\n",
        "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
        "               .skip(1)  # Skip header row\n",
        "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
        "    if perform_shuffle:\n",
        "        # Randomizes input using a window of 256 elements (read into memory)\n",
        "        dataset = dataset.shuffle(buffer_size=256)\n",
        "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
        "    dataset = dataset.batch(4)  # Batch size to use\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    batch_features, batch_labels = iterator.get_next()\n",
        "    return batch_features, batch_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pEzfHXdDRMV"
      },
      "source": [
        "**As an illustration of how Datasets work, Fetch Data one element at at time**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVx347Cx_mcd"
      },
      "source": [
        "FILE_TRAIN = \"iris_training.csv\"\n",
        "next_batch = my_input_fn(FILE_TRAIN, False)  # Will return Batch_Size elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8rBGC8_yNI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e11b4be9-2312-4cea-b06a-82274f8a72dd"
      },
      "source": [
        "# Now let's try it out, retrieving and printing one batch of data.\n",
        "# Although this code looks strange, you don't need to understand\n",
        "# the details.\n",
        "with tf.Session() as sess:\n",
        "    first_batch = sess.run(next_batch)\n",
        "    second_batch = sess.run(next_batch)\n",
        "print(first_batch)\n",
        "print(second_batch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "({'SepalLength': array([6.4, 5. , 4.9, 4.9], dtype=float32), 'SepalWidth': array([2.8, 2.3, 2.5, 3.1], dtype=float32), 'PetalLength': array([5.6, 3.3, 4.5, 1.5], dtype=float32), 'PetalWidth': array([2.2, 1. , 1.7, 0.1], dtype=float32)}, array([2, 1, 2, 0], dtype=int32))\n",
            "({'SepalLength': array([5.7, 4.4, 5.4, 6.9], dtype=float32), 'SepalWidth': array([3.8, 3.2, 3.4, 3.1], dtype=float32), 'PetalLength': array([1.7, 1.3, 1.5, 5.1], dtype=float32), 'PetalWidth': array([0.3, 0.2, 0.4, 2.3], dtype=float32)}, array([0, 0, 0, 2], dtype=int32))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_02E90hEUqT"
      },
      "source": [
        "**Train an Estimator using the Input Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwwOh5GAEa1c"
      },
      "source": [
        "# Create the feature_columns, which specifies the input to our model.\n",
        "# All our input features are numeric, so use numeric_column for each one.\n",
        "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\n",
        "\n",
        "# Create a deep neural network regression classifier.\n",
        "# Use the DNNClassifier pre-made estimator\n",
        "PATH = \".\"\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "   feature_columns=feature_columns, # The input features to our model\n",
        "   hidden_units=[10, 10], # Two layers, each with 10 neurons\n",
        "   n_classes=3,\n",
        "   model_dir=PATH) # Path to where checkpoints etc are stored"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RA1tGq1FQge"
      },
      "source": [
        "# This is where we hook up Datasets with the Estimators! Estimators needs data to perform training, evaluation, and prediction, \n",
        "# and it uses the input_fn to fetch the data. Estimators require an input_fn with no arguments, so we create a function with \n",
        "# no arguments using lambda, which calls our input_fn with the desired arguments: the file_path, shuffle setting, and repeat_count.\n",
        "\n",
        "# Train our model, use the previously defined function my_input_fn\n",
        "# Input to training is a file with training example\n",
        "# Stop training after 8 iterations of train data (epochs)\n",
        "classifier.train(\n",
        "    input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohZSrmN8Ndv8"
      },
      "source": [
        "### Linear Regression model\n",
        "\n",
        "Based on examples from this article - https://medium.com/learning-machine-learning/introduction-to-tensorflow-estimators-part-1-39f9eb666bc7\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jxtb-wjSNdv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "480ccfae-95b4-4fb1-9d0f-a08c9ab955bf"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-lW9q1WHe29"
      },
      "source": [
        "# Create our model parameters, W and b which stands for weight and bias\n",
        "W = tf.Variable([3.0],name='weight')\n",
        "b = tf.Variable([-2.0],name='bias')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4XekVV6H2Xw"
      },
      "source": [
        "# training data - create two placeholders X and y for the training set and labels. We’ll feed the inputs to them during training\n",
        "X = tf.placeholder(tf.float32)\n",
        "# y\n",
        "Y = tf.placeholder(tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Xyq7b9IEoO"
      },
      "source": [
        "# Define the model for a single feature.\n",
        "predictions = W*X + b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCVtau-qIbDI"
      },
      "source": [
        "# loss function. Here we use sum of squared errors. errors are calculated for each sample or instance, while the loss tells us how good our model is performing on the whole dataset.\n",
        "loss = tf.reduce_sum(tf.square(predictions-Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_spDO4QhIwPZ"
      },
      "source": [
        "Optimizers are used for finding the best value for some parameters with respect to some loss function in machine learning models. There’s many kind of optimizers, the one we are using here is called Gradient Descent.We start with a random value for the weight and the bias. The optimizer updates the weight and the bias parameter in a direction(increasing or decreasing the numbers) to minimize the value of the loss. We also provide a learning rate to use as a scale factor while updating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sV96wO_Ip6Z"
      },
      "source": [
        "# training op\n",
        "train = tf.train.GradientDescentOptimizer(0.001).minimize(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWsTqaYhJUKY"
      },
      "source": [
        "# create some random values for training set and label and feed them into the model during running the code.\n",
        "x = [1.1,2.0,3.5,4.8]\n",
        "y = [2.0,3.4,4.2,5.1]\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for train_step in range(2000):\n",
        "    sess.run(train,{X:x,Y:y})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RODZ2LF9JzWn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2f779478-a514-461a-9675-6a4ce3eb809d"
      },
      "source": [
        "# Execute the model to see the final results\n",
        "weight, bias, loss = sess.run([W, b, loss], {X:x, Y:y})\n",
        "print(\"W: %s b: %s loss: %s\"%(weight,bias,loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W: [0.8422483] b: [1.2378168] loss: 0.2880003\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}