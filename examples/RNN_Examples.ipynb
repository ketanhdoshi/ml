{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Examples.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "37-DQ4MILfcH",
        "uLa8KrHhC0uG",
        "hJymslRyDJ0Z",
        "fhj9PN_-Vc4j",
        "CfXIKU5v167A"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ketanhdoshi/ml/blob/master/examples/RNN_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37-DQ4MILfcH"
      },
      "source": [
        "### Basic RNN example built from scratch using Tensorflow - [here](https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767)\n",
        "\n",
        "A simple Echo-RNN that remembers the input data and then echoes it after a few time-steps\n",
        "\n",
        "The input data-matrix, and the current batch batchX_placeholder is in the dashed rectangle. This “batch window” is slided truncated_backprop_length steps to the right at each run, hence the arrow. In our example below batch_size = 3, truncated_backprop_length = 3, and total_series_length = 36. Note that these numbers are just for visualization purposes, the values are different in the code.\n",
        "\n",
        "The dark squares show '1' values and the light squares show '0' values\n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*n45uYnAfTDrBvG87J-poCA.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7Ha6EcNLSjI"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_epochs = 100\n",
        "total_series_length = 50000\n",
        "truncated_backprop_length = 15 # Number of timesteps per sample\n",
        "state_size = 4\n",
        "num_classes = 2 # Number of output classes\n",
        "echo_step = 3  # Number of steps by which to shift the input data for echoing\n",
        "batch_size = 5 # Number of training samples per batch\n",
        "num_batches = total_series_length//batch_size//truncated_backprop_length\n",
        "\n",
        "def generateData():\n",
        "    # Populate 1D array randomly with values either '0' or '1', each with a\n",
        "    # probability of 50%\n",
        "    x = np.array(np.random.choice(2, total_series_length, p=[0.5, 0.5]))\n",
        "    \n",
        "    # Shift values of 'x' rightwards by 'echo_step' steps, and set the\n",
        "    # newly shifted-in values to 0. This is the label (ie. expected output)\n",
        "    y = np.roll(x, echo_step)\n",
        "    y[0:echo_step] = 0\n",
        "\n",
        "    # Reshape 'x' and 'y' to 2D with 'batch_size' rows\n",
        "    x = x.reshape((batch_size, -1))  # The first index changing slowest, subseries as rows\n",
        "    y = y.reshape((batch_size, -1))\n",
        "\n",
        "    return (x, y)\n",
        "\n",
        "# Initialise the X and Y as placeholders for feeding the input and label data\n",
        "batchX_placeholder = tf.placeholder(tf.float32, [batch_size, truncated_backprop_length])\n",
        "batchY_placeholder = tf.placeholder(tf.int32, [batch_size, truncated_backprop_length])\n",
        "\n",
        "# Initialise the activation state\n",
        "init_state = tf.placeholder(tf.float32, [batch_size, state_size])\n",
        "\n",
        "# Initialise the weights and biases as Variables to be learned\n",
        "# This is [W_aa W_ax] and b_a. The 'state_size + 1' in the weight dimension is because we\n",
        "# are concatenating W_aa and W_ax\n",
        "W = tf.Variable(np.random.rand(state_size+1, state_size), dtype=tf.float32)\n",
        "b = tf.Variable(np.zeros((1,state_size)), dtype=tf.float32)\n",
        "\n",
        "# This is W_ay and b_y\n",
        "W2 = tf.Variable(np.random.rand(state_size, num_classes),dtype=tf.float32)\n",
        "b2 = tf.Variable(np.zeros((1,num_classes)), dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y6OnNNUVGOt"
      },
      "source": [
        "**Unstack** the columns (axis = 1) of the batch into a Python list. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example.\n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*f2iL4zOkBUBGOpVE7kyajg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk5w02rMVNwV"
      },
      "source": [
        "# Unstack columns - split the batch data into adjacent time-steps\n",
        "# Each of the two variables below is a list that represents a time-series with multiple entries at each step.\n",
        "inputs_series = tf.unstack(batchX_placeholder, axis=1)\n",
        "labels_series = tf.unstack(batchY_placeholder, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-wDqJYJn6Nm"
      },
      "source": [
        "**Forward pass calculation**\n",
        "\n",
        "Calculate the sum of two affine transforms current_input * Wa + current_state * Wb in the figure below. By concatenating those two tensors you will only use one matrix multiplication. The addition of the bias b is broadcasted on all samples in the batch\n",
        "\n",
        "![alt text](https://miro.medium.com/max/700/1*fdwNNJ5UOE3Sx0R_Cyfmyg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1EoRjFlWSUD"
      },
      "source": [
        "# Forward pass to do the actual RNN computation\n",
        "current_state = init_state\n",
        "states_series = []\n",
        "for current_input in inputs_series:\n",
        "    current_input = tf.reshape(current_input, [batch_size, 1])\n",
        "    input_and_state_concatenated = tf.concat([current_input, current_state], 1)  # Increasing number of columns\n",
        "\n",
        "    next_state = tf.tanh(tf.matmul(input_and_state_concatenated, W) + b)  # Broadcasted addition\n",
        "    states_series.append(next_state)\n",
        "    current_state = next_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1uP9L9wp10Z"
      },
      "source": [
        "# Calculate loss for the batch, after a fully connected softmax layer from \n",
        "# the state to the output which makes the classes one-hot encoded\n",
        "logits_series = [tf.matmul(state, W2) + b2 for state in states_series] #Broadcasted addition\n",
        "predictions_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
        "\n",
        "# logits is of shape [batch_size, num_classes] and labels of shape [batch_size]\n",
        "losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series,labels_series)]\n",
        "total_loss = tf.reduce_mean(losses)\n",
        "\n",
        "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVnhB1yNsFF-"
      },
      "source": [
        "**Visualize** the training - plot the loss over time, show training input, training output and the current predictions by the network on different sample series in a training batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwAbVRGnr_uz"
      },
      "source": [
        "def plot(loss_list, predictions_series, batchX, batchY):\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.cla()\n",
        "    plt.plot(loss_list)\n",
        "\n",
        "    for batch_series_idx in range(5):\n",
        "        one_hot_output_series = np.array(predictions_series)[:, batch_series_idx, :]\n",
        "        single_output_series = np.array([(1 if out[0] < 0.5 else 0) for out in one_hot_output_series])\n",
        "\n",
        "        plt.subplot(2, 3, batch_series_idx + 2)\n",
        "        plt.cla()\n",
        "        plt.axis([0, truncated_backprop_length, 0, 2])\n",
        "        left_offset = range(truncated_backprop_length)\n",
        "        plt.bar(left_offset, batchX[batch_series_idx, :], width=1, color=\"blue\")\n",
        "        plt.bar(left_offset, batchY[batch_series_idx, :] * 0.5, width=1, color=\"red\")\n",
        "        plt.bar(left_offset, single_output_series * 0.3, width=1, color=\"green\")\n",
        "\n",
        "    plt.draw()\n",
        "    plt.pause(0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw2lXyb2scl2"
      },
      "source": [
        "**Run the training** by executing the TensorFlow graph in a session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Kz-iegsa9m"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.initialize_all_variables())\n",
        "    plt.ion()\n",
        "    plt.figure()\n",
        "    plt.show()\n",
        "    loss_list = []\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        # New data is generated on each epoch (not the usual way to do it, but \n",
        "        # it works in this case since everything is predictable\n",
        "        x,y = generateData()\n",
        "        _current_state = np.zeros((batch_size, state_size))\n",
        "\n",
        "        print(\"New data, epoch\", epoch_idx)\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * truncated_backprop_length\n",
        "            end_idx = start_idx + truncated_backprop_length\n",
        "\n",
        "            batchX = x[:,start_idx:end_idx]\n",
        "            batchY = y[:,start_idx:end_idx]\n",
        "\n",
        "            _total_loss, _train_step, _current_state, _predictions_series = sess.run(\n",
        "                [total_loss, train_step, current_state, predictions_series],\n",
        "                feed_dict={\n",
        "                    batchX_placeholder:batchX,\n",
        "                    batchY_placeholder:batchY,\n",
        "                    init_state:_current_state\n",
        "                })\n",
        "\n",
        "            loss_list.append(_total_loss)\n",
        "\n",
        "            if batch_idx%100 == 0:\n",
        "                print(\"Step\",batch_idx, \"Loss\", _total_loss)\n",
        "                plot(loss_list, _predictions_series, batchX, batchY)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv3PqOgoCzJy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLa8KrHhC0uG"
      },
      "source": [
        "### Modify basic RNN Example using the terminology and logic in Andrew Ng [assignment](https://github.com/Kulbear/deep-learning-coursera/blob/master/Sequence%20Models/Building%20a%20Recurrent%20Neural%20Network%20-%20Step%20by%20Step%20-%20v2.ipynb)\n",
        "\n",
        "This is a similar RNN as the example above. It has been re-written so that it is easier to understand and uses the terminology from Andrew Ng's RNN assignment. Also, generated data is re-organised in a more intuitive way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2YziIaWx0lj"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-------------------------\n",
        "# Input/Output Data parameters\n",
        "#-------------------------\n",
        "m = 30 # Number of training samples per batch\n",
        "num_batches = 120\n",
        "num_epochs = 80 # Number of training epochs\n",
        "echo_steps = 3 # Number of steps by which to shift the input data for echoing\n",
        "\n",
        "#-------------------------\n",
        "# RNN cell parameters\n",
        "#-------------------------\n",
        "n_x = 1 # Number of input features\n",
        "n_a = 4 # Number of activation state features\n",
        "n_y = 2 # Number of output features\n",
        "T_x = 15 # Number of time steps\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "# Generate input and output data in the right format for the problem\n",
        "# The input data consists of a single long sequence of 0s and 1s. The\n",
        "# expected output data echoes the same sequence but shifted by 'echo_steps'\n",
        "#\n",
        "# Total number of rows = 'm' * num_batches\n",
        "# Each row has values in 'T_x' time-steps\n",
        "# Each value in the input data has 'n_x' features\n",
        "#------------------------------------------------------------------\n",
        "def generate_data (n_x, T_x, m, num_batches, echo_steps):\n",
        "  seq_length = num_batches * m * T_x\n",
        "  \n",
        "  # Populate 1D array randomly with values either '0' or '1', each with a\n",
        "  # probability of 50%\n",
        "  x = np.array(np.random.choice(2, seq_length, p=[0.5, 0.5]))\n",
        "    \n",
        "  # Shift values of 'x' rightwards by 'echo_step' steps, and set the\n",
        "  # newly shifted-in values to 0. This is the label (ie. expected output)\n",
        "  y = np.roll(x, echo_steps)\n",
        "  y[0:echo_steps] = 0\n",
        "\n",
        "  # Reshape 'x' to 3D with 'n_x' columns and 'T_x' depth. Number of rows will be\n",
        "  # 'm' * num_batches\n",
        "  x = x.reshape((-1, n_x, T_x))  # The first index changing slowest, subseries as rows\n",
        "  \n",
        "  # Reshape 'y' to 2D\n",
        "  y = y.reshape((-1, T_x))\n",
        "  \n",
        "  return (x, y)\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "# RNN class\n",
        "#------------------------------------------------------------------\n",
        "class RNN(object):\n",
        "    def __init__(self, m, n_a, n_x, n_y, T_x):\n",
        "      self.m = m\n",
        "      self.n_a = n_a\n",
        "      self.n_x = n_x\n",
        "      self.n_y = n_y\n",
        "      self.T_x = T_x\n",
        "\n",
        "      # Initialise the weights and biases as Variables to be learned\n",
        "      # Note the shapes of each of these\n",
        "      self.W_aa = tf.Variable(np.random.rand(n_a, n_a), dtype=tf.float32)\n",
        "      self.b_a = tf.Variable(np.zeros((1, n_a)), dtype=tf.float32)\n",
        "  \n",
        "      self.W_ax = tf.Variable(np.random.rand(n_x, n_a), dtype=tf.float32)\n",
        "  \n",
        "      self.W_ay = tf.Variable(np.random.rand(n_a, n_y),dtype=tf.float32)\n",
        "      self.b_y = tf.Variable(np.zeros((1, n_y)), dtype=tf.float32)\n",
        "  \n",
        "      # Initialise the X and Y as placeholders for feeding the input and label data\n",
        "      self.batchX = tf.placeholder(tf.float32, [m, n_x, T_x])\n",
        "      self.batchY = tf.placeholder(tf.int32, [m, T_x])\n",
        "\n",
        "      # Initialise the activation state\n",
        "      self.a_init = tf.placeholder(tf.float32, [m, n_a])\n",
        "\n",
        "    #------------------------------------------------------------------\n",
        "    # A single RNN cell\n",
        "    #------------------------------------------------------------------\n",
        "    def _cell (self, a_prev, X_t):\n",
        "      \n",
        "      a_next = tf.tanh(tf.matmul(X_t, self.W_ax) + tf.matmul(a_prev, self.W_aa) + self.b_a)\n",
        "      logit = tf.matmul(a_next, self.W_ay) + self.b_y\n",
        "      y_pred = tf.nn.softmax(logit)\n",
        "      \n",
        "      return (a_next, logit, y_pred)\n",
        "      \n",
        "    #------------------------------------------------------------------\n",
        "    # A complete forward-pass by unrolling the RNN cell by the required\n",
        "    # number of time-steps\n",
        "    #------------------------------------------------------------------\n",
        "    def _forward (self):\n",
        "      # Unstack columns - split the batch data into adjacent time-steps\n",
        "      # Each of the two variables below is a list that represents a time-series with multiple entries at each step.\n",
        "      X_list = tf.unstack(self.batchX, axis=2) # Axis 2 is the time-step (ie. T_x) axis\n",
        "      Y_list = tf.unstack(self.batchY, axis=1) # Axis 1 is the time-step (ie. T_x) axis\n",
        "      \n",
        "      # Initialise the activation state\n",
        "      a_prev = self.a_init\n",
        "      logit_list = []\n",
        "\n",
        "      # Loop through each time-step\n",
        "      for X_t in X_list:\n",
        "        a_next, logit, y_pred = self._cell (a_prev, X_t)\n",
        "        a_prev = a_next\n",
        "        \n",
        "        # List of logits from each step\n",
        "        logit_list.append (logit)\n",
        "          \n",
        "      # logits is of shape [batch_size, num_classes] and labels of shape [batch_size]\n",
        "      losses = [tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logit_list, Y_list)]\n",
        "      self.total_loss = tf.reduce_mean(losses)\n",
        "      self.train_step = tf.train.AdagradOptimizer(0.3).minimize(self.total_loss)\n",
        "\n",
        "    #------------------------------------\n",
        "    # Build the RNN\n",
        "    #------------------------------------\n",
        "    def build (self):\n",
        "      self._forward ()\n",
        "    \n",
        "    #------------------------------------\n",
        "    # Train the RNN\n",
        "    #--------------------------\n",
        "    def train(self, x, y, num_epochs):\n",
        "      with tf.Session() as sess:\n",
        "        tf.global_variables_initializer().run()\n",
        "        print('Initialized')\n",
        "        loss_list = []\n",
        "        \n",
        "        # Loop through each epoch\n",
        "        for epoch_idx in range(num_epochs):\n",
        "          a_0 = np.zeros((self.m, self.n_a))\n",
        "\n",
        "          print()\n",
        "\n",
        "          for batch_idx in range(num_batches):\n",
        "            \n",
        "            # Select 'm' rows from the data at a time\n",
        "            start_idx = batch_idx * m\n",
        "            end_idx = start_idx + m\n",
        "\n",
        "            batchX = x[start_idx:end_idx, :, :]\n",
        "            batchY = y[start_idx:end_idx, :]\n",
        "            #print ('Epoch ', epoch_idx, ', Batch', batch_idx)\n",
        "\n",
        "            _total_loss, _train_step = sess.run(\n",
        "                [self.total_loss, self.train_step],\n",
        "                feed_dict={\n",
        "                    self.batchX:batchX,\n",
        "                    self.batchY:batchY,\n",
        "                    self.a_init:a_0\n",
        "                })\n",
        "            \n",
        "            loss_list.append(_total_loss)\n",
        "\n",
        "            if batch_idx % 25 == 0:\n",
        "                print('Epoch ', epoch_idx, 'Batch', batch_idx, \"Loss\", _total_loss)\n",
        "                \n",
        "model = RNN (m, n_a, n_x, n_y, T_x)\n",
        "model.build()\n",
        "\n",
        "# NB: Alternately we can generate the data before the RNN, then pass the\n",
        "#'x' and 'y'to the RNN constructor. The constructor can check the shape of 'x'\n",
        "# and 'y' and create the RNN accordingly\n",
        "x,y = generate_data(n_x, T_x, m, num_batches, echo_steps)\n",
        "model.train(x, y, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJymslRyDJ0Z"
      },
      "source": [
        "### Modify Example to use static_rnn()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duoAlGfMFeD4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8WagPRBDgER"
      },
      "source": [
        "### Upgrade the above example to use dynamic_rnn(). Use it for a time-series prediction - Geron pg 643"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCM6tF29EY7f"
      },
      "source": [
        "### Optionally experiment with Variable Input Length - Geron pg 635"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLXRpwk4HKUy"
      },
      "source": [
        "### Optionally use a LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QmdD5nUHV6c"
      },
      "source": [
        "### Use Keras with deep multi-layered LSTM with dropout for a real-world application"
      ]
    }
  ]
}