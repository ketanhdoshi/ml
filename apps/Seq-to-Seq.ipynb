{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Seq-to-Seq.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"CfXIKU5v167A","colab_type":"text"},"source":["### Character-level Sequence-to-Sequence for Machine Translation - [Keras tutorial](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)\n","\n","implement a basic character-level sequence-to-sequence model to translate short English sentences into short French sentences, character-by-character. Note that it is fairly unusual to do character-level, rather than word-level machine translation."]},{"cell_type":"code","metadata":{"id":"ioXSmJB12Ccz","colab_type":"code","outputId":"ba987438-9b2d-4d17-e6f9-919b1d90aa02","executionInfo":{"status":"ok","timestamp":1563509014854,"user_tz":-330,"elapsed":83468,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["# Use a dataset of pairs of English sentences and their French translation\n","\n","!wget http://www.manythings.org/anki/fra-eng.zip\n","!unzip fra-eng.zip -d fra-eng\n","!more fra-eng/fra.txt"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2019-07-19 04:02:11--  http://www.manythings.org/anki/fra-eng.zip\n","Resolving www.manythings.org (www.manythings.org)... 104.24.109.196, 104.24.108.196, 2606:4700:30::6818:6dc4, ...\n","Connecting to www.manythings.org (www.manythings.org)|104.24.109.196|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3467257 (3.3M) [application/zip]\n","Saving to: ‘fra-eng.zip’\n","\n","fra-eng.zip         100%[===================>]   3.31M  2.15MB/s    in 1.5s    \n","\n","2019-07-19 04:02:13 (2.15 MB/s) - ‘fra-eng.zip’ saved [3467257/3467257]\n","\n","Archive:  fra-eng.zip\n","  inflating: fra-eng/_about.txt      \n","  inflating: fra-eng/fra.txt         \n","Go.\tVa !\n","Hi.\tSalut !\n","Hi.\tSalut.\n","Run!\tCours !\n","Run!\tCourez !\n","Who?\tQui ?\n","Wow!\tÇa alors !\n","Fire!\tAu feu !\n","Help!\tÀ l'aide !\n","Jump.\tSaute.\n","Stop!\tÇa suffit !\n","Stop!\tStop !\n","Stop!\tArrête-toi !\n","Wait!\tAttends !\n","Wait!\tAttendez !\n","Go on.\tPoursuis.\n","Go on.\tContinuez.\n","Go on.\tPoursuivez.\n","Hello!\tBonjour !\n","Hello!\tSalut !\n","I see.\tJe comprends.\n","I try.\tJ'essaye.\n","I won!\tJ'ai gagné !\n","\u001b[K"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HgmwigA45aCS","colab_type":"text"},"source":["A few samples from the dataset\n","\n","Stop!\tArrête-toi !\n","\n","Wait!\tAttends !\n","\n","Wait!\tAttendez !\n","\n","Go on.\tPoursuis.\n","\n","Go on.\tContinuez.\n","\n","Go on.\tPoursuivez."]},{"cell_type":"code","metadata":{"id":"p3n1IIBX30HC","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","import numpy as np\n","\n","batch_size = 64  # Batch size for training.\n","epochs = 100  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","data_path = 'fra-eng/fra.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSDZvruCyeRa","colab_type":"text"},"source":["**Training the model**\n","\n","The model has two parts:\n","\n","\n","1.   A RNN layer acts as \"encoder\": it processes the input sequence and returns its own internal state.\n","2.   Another RNN layer acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep. Importantly, the encoder uses as initial state the state vectors from the encoder\n","\n","![alt text](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n","\n","To prepare the training data, we read sentences from the data file and convert them into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n","\n","1.   encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences\n","2.   decoder_input_data is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n","3.   decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :].\n"]},{"cell_type":"code","metadata":{"id":"xtb8b7W_9GlV","colab_type":"code","outputId":"7bbb6fb3-d5c3-42a3-8415-93eb62ce961d","executionInfo":{"status":"ok","timestamp":1563516083689,"user_tz":-330,"elapsed":2258,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["#----------------------------------------------\n","# Convert a string to an array of characters\n","#----------------------------------------------\n","def word2char (line):\n","   char = [c for c in line]\n","   return char \n","\n","#----------------------------------------------\n","# Given an array of sentences, find the length of the longest sentence\n","#----------------------------------------------\n","def max_sentence_length (char_sentences):\n","  max_sentence_length = 0\n","  for char_sentence in char_sentences:\n","    max_sentence_length = max (max_sentence_length, len (char_sentence))\n","  return (max_sentence_length)    \n","\n","#----------------------------------------------\n","# Read num_samples lines from the data file, and return three lists of \n","# sentences. Each sentence is an array of characters.\n","#\n","# eng_char_sentences = [['G', 'o'], ['H', 'i', '.'], ...]\n","# fr_char_sentences_shift = [[V', 'a', '!'], ['S', 'a', 'l', 'u', 't', '!'], ...]\n","# fr_char_sentences = [['\\t', 'V', 'a', '!'], ['\\t', 'S', 'a', 'l', 'u', 't', '!'], ...]\n","#\n","# The 'eng_char_sentences' are the original English sentences and will be input to the Encoder\n","# The 'fr_char_sentences_shift' are the original French sentences and will be the target output of the Decoder\n","# The 'fr_char_sentences' are the original French sentences with a Tab('\\t') start token and will be the input to the Decoder\n","#----------------------------------------------\n","def read_sentences (data_path, num_samples):\n","  eng_char_sentences = []\n","  fr_char_sentences = []\n","  fr_char_sentences_shift = []\n","  \n","  with open(data_path, \"r\") as fp:\n","    # Read the file line by line\n","    line = fp.readline()\n","  \n","    # We will read up to num_samples lines\n","    samples = 0\n","    while line and (samples < num_samples):\n","      # Split the line as <eng_sentence>TAB<fr_sentence>\n","      eng_sentence,fr_sentence = line.split('\\t')\n","      fr_sentence_shift = fr_sentence\n","      # We use \"tab\" as the \"start sequence\" character\n","      fr_sentence = '\\t' + fr_sentence\n","    \n","      # Convert the sentence from a string to an array of characters\n","      eng_char_sentence = word2char (eng_sentence)\n","      fr_char_sentence = word2char (fr_sentence)\n","      fr_char_sentence_shift = word2char (fr_sentence_shift)\n","    \n","      # Add to the array of sentences, each sentence in turn being an array of characters\n","      eng_char_sentences.append (eng_char_sentence)\n","      fr_char_sentences.append (fr_char_sentence)\n","      fr_char_sentences_shift.append (fr_char_sentence_shift)\n","      \n","      # Increment samples read so far, and then read the next line\n","      samples = samples + 1\n","      line = fp.readline()\n","  \n","  return (eng_char_sentences, fr_char_sentences, fr_char_sentences_shift)\n","\n","# Read sentences from the data file\n","eng_char_sentences, fr_char_sentences, fr_char_sentences_shift = read_sentences (data_path, num_samples)\n","\n","# Get the length of the longest sentence\n","max_eng_sentence_length = max_sentence_length (eng_char_sentences)\n","max_fr_sentence_length = max_sentence_length (fr_char_sentences)\n","\n","print (len (eng_char_sentences), len (fr_char_sentences))\n","print (eng_char_sentences[10], fr_char_sentences[10], fr_char_sentences_shift[10])\n","print (eng_char_sentences[75], fr_char_sentences[75], fr_char_sentences_shift[75])\n","print (max_eng_sentence_length, max_fr_sentence_length)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["10000 10000\n","['S', 't', 'o', 'p', '!'] ['\\t', 'Ç', 'a', ' ', 's', 'u', 'f', 'f', 'i', 't', '\\u202f', '!', '\\n'] ['Ç', 'a', ' ', 's', 'u', 'f', 'f', 'i', 't', '\\u202f', '!', '\\n']\n","['A', 'w', 'e', 's', 'o', 'm', 'e', '!'] ['\\t', 'F', 'a', 'n', 't', 'a', 's', 't', 'i', 'q', 'u', 'e', '\\u202f', '!', '\\n'] ['F', 'a', 'n', 't', 'a', 's', 't', 'i', 'q', 'u', 'e', '\\u202f', '!', '\\n']\n","16 59\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3w_hjlg6Hd35","colab_type":"code","outputId":"3ae316a9-d812-44b0-e8c7-514fdce5009b","executionInfo":{"status":"ok","timestamp":1563516424480,"user_tz":-330,"elapsed":2055,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#----------------------------------------------\n","# Create the vocab as a set of all the characters in the corpus of sentences\n","#----------------------------------------------\n","def vocab (char_sentences):\n","  # Single flat list of all the characters from all sentences\n","  merged_chars = [char for char_sentence in char_sentences for char in char_sentence]\n","  \n","  # Create a set from the list, so that it contains only unique characters\n","  vocab_chars = sorted (set (merged_chars))\n","  vocab_size = len (vocab_chars)\n","  return (vocab_chars, vocab_size)\n","\n","# Get the set of characters in the vocab\n","eng_vocab_chars, eng_vocab_size = vocab (eng_char_sentences)\n","fr_vocab_chars, fr_vocab_size = vocab (fr_char_sentences)\n","\n","eng_vocab_size, fr_vocab_size"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70, 93)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"zCBUQme8JWd3","colab_type":"code","outputId":"a95de14c-42f0-4e65-d250-3fc805860972","executionInfo":{"status":"ok","timestamp":1563516455825,"user_tz":-330,"elapsed":1841,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["#----------------------------------------------\n","# Dictionary to map from char to char_index in the vocab\n","#----------------------------------------------\n","eng_vocab_dict = {c:i for i, c in enumerate (eng_vocab_chars)}\n","fr_vocab_dict = {c:i for i, c in enumerate (fr_vocab_chars)}\n","\n","#----------------------------------------------\n","# Char -> index utility function\n","#----------------------------------------------\n","def char2idx (vocab_dict, c):\n","  return (vocab_dict [c])\n","\n","#----------------------------------------------\n","# Index -> char utility function\n","#----------------------------------------------\n","def idx2char (vocab_chars, i):\n","  return (vocab_chars[i])\n","  \n","print (char2idx (eng_vocab_dict, 'G'))\n","print (idx2char (eng_vocab_chars, 7))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["26\n","-\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s_2LwRv6LntS","colab_type":"code","colab":{}},"source":["#----------------------------------------------\n","# The training data consists of three array, each of which is a 3D array of \n","# shape (num of samples, num of timesteps, num of input values per timestep)\n","#\n","# Each sample is one sentence\n","#\n","# The input at each timestep will be a single character from a sentence. Since we will have a fixed \n","# number of timesteps, it will be taken as the maximum number of characters of any sentence\n","#\n","# Since each character will be one-hot encoded from the vocab, the number of input values per timestep\n","# will be the size of the vocab\n","#----------------------------------------------\n","\n","# Create the training data of the right shape, and initialise all values to 0\n","encoder_input_data = np.zeros((num_samples, max_eng_sentence_length, eng_vocab_size), dtype='float32')\n","decoder_input_data = np.zeros((num_samples, max_fr_sentence_length, fr_vocab_size), dtype='float32')\n","decoder_target_data = np.zeros((num_samples, max_fr_sentence_length, fr_vocab_size), dtype='float32')\n","\n","#----------------------------------------------\n","# Populate the training data 3D array\n","# We go through each sample (ie. first dimension) and each character in that sample (ie. second dimension)\n","# And for that character we convert it to its index, and then one-hot encode it (third dimension)\n","#----------------------------------------------\n","def char_indices (data, char_sentences, vocab_dict):\n","  # Loop through each sentence (ie. each sample)\n","  for i, char_sentence in enumerate (char_sentences):\n","    \n","    # Loop through each character in each sentence\n","    for j, char in enumerate (char_sentence):\n","      \n","      # Get the character index for the character\n","      idx = char2idx (vocab_dict, char)\n","      \n","      # To one-hot encode the character, set the array element for that index to 1\n","      data [i, j, idx] = 1.0\n","\n","# Prepare all three arrays for training\n","char_indices (encoder_input_data, eng_char_sentences, eng_vocab_dict)\n","char_indices (decoder_input_data, fr_char_sentences, fr_vocab_dict)\n","char_indices (decoder_target_data, fr_char_sentences_shift, fr_vocab_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bb1RbgEb1As","colab_type":"code","colab":{}},"source":["#----------------------------------------------\n","# Now that the training data has been prepared, we will build the a LSTM Encoder-Decoder model and\n","# train it to predict decoder_target_data given encoder_input_data and decoder_input_data.\n","#\n","# The training process and inference process (decoding sentences) are quite different, so we will \n","# use different models for both. However both models will use the same inner layers\n","#\n","# We have one Embedding Layer and one Encoder LSTM layer\n","# We have another Embedding Layer, one Decoder LSTM layer and a Dense layer for predictions\n","# We build the model using Keras Functional API\n","#----------------------------------------------\n","\n","num_encoder_tokens = eng_vocab_size\n","num_decoder_tokens = fr_vocab_size\n","\n","#----------------------------------------------\n","# Build the Encoder - it is a LSTM layer that takes the encoder_input_data as input\n","# We discard its output and keep the states, so that they can be passed to the Decoder\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","# return_state=True tells the RNN layer to return a list where the first entry is the \n","# outputs and the next entries are the internal RNN states\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","#----------------------------------------------\n","\n","#----------------------------------------------\n","# Build the Decoder - it is a LSTM layer that takes the decoder_input_data as input\n","# and uses the `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# return_sequences=True tells the RNN layer to return its full sequence of outputs (instead \n","# of just the last output, which the defaults behavior). We also return the internal\n","# states. We don't use those states in the training but we will use them during inference\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","# Pass the decoder outputs through a Dense layer with Softmax to get the predictions\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","#----------------------------------------------\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# Run training\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_split=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RTzJDkkHcil","colab_type":"text"},"source":["**Doing Inference**\n","\n","To decode a test sentence, we will repeatedly:\n","\n","1.  Encode the input sequence into state vectors.\n","2.  Start with a target sequence of size 1 (just the start-of-sequence character).\n","3.  Feed the state vectors and 1-char target sequence to the decoder and run one step of the decoder to produce predictions for the next character.\n","4.  Sample the next character using these predictions (we simply use argmax).\n","5.  Append the sampled character to the target sequence\n","6.  Repeat until we generate the end-of-sequence character or we hit the character limit.\n","\n","![alt text](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)"]},{"cell_type":"code","metadata":{"id":"dr4XkIgypbDZ","colab_type":"code","colab":{}},"source":["#----------------------------------------------\n","# We will build the Inference model using the Encoder-Decoder layers we created\n","# earlier. \n","#----------------------------------------------\n","\n","# This is simply to change some variable names\n","max_encoder_seq_length = max_eng_sentence_length\n","max_decoder_seq_length = max_fr_sentence_length\n","input_token_index = eng_vocab_dict\n","target_token_index = fr_vocab_dict\n","input_texts = eng_char_sentences\n","\n","# ------------------------------------------\n","# Build the Encoder model, which returns the encoder states\n","encoder_model = Model(encoder_inputs, encoder_states)\n","# ------------------------------------------\n","\n","# ------------------------------------------\n","# The Decoder will work in a loop. For its first iteration, its input sequence contains only\n","# the start token. And its initial state is the Encoder state. It then generates the\n","# next character output along with its own internal state.\n","# \n","# Now for the next iteration, it appends the next character to the input sequence\n","# and uses that as input. It also takes its own internal state and feeds that back\n","# to itself as the initial state for the next iteration\n","#\n","# Hence, when building the model, we define the Decoder Initial State as an input\n","# variable (rather than using the Encoder state directly since we will take the Encoder state \n","# only for the first time)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","# Use the Decoder LSTM layer created during training, but with different inputs\n","# and initial states\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","# Use the Dense layer created during training to generate predictions from the\n","# Decoder's outputs\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)\n","# ------------------------------------------\n","\n","# Try to understand shapes\n","print (encoder_inputs, '\\n', encoder_outputs, '\\n', state_h, '\\n', state_c)\n","print (decoder_inputs, '\\n', decoder_outputs, '\\n', decoder_states_inputs)\n","\n","# Reverse-lookup token index to decode sequences back to characters\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n","\n","# ------------------------------------------\n","# Implements the Inference process. \n","# ------------------------------------------\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","        \n","        #print ('Target seq = ', target_seq.shape)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # KD - the original logic was to always use a target sequence\n","        # of length 1, which we update with the last sampled token. We\n","        # also feed the states_value back as the initial_state for the\n","        # next iteration\n","        #\n","        # The alternate logic which I tried was to keep extending the target\n","        # sequence in each iteration, so that the last sampled token is appended\n","        # to the previous target sequence. If we do that we don't change the\n","        # initial_state value at all, and continue to use the earlier value\n","        #\n","        # Both alternates are giving the same results. But I should experiment\n","        # some more, try out other unseen input sequences and see how both\n","        # alternates perform\n","        kdAlternate = True\n","        if (kdAlternate):\n","          b = np.zeros((1, target_seq.shape[1] + 1, num_decoder_tokens))\n","          b[:,:-1] = target_seq\n","          b[0, -1, sampled_token_index] = 1.\n","          target_seq = b\n","        else:\n","          # Update the target sequence (of length 1).\n","          target_seq = np.zeros((1, 1, num_decoder_tokens))\n","          target_seq[0, 0, sampled_token_index] = 1.\n","\n","          # Update states\n","          states_value = [h, c]\n","\n","    return decoded_sentence\n","\n","\n","for seq_index in range(20):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"],"execution_count":0,"outputs":[]}]}