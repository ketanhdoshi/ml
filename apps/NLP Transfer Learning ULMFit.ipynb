{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP Transfer Learning ULMFit.ipynb","provenance":[],"collapsed_sections":["nHy1RZle4smK","h1Ruc7yI5XP8","oDqvMEF25fhd","Q5ahVATo09xU","TVaxtq_tLJCl","B9BhlQTALhHL","00D26zxcoo4Q","OoNv-USiMrju","vqfv0q3l9tE3","Qp8GLART4ztV","2JWNDxOGWR2n","rg3Ke1P2WxZh","9gU9HvOEKNuX","xTWaPxzB1cnc","-r4-NwBaBj1A","SxvVA2WJhcbW","tlUoEmuhkeTN","ywGHyIlY9NoV"],"authorship_tag":"ABX9TyNGXNmFqdlO78QQgSDCiTLU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"nHy1RZle4smK","colab_type":"text"},"source":["###NLP Transfer Learning using ULMFit"]},{"cell_type":"markdown","metadata":{"id":"TO90F6LPBy88","colab_type":"text"},"source":["Universal Language Model Fine-Tuning(ULMFit) is a transfer learning technique for NLP tasks. It is a state-of-the-art NLP technique along with alternatives such as BERT and XLNet (for text classification)\n","\n","We use transfer learning to do text classification of IMDB movie reviews, using a language model pre-trained on Wikitext.\n","\n","![alt text](https://nlp.fast.ai/images/ulmfit_approach.png)\n","\n","The code here is based on the following four notebooks in Fastai's Course V3 Part 2 - [text pre-process](https://github.com/fastai/course-v3/blob/master/nbs/dl2/12_text.ipynb), [awd-lstm](https://github.com/fastai/course-v3/blob/master/nbs/dl2/12a_awd_lstm.ipynb), [LM pre-train](https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb) and [ULMFit](https://github.com/fastai/course-v3/blob/master/nbs/dl2/12c_ulmfit.ipynb). The logic for text pre-processing has been implemented in the data_lib and imported into this notebook.\n","\n","NB: This is intended to be a Clean Updated version of the notebook KD AWD LSTM which had become very messy and intertwined."]},{"cell_type":"markdown","metadata":{"id":"p7haWo1RuInh","colab_type":"text"},"source":["**ULMFit**\n","\n","Note that our transfer learning is transforming in two different dimensions:\n","1. Dataset from wikitext corpus to IMDB movie reviews\n","2. Problem from language model (ie. next word prediction) to text classification\n","\n","Hence, the overall process we follow is as shown below:\n","\n","![alt text](https://miro.medium.com/max/1413/1*stYzRq07Blajrg2l6gw9Aw.png)\n","\n","1. Load text data for IMDB and Wikitext and pre-process it for Language Modeling\n","2. Build our AWD-LSTM module which will serve as the core module for both our Language Model and Text Classification architecture.  \n","3. Build our Language Model architecture using AWD-LSTM and a Linear Decoder\n","4. Train our Language Model using Wikitext data\n","5. Adapt the data of the pre-trained Wikitext Language Model to IMDB - transform the vocab and embeddings\n","6. Adapt the weights of the pre-trained Wikitext Language Model to IMDB - retrain on IMDB\n","7. Load text data for IMDB and pre-process it for Classification\n","8. Build our Classification architecture using AWD-LSTM and a Pooling Classifier\n","9. Train our Classification using IMDB data"]},{"cell_type":"markdown","metadata":{"id":"233jN_MC1KLk","colab_type":"text"},"source":["**Todos**\n","*   Add DataBundle for data instead of Fastai data\n","*   Track results in DTR\n","*   DTR Hyperparameters tracking\n","*   Trigger the shuffle of the LMDataset at the beginning of each epoch. Should be done inside AWD-LSTM Callback\n","*   Add comments as required\n","*   Cleanup cells with Fastai code\n","*   Run for epochs and check results. Compare data/results with Fastai\n","*   Make sure App and Arch are clean and as per standard template\n","*   Display batch and results\n","*   Train the model with Wikitext data and save the data and the model\n","*   Check all the parameters at each step with what is used in the Fastai lesson eg. lr, bs, bptt, emb_sz, nh, nl, dps, scheduling rates, gradient clipping, optimiser hyper params, AWD-LSTM CB params, num_epochs"]},{"cell_type":"markdown","metadata":{"id":"h1Ruc7yI5XP8","colab_type":"text"},"source":["### Import Fastai Lesson code - this is needed only to load the data (from IMDB)"]},{"cell_type":"code","metadata":{"id":"20qex_UM6A9K","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Import Fastai Lesson code\n","#----------------------------------------------------\n","!git clone https://github.com/fastai/course-v3.git\n","!mv course-v3/nbs/dl2/exp .\n","import IPython.core.debugger as db\n","\n","! git clone https://github.com/NVIDIA/apex\n","! pip install -v --no-cache-dir apex/\n","\n","#export\n","from exp.nb_12 import *"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"faRhge3y6rUR","colab_type":"code","outputId":"d9997536-7e57-4c90-8fa3-391a60fff84f","executionInfo":{"status":"ok","timestamp":1584685100311,"user_tz":-330,"elapsed":276206,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":91}},"source":["#----------------------------------------------------\n","# Create the Data Bunch using Fastai Lesson code\n","# After running this cell once in a session, set 'loaded' to True to simply load the pickle file instead of running all the processing again\n","#----------------------------------------------------\n","torch.manual_seed(0)\n","path = datasets.untar_data(datasets.URLs.IMDB)\n","loaded=True\n","if (not loaded):\n","  il = TextList.from_files(path, include=['train', 'test', 'unsup'])\n","  sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))\n","  proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()\n","  ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])\n","  pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\n","  pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))\n","ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))\n","vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))\n","bs,bptt = 64,70\n","data = lm_databunchify(ll, bs, bptt)\n","\n","#----------------------------------------------------\n","# The Data Bunch loaded above takes 12-30 minutes to train 1 epoch.\n","# So we make a subset of that data to allow faster training while debugging\n","#----------------------------------------------------\n","\n","# Get subset of data\n","num_t = 2500\n","num_v = num_t // 5\n","\n","new_t, new_v = ll.train, ll.valid\n","tx, ty=TextList(new_t.x[:num_t]), TextList(new_t.y[:num_t])\n","vx, vy=TextList(new_v.x[:num_v]), TextList(new_v.y[:num_v])\n","tll = LabeledData(tx, ty)\n","vll = LabeledData(vx, vy)\n","new_sd = SplitData(tll, vll)\n","subset_data = lm_databunchify(new_sd, bs, bptt)\n","\n","# The below is just to get a single batch running some tests below\n","x,y = next(iter(subset_data.train_dl))\n","print('Data ', x.float().mean(), y.float().mean())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading https://s3.amazonaws.com/fast-ai-nlp/imdb\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='46' class='' max='46', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [46/46 02:18<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [5/5 00:15<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Data  tensor(1626.1665) tensor(1617.8551)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oDqvMEF25fhd","colab_type":"text"},"source":["### Import KD Libraries"]},{"cell_type":"code","metadata":{"id":"EHLPROiP6Lpk","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nx-Xm5ShB3A7","colab_type":"code","colab":{}},"source":["import IPython.core.debugger as db\n","from functools import partial\n","import warnings\n","import torch\n","import torch.nn.functional as F\n","from torch import tensor, nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QwN17d7_DKe","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","gd_path = 'gdrive/My Drive/Colab Data/fastai-v3'  #change dir to your project folder\n","gn_path = 'gdrive/My Drive/Colab Notebooks'  #change dir to your project folder\n","\n","import sys\n","sys.path.insert(1, gn_path + '/exp')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcLxkgTy_P1L","colab_type":"code","colab":{}},"source":["from nb_data import *\n","from nb_training import *\n","from nb_optimiser import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q5ahVATo09xU","colab_type":"text"},"source":["### Define the Language Model Application class"]},{"cell_type":"code","metadata":{"id":"9gNXqEJPEkLT","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Language Model Application\n","#----------------------------------------------------\n","class AppLanguageModel():\n","\n","  def __init__(self):\n","    self._arch = None\n","    self.lmdb = None\n","    self.vocab = None\n","    pass\n","\n","  def load_data(self, data_path):\n","    ds_params = {'target_ds': FastaiLMDataset, 'bs': 64, 'bptt': 70}\n","    self.lmdb = LanguageModelFolderDataBundle(data_path, ds_params)\n","    self.lmdb.do()\n","    self.vocab = self.lmdb.convert_state['vocab_i2w']\n","\n","  def create_arch(self, *dps):\n","    tok_pad = self.vocab.index(PAD)\n","    emb_sz, n_h, n_layers, pad_idx = 300, 300, 2, tok_pad\n","    self._arch = ArchLanguageModel(self.vocab, emb_sz, n_h, n_layers, pad_idx, *dps)\n","    return self._arch\n","\n","  def run_train(self, split_lr=[5e-3], split=False, one_cycle=False, freeze=\"UNFREEZE_LSTM\", num_epochs=1):\n","    train_dl = self.lmdb.train_dl\n","    valid_dl = self.lmdb.valid_dl\n","    # NB: Use cross_entropy_flat for Language Model\n","    loss_func = cross_entropy_flat\n","\n","    # split_lr is a list:\n","    #   1. a single-element list [0.01] - same LR for all groups. \n","    #        If 'Split' is False, there is only one group. \n","    #        If 'Split' is True, there are multiple groups.\n","    #   2. a multi-element list [0.01, 0.03, 0.05] - discriminative LR for different groups. \n","    #        'Split' cannot be False\n","    # eg. split_lr = [lr/2., lr/2., lr] for one cycle\n","    assert(isinstance(split_lr, list))\n","    assert(len(split_lr) > 0)\n","    assert(not ((split == False) and (len(split_lr) > 1)))\n","\n","    # NB: Use AwdLstmCB for Language Model and accuracy_flat\n","    callbs=[CudaCB(device = torch.device('cuda',0)), AwdLstmCB(alpha=2., beta=1.), GradientClipping(clip=0.1), ProgressCallback(), MetricsCB({\"acc\": accuracy_flat})]\n","    if (one_cycle):\n","      one_cycle_callbs = create_OneCycleCB(split_lr, phases=[0.5, 0.5], mom_start=0.8, mom_mid=0.7, mom_end=0.8)\n","      callbs = callbs + one_cycle_callbs\n","\n","    model = self._arch.model\n","    self._arch.freeze(freeze)\n","\n","    print ('BEFORE Hyper parameters')\n","    # eg. opt_func = optim.SGD, sgd_opt_func, adam_opt_func\n","    opt_func=adam_opt_func\n","    lr = split_lr[0]\n","    if (split and (len(split_lr) == 1)):\n","      hypers_group = [{}] * self._arch.n_splits\n","      opt_groups=(self._arch.splitter, hypers_group, {'lr': lr})\n","    elif (split and (len(split_lr) > 1)):\n","      hypers_group = [{'lr': lr_g} for lr_g in split_lr]\n","      opt_groups=(self._arch.splitter, hypers_group, {})\n","    else:\n","      opt_groups = None\n","    opt = get_optimiser(model, lr, opt_func, opt_groups)\n","\n","    dtr = DebugTracker(disp=(True, False))\n","    debug_cbs = [dtr, DebugYhatLossCB()]\n","    callbs = callbs + debug_cbs\n","\n","    loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs, dtr=dtr)\n","    loop.fit(num_epochs=num_epochs)\n","\n","    # TODO !!!!!!! Make _print_opt() a public function, maybe using repr\n","    print ('AFTER Hyper parameters')\n","    loop.opt._print_opt()\n","    return loop"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TVaxtq_tLJCl","colab_type":"text"},"source":["### Step 1 - Load Text Data (IMDB and Wikitext) and pre-process "]},{"cell_type":"code","metadata":{"id":"dUYhQHBlEmMV","colab_type":"code","outputId":"545aa2dc-9c39-4120-e68f-ec3f320e0cac","executionInfo":{"status":"ok","timestamp":1585060177270,"user_tz":-330,"elapsed":12568,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["#----------------------------------------------------\n","# Import IMDb dataset that contains 50,000 label reviews and 50,000 unlabeled reviews. \n","#----------------------------------------------------\n","\n","lm_app = AppLanguageModel()\n","\n","path_imdb = datasets.untar_data(datasets.URLs.IMDB)\n","lm_app.load_data(path_imdb)\n","\n","# !!!!!!! TEMP\n","lm_imdb_vocab = lm_app.vocab"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--------- IMDB Language Model DataBundle init /root/.fastai/data/imdb {'target_ds': <class 'nb_data.FastaiLMDataset'>, 'bs': 64, 'bptt': 70}\n","FolderItemContainer loaded 100001 items of type TextFileItemList\n","Split using split_random into 3000, 2000 and 0 items of type TextFileItemList\n","Extracted 3000 items of type SentenceItemList using extract_doc\n","Extracted 3000 items of type DummyItemList using extract_dummy\n","Converted 3000 items to type SentenceWordItemList using SentenceToWord\n","Converted 3000 items to type SentenceWordIdItemList using WordToWordId\n","Converted 894342 items to type StreamWordIdItemList using WordIdToStream\n","Extracted 2000 items of type SentenceItemList using extract_doc\n","Extracted 2000 items of type DummyItemList using extract_dummy\n","Converted 2000 items to type SentenceWordItemList using SentenceToWord\n","Converted 2000 items to type SentenceWordIdItemList using WordToWordId\n","Converted 595678 items to type StreamWordIdItemList using WordIdToStream\n","Final StreamWordIdItemList (894342 items)\n","[2, 21, 7, 65, 257, 10, 90, 245, 1307, 16...] \n"," DummyItemList (3000 items)\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n","Final StreamWordIdItemList (595678 items)\n","[2, 18, 193, 35, 924, 61, 153, 101, 38, 672...] \n"," DummyItemList (2000 items)\n","[0, 0, 0, 0, 0, 0, 0, 0, 0, 0...]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B9BhlQTALhHL","colab_type":"text"},"source":["### Step 2 - Build AWD-LSTM Module (Dropout Layers)"]},{"cell_type":"code","metadata":{"id":"KJNyi6m9WTR1","colab_type":"code","outputId":"2a41bfb5-ff08-4759-9716-dde5c249f566","executionInfo":{"status":"ok","timestamp":1585060094716,"user_tz":-330,"elapsed":71824,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["#----------------------------------------------------\n","# AWD-LSTM implements dropout at several places in the LSTM model\n","#\n","# Essentially dropout consists of replacing some coefficients by 0 with probability p. To ensure that \n","# the average of the weights remains constant, we apply a correction to the weights that aren't nullified \n","# of a factor 1/(1-p)\n","#----------------------------------------------------\n","\n","#----------------------------------------------------\n","# Create a mask that tells us which elements to nullify or not\n","#----------------------------------------------------\n","def dropout_mask(x, sz, p):\n","  return x.new(*sz).bernoulli_(1-p).div_(1-p)\n","\n","#----------------------------------------------------\n","# Dropout for an RNN has to work differently than dropout for a fully-connected network. Since a RNN\n","# has timesteps, we want dropout to replace a value by 0 for all timesteps.\n","#\n","# Inside a RNN, all tensors have shape (samples, timesteps, features). We want to consistently apply \n","# the dropout mask across the timesteps dimension, so that the dropout value for all timesteps should \n","# be the same. Therefore, we create a dropout mask for the samples and features dimension and broadcast\n","# it to the timesteps dimension.\n","# \n","# Once we have a mask, applying the dropout to a tensor 'x' is simply done by 'x = x * mask'\n","#----------------------------------------------------\n","class RNNDropout(nn.Module):\n","  def __init__(self, p=0.5):\n","    super().__init__()\n","    self.p=p\n","\n","  def forward(self, x):\n","    if not self.training or self.p == 0.: return x\n","    # The mask 'm' has shape (samples, 1, features). The timesteps dimension\n","    # which has size 1 gets replicated since it gets broadcast when the mask \n","    # is multiplied with 'x'\n","    m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n","    return x * m\n","\n","#----------------------------------------------------\n","# Weight Dropout dropout is applied to hidden-to-hidden matrix inside the LSTM by zeroing out \n","# hidden units randomly. This is a little hacky if we want to preserve the CuDNN speed and not \n","# reimplement the cell from scratch. This needs to be done in a way that ensure the gradients \n","# are still computed and the initial weights still updated. \n","#\n","# We wrap the LSTM with the WeightDropout module. We add a parameter that will contain the raw weights. \n","# Weight masks are randomly refreshed at each batch's forward pass.\n","#----------------------------------------------------\n","WEIGHT_HH = 'weight_hh_l0'\n","class WeightDropout(nn.Module):\n","  def __init__(self, module, weight_p=[0.], param_names=[WEIGHT_HH]):\n","    super().__init__()\n","    self.module,self.weight_p,self.param_names = module,weight_p,param_names\n","    \n","    # Keep a list of the LSTM's weight param names which we want to dropout\n","    for param in self.param_names:\n","      # Get the LSTM weights and copy it into ourself.\n","      w = getattr(self.module, param)\n","      self.register_parameter(f'{param}_raw', nn.Parameter(w.data))\n","\n","      # !!!!! DON'T UNDERSTAND THIS LOGIC EXACTLY\n","      self.module._parameters[param] = F.dropout(w, p=self.weight_p, training=False)\n","\n","  # ----------------------------\n","  # Update the LSTM weights at the beginning of each forward pass\n","  # ----------------------------\n","  def _setweights(self):\n","    for param in self.param_names:\n","      # !!!!! DON'T UNDERSTAND THIS LOGIC EXACTLY\n","      # Get the weights which we had saved away and apply the dropout on it\n","      raw_w = getattr(self, f'{param}_raw')\n","      self.module._parameters[param] = F.dropout(raw_w, p=self.weight_p, training=self.training)\n","\n","  # ----------------------------\n","  # During our forward, we update the LSTM weights and then call the LSTM forward. Since we\n","  # are wrapping the LSTM, the LSTM forward is not called directly.\n","  # ----------------------------\n","  def forward(self, *args):\n","    self._setweights()\n","    with warnings.catch_warnings():\n","      #To avoid the warning that comes because the weights aren't flattened.\n","      warnings.simplefilter(\"ignore\")\n","      return self.module.forward(*args)\n","\n","#----------------------------------------------------\n","# Embedding Dropout is applied when we lookup the ids of our tokens inside the embedding matrix\n","# As the dropout occurs on the embedding matrix that is used for a full forward and backward \n","# pass, this means that all occurrences of a specific word will disappear within that pass\n","#----------------------------------------------------\n","class EmbeddingDropout(nn.Module):\n","  \"Applies dropout in the embedding layer by zeroing out some elements of the embedding vector.\"\n","  def __init__(self, emb, embed_p):\n","    super().__init__()\n","    self.emb,self.embed_p = emb,embed_p\n","    self.pad_idx = self.emb.padding_idx\n","    if self.pad_idx is None: self.pad_idx = -1\n","\n","  def forward(self, words, scale=None):\n","    if self.training and self.embed_p != 0:\n","      # Since we zero out the entire embedding vector, our mask only looks at the\n","      # first size dimension ie. the number of embedding words\n","      size = (self.emb.weight.size(0),1)\n","      mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n","\n","      # Apply the dropout mask on the embedding\n","      masked_embed = self.emb.weight * mask\n","    else:\n","      # No dropout, use the original embedding\n","      masked_embed = self.emb.weight\n","\n","    if scale: \n","      masked_embed.mul_(scale)\n","\n","    return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n","                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)\n","\n","test_x = torch.randn(4, 5)\n","test_mask = dropout_mask(test_x, (4, 5), 0.5); print('Test Mask is', test_mask)\n","\n","test_dp = RNNDropout(0.3)\n","test_x = torch.randn(2, 3, 4)\n","print ('Original input\\n', test_x, '\\nAfter dropout\\n', test_dp(test_x))\n","\n","test_module = nn.LSTM(5, 2)\n","test_dp_module = WeightDropout(test_module, 0.4)\n","print('Initial weights are', getattr(test_dp_module.module, WEIGHT_HH))\n","\n","test_input = torch.randn(4,8,5)\n","test_h = (torch.zeros(1,8,2), torch.zeros(1,8,2))\n","test_x, test_h = test_dp_module(test_input, test_h)\n","print('After forward weights are', getattr(test_dp_module.module, WEIGHT_HH))\n","\n","test_enc = nn.Embedding(100, 7, padding_idx=1)\n","test_enc_dp = EmbeddingDropout(test_enc, 0.5)\n","test_input = torch.randint(0,100,(8,))\n","test_enc_dp(test_input)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Mask is tensor([[2., 2., 2., 0., 2.],\n","        [2., 0., 0., 2., 2.],\n","        [0., 2., 0., 0., 2.],\n","        [0., 2., 2., 0., 2.]])\n","Original input\n"," tensor([[[-2.0707,  0.6421, -0.9576,  1.9489],\n","         [ 0.2422,  1.5373,  1.3674,  0.9027],\n","         [-0.0936, -2.5835,  0.8783, -1.1348]],\n","\n","        [[-1.2275,  2.2400,  0.7559,  1.0947],\n","         [-0.4146, -1.2804,  1.0682,  1.3891],\n","         [ 0.2492,  0.1286, -1.6805,  1.0375]]]) \n","After dropout\n"," tensor([[[-2.9582,  0.9173, -0.0000,  0.0000],\n","         [ 0.3460,  2.1961,  0.0000,  0.0000],\n","         [-0.1337, -3.6908,  0.0000, -0.0000]],\n","\n","        [[-1.7536,  3.2001,  0.0000,  1.5639],\n","         [-0.5923, -1.8291,  0.0000,  1.9845],\n","         [ 0.3559,  0.1837, -0.0000,  1.4822]]])\n","Initial weights are Parameter containing:\n","tensor([[-1.9371e-02, -7.6613e-02],\n","        [ 2.7376e-04,  3.3425e-01],\n","        [-3.6033e-01, -4.4733e-01],\n","        [ 3.2776e-01, -2.0330e-01],\n","        [-4.5878e-01, -3.2258e-01],\n","        [-6.1957e-01,  5.1847e-01],\n","        [-4.9523e-01, -6.0286e-01],\n","        [-5.1162e-01,  6.6731e-01]], requires_grad=True)\n","After forward weights are tensor([[-0.0000, -0.1277],\n","        [ 0.0000,  0.0000],\n","        [-0.6005, -0.0000],\n","        [ 0.0000, -0.0000],\n","        [-0.0000, -0.5376],\n","        [-1.0326,  0.0000],\n","        [-0.8254, -1.0048],\n","        [-0.8527,  1.1122]], grad_fn=<MulBackward0>)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([[-1.0230, -0.2401,  1.1616, -2.6485,  2.1642,  3.8254,  4.4194],\n","        [ 1.1437,  0.6681,  1.0725, -1.4870, -0.7679, -1.1570, -0.9646],\n","        [-0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000],\n","        [ 0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000],\n","        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000],\n","        [ 1.7627, -2.3011,  2.8185, -3.2187,  1.5134,  0.2587, -0.8498],\n","        [ 0.0805,  2.8928,  0.9860, -0.2880,  0.4170,  0.0177,  1.6220],\n","        [ 1.5210, -1.3632, -1.6870,  0.4517,  0.4301, -1.5746, -1.2623]],\n","       grad_fn=<EmbeddingBackward>)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"00D26zxcoo4Q","colab_type":"text"},"source":["### Step 2 - Build AWD-LSTM Module (Core) and Callbacks"]},{"cell_type":"markdown","metadata":{"id":"6PCFN3VCNxiH","colab_type":"text"},"source":["**Core AWD-LSTM Module**"]},{"cell_type":"code","metadata":{"id":"Pz8ydmUPoviP","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# AWD-LSTM architecture. It is a multi-layer LSTM network with 'n_layers' layers.\n","# The input data goes through an embedding layer (with embedding dropout). The resulting output of\n","# the embedding layer goes through the 'input dropout'. This is then fed to the first LSTM\n","# layer. Each LSTM layer has a 'weight dropout'. The output of the first LSTM layer goes through\n","# a 'hidden dropout' before being fed to the second LSTM layer. And so on for each layer.\n","#\n","# The last LSTM layer doesn't have a 'hidden dropout' but an 'output dropout' instead.\n","# That then goes through a Linear Decoder module which produces the final output.\n","#----------------------------------------------------\n","\n","class AWD_LSTM(nn.Module):\n","  emb_init=0.1\n","\n","  # ----------------------------\n","  # Build the architecture\n","  # ----------------------------\n","  def __init__(self, vocab_sz, emb_sz, n_h, n_layers, pad_idx, emb_p, inp_p, weight_p, hidden_p, apply_packing=False):\n","    super().__init__()\n","    self.bs = 1\n","    self.vocab_sz, self.emb_sz, self.n_h, self.n_layers = vocab_sz, emb_sz, n_h, n_layers\n","    self.pad_idx, self.apply_packing = pad_idx, apply_packing\n","\n","    # Probabilities for various dropoouts\n","    self.emp_p, self.inp_p, self.weight_p, self.hidden_p = emb_p, inp_p, weight_p, hidden_p\n","\n","    # Embedding layer with Dropout\n","    # Embedding matrix has shape [vocab size, embedding vector size]\n","    self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_idx)\n","    self.emb.weight.data.uniform_(-self.emb_init, self.emb_init)\n","    self.emb_dp = EmbeddingDropout(self.emb, emb_p)\n","\n","    # Input dropout (post Embedding)\n","    self.inp_dp = RNNDropout(inp_p)\n","\n","    # Create the multi-layer deep RNN network. We create a list of LSTM layers and another list\n","    # of Hidden Dropouts, one for each LSTM layer.\n","    self.lstm_layers = nn.ModuleList([self._create_lstm(*self._layer_sz(i)) for i in range(n_layers)])\n","    self.hidden_dps = nn.ModuleList([self._create_hidden_dp(i == n_layers-1) for i in range(n_layers)])\n","\n","    # Re-initialise the hidden state for all LSTM layers\n","    self.reset_state()\n","\n","  # ----------------------------\n","  # Process the forward pass\n","  # ----------------------------\n","  def forward(self, input):\n","    # If the batch size changes, then re-initialise the hidden state (whose shape depends on batch size)\n","    bs, ts = input.size()\n","    if bs!=self.bs:\n","      self.bs=bs\n","      self.reset_state()\n","\n","    # When this module is used for Classification, the input data is padded as the sequences \n","    # could be of different lengths. To make the LSTM computations efficient we pack this\n","    # data more compactly as an optimisation. Padding (and therefore packing) is not needed when \n","    # the module is used for a Language Model as the input data is a continuous stream of data \n","    # rather than individual sentences which doesn't require padding.\n","    if (self.apply_packing):\n","      input, pad_mask, seq_lengths = self._pad_mask(input, ts)\n","\n","    # Input goes through Embedding layer with dropout\n","    # Input has shape [samples, timesteps]\n","    # Emb_val has shape [samples, timesteps, embedding size]\n","    emb_val = self.emb_dp(input)\n","\n","    # Now apply Input dropout to the result of the embedding\n","    # This will then be fed to the LSTM layers\n","    # Inp_val has shape [samples, timesteps, embedding size]\n","    inp_val = self.inp_dp(emb_val)\n","\n","    #print ('KD input and inp val', input.float().mean(), inp_val.mean())\n","\n","    # Keep a list of hidden state, raw output values and output values post dropout for each LSTM layer\n","    new_states, out_vals, out_dp_vals = [], [], []\n","\n","    # Go through each LSTM layer and its corresponding Hidden Dropout and Hidden State\n","    for lstm_dp, hidden_dp, state in zip(self.lstm_layers, self.hidden_dps, self.state):\n","      \n","      if (self.apply_packing):\n","        # Pack the data, apply the LSTM and then unpack the data\n","        inp_val = pack_padded_sequence(inp_val, seq_lengths, batch_first=True)\n","        out_val, new_state = lstm_dp(inp_val, state)\n","        out_val = pad_packed_sequence(out_val, batch_first=True)[0]\n","      else:\n","        # Apply the LSTM directly\n","        out_val, new_state = lstm_dp(inp_val, state)\n","\n","      # Apply the Hidden Dropout to the LSTM output\n","      out_dp_val = hidden_dp(out_val)\n","\n","      #print ('KD layer', out_val.mean(), state[0].mean(), state[1].mean(), new_state[0].mean(), new_state[1].mean())\n","      #print ('KD hidden', out_dp_val.mean())\n","\n","      # Add the state, raw output value and output value post dropout for this layer, to the lists\n","      # [hidden state layer 1, hidden state layer 2, ....]\n","      # [raw output layer 1, raw output layer 2, ...]\n","      # [(post dropout) output layer 1, output layer 2, ...]\n","      new_states.append(new_state)\n","      out_vals.append(out_val)\n","      out_dp_vals.append(out_dp_val)\n","\n","      # The post-dropout output will become the input to the next layer\n","      inp_val = out_dp_val\n","\n","    # Save the new hidden states\n","    self.state = self._to_detach(new_states)\n","\n","    # Return ([list of raw outputs for each layer], [list of outputs for each layer])\n","    if (self.apply_packing):\n","      return (out_vals, out_dp_vals, pad_mask)\n","    else:\n","      return (out_vals, out_dp_vals)\n","\n","  # ----------------------------\n","  # Initialise the hidden state for all LSTM layers\n","  # ----------------------------\n","  def reset_state(self):\n","    # [hidden state layer 1, hidden state layer 2, ...]\n","    self.state = [self._zero_state(*self._layer_sz(i)) for i in range(self.n_layers)]\n","\n","  # ----------------------------\n","  # Detach 'h' from its history\n","  # ----------------------------\n","  def _to_detach(self, h):\n","    return h.detach() if type(h) == torch.Tensor else tuple(self._to_detach(v) for v in h)\n","\n","  # ----------------------------\n","  # Compute the dimensions of the i'th LSTM layer\n","  #   n_x = number of input features of the LSTM\n","  #   n_h = number of hidden features of the LSTM\n","  #\n","  # NB:     \n","  #     the input of a LSTM layer has shape (samples, timesteps, input features)\n","  #    the output of a LSTM layer has shape (samples, timesteps, hidden features)\n","  #    Since the first two dimensions are always the same, we mention only the third\n","  #    dimension in the comments below.\n","  # ----------------------------\n","  def _layer_sz(self, layer_i):\n","    is_last = False\n","\n","    if (layer_i == 0):\n","      # First layer has (n_x, n_h) = (embedding size, hidden size)\n","      # ie. input size matches the embedding, and output size is hidden\n","      n_x = self.emb_sz\n","      n_h = self.n_h\n","\n","    elif (layer_i == self.n_layers-1):\n","      # Last layer has (n_x, n_h) = (hidden size, embedding size)\n","      # ie. input size matches the output size of previous layer = hidden\n","      # and output size is same as embedding\n","      n_x = self.n_h\n","      n_h = self.emb_sz\n","      is_last = True\n","\n","    else:\n","      # All middle layers have (n_x, n_h) = (hidden size, hidden size)\n","      # ie. input size matches the output size of previous layer = hidden\n","      # and output size is also hidden\n","      n_x = self.n_h\n","      n_h = self.n_h\n","\n","    return (n_x, n_h, is_last)\n","\n","  # ----------------------------\n","  # Initialise the hidden state for one LSTM layer\n","  # ----------------------------\n","  def _zero_state(self, n_x, n_h, is_last):\n","    # This is done only to get the data type of the existing hidden state. When the\n","    # new hidden state is cloned from the parameters below, it preserves\n","    # the type\n","    some_param = next(self.parameters()).data\n","\n","    # Create new hidden state and set all values to 0.\n","    # Hidden and Cell state has shape (1, samples, hidden features)\n","    state_h = some_param.new(1, self.bs, n_h).zero_()\n","    state_c = some_param.new(1, self.bs, n_h).zero_()\n","    return ((state_h, state_c))\n","\n","  # ----------------------------\n","  # Create one LSTM layer with its Weight Dropout\n","  # ----------------------------\n","  def _create_lstm(self, n_x, n_h, is_last):\n","    # 'batch_first' is true because the data is shaped (samples, timesteps, hidden size)\n","    # instead of (timesteps, samples, hidden size)\n","    lstm = nn.LSTM(n_x, n_h, 1, batch_first=True)\n","    lstm_dp = WeightDropout(lstm, self.weight_p)\n","    return (lstm_dp)\n","\n","  # ----------------------------\n","  # Create one Hidden Dropout layer\n","  # Hidden Dropout is not applied to the last layer so we use an Identity layer\n","  # which does a No-op\n","  # ----------------------------\n","  def _create_hidden_dp(self, is_last):\n","    if (is_last):\n","      hidden_dp = nn.Identity()\n","    else:\n","      hidden_dp = RNNDropout(self.hidden_p)\n","    return (hidden_dp)\n","\n","  # ----------------------------\n","  # Compute the padding mask\n","  # ----------------------------\n","  def _pad_mask(self, input, ts):\n","    # Mask contains True for all elements which are padding\n","    # Shape is same as input ie. [samples, timesteps]\n","    pad_mask = (input == self.pad_idx)\n","    # Count number of padding values in each row by adding up all True mask values per row\n","    # Subtract that from the total width (ie. timesteps) to get the length of the data sequence\n","    # Shape is [samples]\n","    seq_lengths = ts - pad_mask.long().sum(1)\n","\n","    # Strip out rows with no sequence data\n","    n_empty = (seq_lengths == 0).sum()\n","    if n_empty > 0:\n","      input = input[:-n_empty]\n","      seq_lengths = seq_lengths[:-n_empty]\n","      self.state = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.state]\n","\n","    return (input, pad_mask, seq_lengths)\n","\n","#----------------------------------------------------\n","# A sequential module that passes the reset call to its children\n","# KD - Not sure when this reset functionally is needed or how it is used\n","#----------------------------------------------------\n","class SequentialRNN(nn.Sequential):\n","  def reset(self):\n","    for c in self.children():\n","      if hasattr(c, 'reset_state'): c.reset_state()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heQkZ1osNNky","colab_type":"text"},"source":["**AWD-LSTM Callbacks**"]},{"cell_type":"code","metadata":{"id":"9IfbisZhNMAx","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Implement AWD LSTM logic for:\n","#     1. Modify the architecture's output which returns a tuple (decoded, raw output, output) to\n","#        keep only the the decoded tensor as the 'yhat' value (for the loss function). The \n","#        raw output and output are stored separately\n","#     2. Apply Activation Regularization (AR): we add to the loss an L2 penalty on the last \n","#        activations of the AWD LSTM (with dropout applied)\n","#     3. Apply Temporal Activation Regularization (TAR): we add to the loss an L2 penalty on \n","#        the difference between two consecutive (in terms of timesteps) raw outputs\n","#     4. TODO - Trigger the shuffle of the LMDataset at the beginning of each epoch\n","#----------------------------------------------------\n","class AwdLstmCB(Callback):\n","  def __init__(self, alpha, beta):\n","    self.alpha, self.beta = alpha, beta\n","\n","  def after_tr_pred(self, ctx):\n","    self._extract_yhat(ctx)\n","\n","  def after_val_pred(self, ctx):\n","    self._extract_yhat(ctx)\n","\n","  def _extract_yhat(self, ctx):\n","    ctx.out_vals, ctx.out_dp_vals = ctx.yhat[1], ctx.yhat[2]\n","    ctx.yhat = ctx.yhat[0]\n","\n","  def _regularise_ar_tar(self, ctx):\n","    if self.alpha != 0.:\n","      ctx.loss += self.alpha * ctx.out_dp_vals[-1].float().pow(2).mean()\n","\n","    if self.beta != 0.:\n","      h = ctx.out_vals[-1]\n","      if h.size(1)>1: \n","        ctx.loss += self.beta * (h[:,1:] - h[:,:-1]).float().pow(2).mean()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OoNv-USiMrju","colab_type":"text"},"source":["### Step 3 - Build Language Model Architecture"]},{"cell_type":"markdown","metadata":{"id":"RfiYi9VHNl9Y","colab_type":"text"},"source":["**Linear Decoder Module**"]},{"cell_type":"code","metadata":{"id":"odc2mZgFMx-Z","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Linear Decoder module to process the output of the LSTM layers\n","# It applies the Output Dropout on the last LSTM layer and then uses a simple\n","# Linear layer. It optionally uses the same weights as the weights of the embedding\n","# layer\n","#----------------------------------------------------\n","class LinearDecoder(nn.Module):\n","  def __init__(self, n_inp, n_out, out_p, tie_encoder):\n","    super().__init__()\n","\n","    # Output Dropout\n","    self.out_dp = RNNDropout(out_p)\n","\n","    # Decoder is a simple linear layer\n","    self.decoder = nn.Linear(n_inp, n_out)\n","    self.decoder.bias.data.zero_()\n","\n","    # If we are using weight-tying, then copy the weights from the embedding\n","    # layer. If not, use Kaiming Init to initialise them\n","    if (tie_encoder):\n","      self.decoder.weight = tie_encoder.weight\n","    else:\n","      init.kaiming_uniform_(self.decoder.weight)\n","\n","  # ----------------------------\n","  # Process the forward pass\n","  # ----------------------------\n","  def forward(self, input):\n","    # Get the list of raw outputs and post-dropout outputs from the LSTM layers\n","    out_vals, out_dp_vals = input\n","\n","    # Apply Output Dropout. Make it contiguous so the subsequent view() function can work\n","    out_dp = self.out_dp(out_dp_vals[-1]).contiguous()\n","\n","    # Flatten the data for the linear layer from 3D to 2D as (samples * timesteps, features)\n","    decoded = self.decoder(out_dp.view(out_dp.size(0)*out_dp.size(1), out_dp.size(2)))\n","    \n","    #print ('KD Decoder output & decoded', out_dp.mean(), decoded.mean())\n","\n","    # Return Output - ([final decoded probabilities for each word], [list of raw outputs for each layer], [list of outputs for each layer])\n","    return (decoded, out_vals, out_dp_vals)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a6LS952xNp27","colab_type":"text"},"source":["**Language Model Architecture**"]},{"cell_type":"code","metadata":{"id":"8rX21w8pm5Tp","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Create the end-to-end language model architecture with the AWD-LSTM followed by the Decoder\n","#----------------------------------------------------\n","class ArchLanguageModel():\n","\n","  def __init__(self, vocab, emb_sz, n_h, n_layers, pad_idx, out_p=0.4, hidden_p=0.2, inp_p=0.6, \n","                       emb_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n","    self.vocab = vocab\n","    vocab_sz = len(vocab)\n","\n","    self.awd_lstm_enc = AWD_LSTM(vocab_sz, emb_sz, n_h, n_layers, pad_idx, emb_p, inp_p, weight_p, hidden_p)\n","\n","    # Get the embedding layer from the AWD-LSTM to enable weight-tying with the\n","    # Decoder\n","    enc = self.awd_lstm_enc.emb if tie_weights else None\n","\n","    # The input of the Decoder has embedding size to match the output of the \n","    # last AWD-LSTM layer. The output of the Decoder has vocab size to produce\n","    # a probability for each word in the vocab\n","    self.decoder = LinearDecoder(emb_sz, vocab_sz, out_p, tie_encoder=enc)\n","\n","    self.model = SequentialRNN(self.awd_lstm_enc, self.decoder)\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def load_weights(self, weights):\n","    self.model.load_state_dict(weights)\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def save_weights(self, awd_lstm_enc_path, vocab_path, weights_path):\n","\n","    # We only need to save the encoder (first part of the model)...\n","    torch.save(self.awd_lstm_enc.state_dict(), awd_lstm_enc_path)\n","\n","    # ...as well as the vocabulary used.\n","    # We will use both for the classification task\n","    pickle.dump(self.vocab, open(vocab_path, 'wb'))\n","\n","    # Save the full model\n","    torch.save(self.model.state_dict(), weights_path)\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def freeze(self, type):\n","    if (type == \"FREEZE_LSTM\"):\n","      for rnn in self.awd_lstm_enc.lstm_layers:\n","        for p in rnn.parameters(): p.requires_grad_(False)\n","    elif (type == \"UNFREEZE_LSTM\"):\n","      for rnn in self.awd_lstm_enc.lstm_layers:\n","        for p in rnn.parameters(): p.requires_grad_(True)\n","\n","  # ----------------------------\n","  # Split into three groups - two for each rnn/corresponding dropout, then one last \n","  # group that contains the embeddings/decoder.\n","  # ----------------------------\n","  n_splits = 3\n","  def splitter(self, type):\n","    awd_lstm_enc = self.awd_lstm_enc\n","    \n","    groups = []\n","    # One group for each LSTM/Hidden Dropout pair\n","    for i in range(len(awd_lstm_enc.lstm_layers)): \n","      groups.append(nn.Sequential(awd_lstm_enc.lstm_layers[i], awd_lstm_enc.hidden_dps[i]))\n","  \n","    # Embedding and Input Dropouts and Decoder\n","    groups += [nn.Sequential(awd_lstm_enc.emb, awd_lstm_enc.emb_dp, awd_lstm_enc.inp_dp, self.decoder)]\n","\n","    # List of list of parameters by group\n","    return [list(o.parameters()) for o in groups]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iy13RqN-Ncr2","colab_type":"text"},"source":["**Callbacks for Language Model**"]},{"cell_type":"code","metadata":{"id":"FCzcA12j5bVr","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Clip the gradients to allow us to use a higher learning rate by putting a \n","# maximum value on the norm of the gradients\n","#----------------------------------------------------\n","class GradientClipping(Callback):\n","  def __init__(self, clip=None): \n","    self.clip = clip\n","\n","  def after_tr_backward(self, ctx):\n","    if self.clip:  \n","      nn.utils.clip_grad_norm_(ctx.model.parameters(), self.clip)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b31SXFe0PScF","colab_type":"text"},"source":["**Flattened versions of the cross entropy loss and the accuracy metric**"]},{"cell_type":"code","metadata":{"id":"iFdQEBsGPZcN","colab_type":"code","colab":{}},"source":["def cross_entropy_flat(input, target):\n","  bs,sl = target.size()\n","  return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))\n","\n","def accuracy_flat(input, target):\n","  bs,sl = target.size()\n","  return accuracy(input.view(bs * sl, -1), target.view(bs * sl))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vqfv0q3l9tE3","colab_type":"text"},"source":["### Step 3 - Test Run language model end-to-end "]},{"cell_type":"code","metadata":{"id":"KKJfSCi4FOtQ","colab_type":"code","colab":{}},"source":["torch.manual_seed(0)\n","lm_app.create_arch()\n","loop = lm_app.run_train(split_lr=[5e-3], split=False, one_cycle=False, freeze=\"UNFREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXWkWjrfXU8p","colab_type":"text"},"source":["### Set up Tensorboard"]},{"cell_type":"markdown","metadata":{"id":"YzP27LL5cLAY","colab_type":"text"},"source":["**To view Tensorboard output locally, use ngrok to tunnel traffic to localhost. First, download and unzip ngrok on the Colab server**"]},{"cell_type":"code","metadata":{"id":"QomVFMK8agNm","colab_type":"code","outputId":"0197852c-ace4-494c-d461-898859e73b3c","executionInfo":{"status":"ok","timestamp":1585060233246,"user_tz":-330,"elapsed":8082,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","! unzip ngrok-stable-linux-amd64.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-03-24 14:30:27--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.206.78.89, 34.192.123.246, 54.174.156.76, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.206.78.89|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13773305 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \r         ngrok-stab   1%[                    ] 205.85K   823KB/s               \r        ngrok-stabl  26%[====>               ]   3.50M  7.00MB/s               \rngrok-stable-linux- 100%[===================>]  13.13M  19.0MB/s    in 0.7s    \n","\n","2020-03-24 14:30:28 (19.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dx9TXX0acOZv","colab_type":"text"},"source":["**Get TensorBoard running in the background**"]},{"cell_type":"code","metadata":{"id":"T3sFSzVhlOVu","colab_type":"code","colab":{}},"source":["# Set the LOGDIR correctly to use Tensorboard\n","LOG_DIR = 'tbtry'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jTEsOw98bUY7","colab_type":"code","colab":{}},"source":["get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhztbwBnc_DL","colab_type":"text"},"source":["**Launch ngrok background process**"]},{"cell_type":"code","metadata":{"id":"JCkaIrwqbfzP","colab_type":"code","colab":{}},"source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9BADzHi9dBh6","colab_type":"text"},"source":["**We get the public URL where we can access the colab TensorBoard web page. This will output a URL you can click on**"]},{"cell_type":"code","metadata":{"id":"RC65XT9PbmyJ","colab_type":"code","outputId":"8447f2ce-4009-4961-f066-0be7a71923af","executionInfo":{"status":"ok","timestamp":1585060237842,"user_tz":-330,"elapsed":4840,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["http://46803a22.ngrok.io\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7qivCJJNNzAA","colab_type":"code","outputId":"d3ebeef5-1d9d-4e3a-ca8d-bd078bb86a67","executionInfo":{"status":"ok","timestamp":1585050209682,"user_tz":-330,"elapsed":7052,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!rm -R tbtry\n","!ls -lR tbtry"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ls: cannot access 'tbtry': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"htrEOGHnV8SP","colab_type":"text"},"source":["### Debug Experimentation"]},{"cell_type":"code","metadata":{"id":"IXniZkEoNunP","colab_type":"code","colab":{}},"source":["dcdrrun_df, dcdrbatch_df, dcdrstep_df, dcdrdf = loop.dtr.pd_results()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B17pUpoak5gA","colab_type":"text"},"source":["### Step 4 - Pre-Train Language Model on Wikitext data\n","\n","TO DO - run code from this [notebook](https://github.com/fastai/course-v3/blob/master/nbs/dl2/12b_lm_pretrain.ipynb)\n","\n","This notebook literally combines the previous two notebooks. High level there is two steps:\n","import and preprocess data\n","create the model we made the previous notebook"]},{"cell_type":"markdown","metadata":{"id":"42NnipEpSQPR","colab_type":"text"},"source":["\n","An important thing to notice is that in IMDb dataset we used different vocabulary than in wikitext 103. To solve this we just combine the two vocabularies by overwriting others and using embeddings from wikitext 103."]},{"cell_type":"markdown","metadata":{"id":"Qp8GLART4ztV","colab_type":"text"},"source":["### Step 5 - Transform the pre-trained Wikitext Language Model vocab to the IMDB vocab"]},{"cell_type":"markdown","metadata":{"id":"6n5FORfuZEyZ","colab_type":"text"},"source":["**Get the IMDB corpus vocab**"]},{"cell_type":"code","metadata":{"id":"DLYp2Kni430N","colab_type":"code","outputId":"4937e53b-34df-417e-e62e-c7eba1bc1a57","executionInfo":{"status":"ok","timestamp":1584685178218,"user_tz":-330,"elapsed":354028,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# The code in the Fastai lesson gets the vocab like this, but there is no need to\n","# do that. We can use the vocab which we had loaded earlier since it is identical.\n","tvocab = ll.train.proc_x[1].vocab\n","\n","# The tvocab and the vocab that we had loaded earlier are actually identical.\n","vocab[50:60], tvocab[50:60]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['!', 'from', 'so', 'like', 'there', 'or', 'just', 'her', 'do', 'about'],\n"," ['!', 'from', 'so', 'like', 'there', 'or', 'just', 'her', 'do', 'about'],\n"," tensor([0.0500, 0.0750, 0.1250, 0.0100, 0.1000]))"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"01DXBoH9s7vK","colab_type":"text"},"source":["**Fetch the Language Model pretrained on Wikitext**\n","\n","We can use the Language Model from Step 4, or download a smaller version that was pretrained by the Fastai team.\n"]},{"cell_type":"code","metadata":{"id":"3gXblXbX5-BI","colab_type":"code","outputId":"e93f0246-eec1-455d-cfcb-b9af0ac50448","executionInfo":{"status":"ok","timestamp":1585040413481,"user_tz":-330,"elapsed":12480,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["# Download a pretrained small model which was trained with wikitext 103.\n","# In the final version we will train our own full model, but during debugging we can use this smaller one\n","path_wiki_model = Path.cwd()/'wikimdl'\n","\n","! wget http://files.fast.ai/models/wt103_tiny.tgz -P {path_wiki_model}\n","! tar xf {path_wiki_model}/wt103_tiny.tgz -C {path_wiki_model}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-03-24 09:00:03--  http://files.fast.ai/models/wt103_tiny.tgz\n","Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n","Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 75482451 (72M) [application/x-gtar-compressed]\n","Saving to: ‘/content/wikimdl/wt103_tiny.tgz’\n","\n","wt103_tiny.tgz      100%[===================>]  71.99M  16.6MB/s    in 5.6s    \n","\n","2020-03-24 09:00:09 (12.8 MB/s) - ‘/content/wikimdl/wt103_tiny.tgz’ saved [75482451/75482451]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n8-pynHZ6WVl","colab_type":"code","outputId":"bf270249-3058-4b95-be67-59246ff6729f","executionInfo":{"status":"ok","timestamp":1585040413483,"user_tz":-330,"elapsed":10697,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# See the directory structure\n","pt = path_wiki_model/'pretrained'\n","path_wiki_model.ls(), pt.ls()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([PosixPath('/content/wikimdl/pretrained'),\n","  PosixPath('/content/wikimdl/wt103_tiny.tgz')],\n"," [PosixPath('/content/wikimdl/pretrained/pretrained.pth'),\n","  PosixPath('/content/wikimdl/pretrained/vocab.pkl')])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"s7a54L1zbKOy","colab_type":"text"},"source":["**Load the weights of the pretrained model**"]},{"cell_type":"code","metadata":{"id":"WaZSZJmT6R-j","colab_type":"code","outputId":"5ee35d67-4d6b-49eb-b547-70b04627fe26","executionInfo":{"status":"ok","timestamp":1585040415117,"user_tz":-330,"elapsed":1614,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Get the model weights of the pretrained model\n","old_wgts  = torch.load(path_wiki_model/'pretrained'/'pretrained.pth')\n","print(old_wgts.keys())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["odict_keys(['0.emb.weight', '0.emb_dp.emb.weight', '0.rnns.0.weight_hh_l0_raw', '0.rnns.0.module.weight_ih_l0', '0.rnns.0.module.weight_hh_l0', '0.rnns.0.module.bias_ih_l0', '0.rnns.0.module.bias_hh_l0', '0.rnns.1.weight_hh_l0_raw', '0.rnns.1.module.weight_ih_l0', '0.rnns.1.module.weight_hh_l0', '0.rnns.1.module.bias_ih_l0', '0.rnns.1.module.bias_hh_l0', '1.decoder.weight', '1.decoder.bias'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"efa60963-f877-4732-a07e-3ffdfc94cd3c","executionInfo":{"status":"ok","timestamp":1585040415748,"user_tz":-330,"elapsed":2226,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"id":"yRWQt-e1FJd7","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# See the different weights in the pretrained model. Below we see that the emb weights, emb dp weights and decoder weights are the same, as expected.\n","old_wgts['0.emb.weight'][500:520, 6], old_wgts['0.emb_dp.emb.weight'][500:520, 6], old_wgts['1.decoder.weight'][500:520, 6]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([ 0.5687,  0.0495,  0.2678, -0.2443, -0.0888,  0.0075, -0.6737,  0.2850,\n","         -0.6399,  0.3175, -0.9170,  0.4346,  0.4071, -0.1929,  0.5366, -0.7570,\n","          0.5967,  0.0317, -0.3167, -0.3438], device='cuda:0'),\n"," tensor([ 0.5687,  0.0495,  0.2678, -0.2443, -0.0888,  0.0075, -0.6737,  0.2850,\n","         -0.6399,  0.3175, -0.9170,  0.4346,  0.4071, -0.1929,  0.5366, -0.7570,\n","          0.5967,  0.0317, -0.3167, -0.3438], device='cuda:0'),\n"," tensor([ 0.5687,  0.0495,  0.2678, -0.2443, -0.0888,  0.0075, -0.6737,  0.2850,\n","         -0.6399,  0.3175, -0.9170,  0.4346,  0.4071, -0.1929,  0.5366, -0.7570,\n","          0.5967,  0.0317, -0.3167, -0.3438], device='cuda:0'))"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"fNV7_wk6bREz","colab_type":"text"},"source":["**Update the Embeddings of the pretrained model so that they conform to the new IMDB vocab rather than the old Wikitest voca**b"]},{"cell_type":"code","metadata":{"id":"CwORXDw7aOG5","colab_type":"code","outputId":"e38242f7-f2dd-43a8-f6b3-8b8f77ea206d","executionInfo":{"status":"ok","timestamp":1585040415752,"user_tz":-330,"elapsed":2217,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# Get the vocab of the pretrained model\n","\n","# In our current vocabulary, it is very unlikely that the ids correspond to what is in the \n","# vocabulary used to train the pretrain model. The tokens are sorted by frequency (apart \n","# from the special tokens that are all first) so that order is specific to the corpus used. \n","# For instance, the word 'house' has different ids in the our current vocab and the pretrained one.\n","\n","old_vocab = pickle.load(open(path_wiki_model/'pretrained'/'vocab.pkl', 'rb'))\n","print(len(old_vocab))\n","\n","idx_house_new, idx_house_old = lm_imdb_vocab.index('house'),old_vocab.index('house')\n","idx_house_new, idx_house_old"],"execution_count":0,"outputs":[{"output_type":"stream","text":["60002\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(343, 230)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"zBWJBwIRMn0A","colab_type":"code","colab":{}},"source":["# We somehow need to match our pretrained weights to the new vocabulary. This is done \n","# on the embeddings and the decoder (since the weights between embeddings and decoders \n","# are tied) by putting the rows of the embedding matrix (or decoder bias) in the right order.\n","# It may also happen that we have words that aren't in the pretrained vocab, in this \n","# case, we put the mean of the pretrained embedding weights/decoder bias.\n","\n","def match_embeds(old_wgts, old_vocab, new_vocab):\n","  old_emb_wgts = old_wgts['0.emb.weight']\n","  old_decoder_bias = old_wgts['1.decoder.bias']\n","  old_emb_wgts_mean, old_decoder_bias_mean = old_emb_wgts.mean(dim=0), old_decoder_bias.mean()\n","\n","  new_emb_wgts = old_emb_wgts.new_zeros(len(new_vocab), old_emb_wgts.size(1))\n","  new_decoder_bias = old_decoder_bias.new(len(new_vocab))\n","  w2i = {word:i for i, word in enumerate(old_vocab)}\n","\n","  for new_i, word in enumerate(new_vocab):\n","    if word in w2i:\n","      old_i = w2i[word]\n","      new_emb_wgts[new_i] = old_emb_wgts[old_i]\n","      new_decoder_bias[new_i] = old_decoder_bias[old_i]\n","    else:\n","      new_emb_wgts[new_i] = old_emb_wgts_mean\n","      new_decoder_bias[new_i] = old_decoder_bias_mean\n","\n","  old_wgts['0.emb.weight'] = new_emb_wgts\n","  old_wgts['0.emb_dp.emb.weight'] = new_emb_wgts\n","  old_wgts['1.decoder.weight'] = new_emb_wgts\n","  old_wgts['1.decoder.bias'] = new_decoder_bias\n","\n","  return (old_wgts)\n","\n","# check that the word \"house\" was properly converted\n","\n","house_wgt  = old_wgts['0.emb.weight'][idx_house_old]\n","house_bias = old_wgts['1.decoder.bias'][idx_house_old]\n","\n","matched_wgts = match_embeds(old_wgts, old_vocab, lm_imdb_vocab)\n","\n","test_near(matched_wgts['0.emb.weight'][idx_house_new],house_wgt)\n","test_near(matched_wgts['1.decoder.bias'][idx_house_new],house_bias)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JWNDxOGWR2n","colab_type":"text"},"source":["### Step 6 - Load our model with the pre-trained weights, in preparation for retraining on IMDB"]},{"cell_type":"code","metadata":{"id":"eAtPCCSXLsGW","colab_type":"code","outputId":"53cde969-1276-4c36-9996-bf52fc169baf","executionInfo":{"status":"ok","timestamp":1585040611110,"user_tz":-330,"elapsed":1793,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["# These are the dropout probabilities \n","dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5\n","_lm_arch = lm_app.create_arch(*dps)\n","# !!!!!!! Think over whether this is the right way to do it\n","_lm_model = _lm_arch.model\n","_lm_model, _lm_model.state_dict().keys(), matched_wgts.keys()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(SequentialRNN(\n","   (0): AWD_LSTM(\n","     (emb): Embedding(17965, 300, padding_idx=1)\n","     (emb_dp): EmbeddingDropout(\n","       (emb): Embedding(17965, 300, padding_idx=1)\n","     )\n","     (inp_dp): RNNDropout()\n","     (lstm_layers): ModuleList(\n","       (0): WeightDropout(\n","         (module): LSTM(300, 300, batch_first=True)\n","       )\n","       (1): WeightDropout(\n","         (module): LSTM(300, 300, batch_first=True)\n","       )\n","     )\n","     (hidden_dps): ModuleList(\n","       (0): RNNDropout()\n","       (1): Identity()\n","     )\n","   )\n","   (1): LinearDecoder(\n","     (out_dp): RNNDropout()\n","     (decoder): Linear(in_features=300, out_features=17965, bias=True)\n","   )\n"," ),\n"," odict_keys(['0.emb.weight', '0.emb_dp.emb.weight', '0.lstm_layers.0.weight_hh_l0_raw', '0.lstm_layers.0.module.weight_ih_l0', '0.lstm_layers.0.module.weight_hh_l0', '0.lstm_layers.0.module.bias_ih_l0', '0.lstm_layers.0.module.bias_hh_l0', '0.lstm_layers.1.weight_hh_l0_raw', '0.lstm_layers.1.module.weight_ih_l0', '0.lstm_layers.1.module.weight_hh_l0', '0.lstm_layers.1.module.bias_ih_l0', '0.lstm_layers.1.module.bias_hh_l0', '1.decoder.weight', '1.decoder.bias']),\n"," odict_keys(['0.emb.weight', '0.emb_dp.emb.weight', '0.rnns.0.weight_hh_l0_raw', '0.rnns.0.module.weight_ih_l0', '0.rnns.0.module.weight_hh_l0', '0.rnns.0.module.bias_ih_l0', '0.rnns.0.module.bias_hh_l0', '0.rnns.1.weight_hh_l0_raw', '0.rnns.1.module.weight_ih_l0', '0.rnns.1.module.weight_hh_l0', '0.rnns.1.module.bias_ih_l0', '0.rnns.1.module.bias_hh_l0', '1.decoder.weight', '1.decoder.bias']))"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"554SaXdXi6-5","colab_type":"code","outputId":"fb7ded43-3f96-45a5-af42-7222a2b8b794","executionInfo":{"status":"ok","timestamp":1585040621239,"user_tz":-330,"elapsed":1654,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["#----------------------------------------------------\n","# So we rename the keys in the weights to correspond to the KD model module names\n","#----------------------------------------------------\n","\n","def rename_wgt_keys(model, wgts):\n","  renamed_wgts=OrderedDict()\n","  for old_key, new_key in zip(wgts.keys(), model.state_dict().keys()):\n","    renamed_wgts[new_key] = wgts[old_key]\n","    print (old_key, new_key, torch.all(torch.eq(renamed_wgts[new_key], wgts[old_key])))\n","  return (renamed_wgts)\n","\n","new_wgts = rename_wgt_keys(_lm_model, matched_wgts)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.emb.weight 0.emb.weight tensor(True, device='cuda:0')\n","0.emb_dp.emb.weight 0.emb_dp.emb.weight tensor(True, device='cuda:0')\n","0.rnns.0.weight_hh_l0_raw 0.lstm_layers.0.weight_hh_l0_raw tensor(True, device='cuda:0')\n","0.rnns.0.module.weight_ih_l0 0.lstm_layers.0.module.weight_ih_l0 tensor(True, device='cuda:0')\n","0.rnns.0.module.weight_hh_l0 0.lstm_layers.0.module.weight_hh_l0 tensor(True, device='cuda:0')\n","0.rnns.0.module.bias_ih_l0 0.lstm_layers.0.module.bias_ih_l0 tensor(True, device='cuda:0')\n","0.rnns.0.module.bias_hh_l0 0.lstm_layers.0.module.bias_hh_l0 tensor(True, device='cuda:0')\n","0.rnns.1.weight_hh_l0_raw 0.lstm_layers.1.weight_hh_l0_raw tensor(True, device='cuda:0')\n","0.rnns.1.module.weight_ih_l0 0.lstm_layers.1.module.weight_ih_l0 tensor(True, device='cuda:0')\n","0.rnns.1.module.weight_hh_l0 0.lstm_layers.1.module.weight_hh_l0 tensor(True, device='cuda:0')\n","0.rnns.1.module.bias_ih_l0 0.lstm_layers.1.module.bias_ih_l0 tensor(True, device='cuda:0')\n","0.rnns.1.module.bias_hh_l0 0.lstm_layers.1.module.bias_hh_l0 tensor(True, device='cuda:0')\n","1.decoder.weight 1.decoder.weight tensor(True, device='cuda:0')\n","1.decoder.bias 1.decoder.bias tensor(True, device='cuda:0')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i99ChSAlWmvC","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Load the pre-trained weights into our LM architecture using the renamed keys\n","#----------------------------------------------------\n","\n","_lm_arch.load_weights(new_wgts)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rg3Ke1P2WxZh","colab_type":"text"},"source":["### Step 6 - Tune the weights of the pre-trained Wikitext Language Model by retraining on IMDB, using discriminative Learning Rates"]},{"cell_type":"markdown","metadata":{"id":"Y8U0kPuzpHIU","colab_type":"text"},"source":["**Step A - Train with LSTM Layers frozen and split param groups (but same hyperparameters over time and across param groups)**"]},{"cell_type":"code","metadata":{"id":"UkmQN8-MQ2uV","colab_type":"code","outputId":"2288b39a-3f3b-4a90-e419-0a3b2742b86b","executionInfo":{"status":"ok","timestamp":1585040682039,"user_tz":-330,"elapsed":12417,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["lm_app.run_train(split_lr=[5e-3], split=True, one_cycle=False, freeze=\"FREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.697655</td>\n","      <td>0.231395</td>\n","      <td>4.287965</td>\n","      <td>0.256731</td>\n","      <td>00:11</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4X5QIyKnpg2Y","colab_type":"text"},"source":["**Step B - Train with LSTM Layers frozen and split param groups (hyperparameters vary over time due to 1 cycle but are the same across param groups)**"]},{"cell_type":"code","metadata":{"id":"JsLGcn4-SRws","colab_type":"code","outputId":"b44ca79f-d90c-431e-9106-680b111bee1c","executionInfo":{"status":"ok","timestamp":1585040694751,"user_tz":-330,"elapsed":12693,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["lm_app.run_train(split_lr=[2e-2], split=True, one_cycle=True, freeze=\"FREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.02}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.02}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.02}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.424740</td>\n","      <td>0.244092</td>\n","      <td>4.216343</td>\n","      <td>0.261604</td>\n","      <td>00:11</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","4 {'momentum': 0.7999745709300017, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.285763141529842e-06}\n","4 {'momentum': 0.7999745709300017, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.285763141529842e-06}\n","2 {'momentum': 0.7999745709300017, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.285763141529842e-06}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kp0bE7T8pvek","colab_type":"text"},"source":["**Step C - Train with all Layers unfrozen and split param groups (hyperparameters vary over time due to 1 cycle and also vary across param groups)**"]},{"cell_type":"code","metadata":{"id":"00kD_d7pSbWf","colab_type":"code","outputId":"e8502d57-698d-48a5-f332-ad4a7ea64fda","executionInfo":{"status":"ok","timestamp":1585040707019,"user_tz":-330,"elapsed":24942,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["lr_tmp = 2e-3\n","split_lr = [lr_tmp/2., lr_tmp/2., lr_tmp]\n","lm_app.run_train(split_lr=split_lr, split=True, one_cycle=True, freeze=\"UNFREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.001}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.001}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.002}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.224579</td>\n","      <td>0.256747</td>\n","      <td>4.177207</td>\n","      <td>0.266004</td>\n","      <td>00:12</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","4 {'momentum': 0.7999745709300017, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 2.6428815707657885e-07}\n","4 {'momentum': 0.7999745709300017, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 2.6428815707657885e-07}\n","2 {'momentum': 0.7999745709300017, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.285763141531577e-07}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"shRz6LwJIe4D","colab_type":"text"},"source":["**Save our model**"]},{"cell_type":"code","metadata":{"id":"mNcV6dDAImBW","colab_type":"code","colab":{}},"source":["! mkdir -p imdbtuned\n","path_imdb_tuned = Path.cwd()/'imdbtuned'\n","\n","_lm_arch.save_weights(path_imdb_tuned/'finetuned_enc.pth', path_imdb_tuned/'vocab_lm.pkl', path_imdb_tuned/'finetuned.pth')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gU9HvOEKNuX","colab_type":"text"},"source":["### Step 7 - Load IMDB Data and pre-process for Classification"]},{"cell_type":"code","metadata":{"id":"WrFQ-sK6I_n-","colab_type":"code","outputId":"094fb2a7-06bb-4ca2-efe7-e5b2a523891b","executionInfo":{"status":"ok","timestamp":1584685414143,"user_tz":-330,"elapsed":589830,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":57}},"source":["loaded=True\n","if (not loaded):\n","  vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))\n","  proc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(vocab=vocab),CategoryProcessor()\n","  il = TextList.from_files(path, include=['train', 'test'])\n","  sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n","  ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)\n","  pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))\n","ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))\n","vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))\n","bs,bptt = 64,70\n","data = clas_databunchify(ll, bs)"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [13/13 00:41<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      100.00% [13/13 00:38<00:00]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"xTWaPxzB1cnc","colab_type":"text"},"source":["### Define the Text Classifier Application class"]},{"cell_type":"code","metadata":{"id":"ZCHxspPm1Tmr","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Text Classifier Application\n","#----------------------------------------------------\n","class AppTextClassifier():\n","\n","  def __init__(self):\n","    self._arch = None\n","    self.tcdb = None\n","    self.vocab = None\n","    pass\n","\n","  def _load_vocab(self, vocab_path):\n","    vocab = pickle.load(open(vocab_path, 'rb'))\n","    return vocab\n","\n","  def load_data(self, data_path, vocab_path):\n","    self.vocab = self._load_vocab(vocab_path)\n","    self.tcdb = TextClassificationFolderDataBundle(data_path, bs=64, vocab_i2w=self.vocab)\n","    self.tcdb.do()\n","\n","  def create_arch(self, *dps):\n","    tok_pad = self.vocab.index(PAD)\n","    vocab_sz = len(self.vocab)\n","    emb_sz, n_h, n_layers, pad_idx, n_out, bptt = 300, 300, 2, tok_pad, 2, 70\n","    self._arch = ArchTextClassifier(vocab_sz, emb_sz, n_h, n_layers, pad_idx, n_out, bptt, *dps)\n","    return self._arch\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def run_train(self, split_lr, split=False, one_cycle=False, freeze=\"UNFREEZE_LSTM\", num_epochs=1):\n","    train_dl = self.tcdb.train_dl\n","    valid_dl = self.tcdb.valid_dl\n","    # NB: We don't use cross_entropy_flat for Classification\n","    loss_func = F.cross_entropy\n","\n","    # split_lr is a list:\n","    #   1. a single-element list [0.01] - same LR for all groups. \n","    #        If 'Split' is False, there is only one group. \n","    #        If 'Split' is True, there are multiple groups.\n","    #   2. a multi-element list [0.01, 0.03, 0.05] - discriminative LR for different groups. \n","    #        'Split' cannot be False\n","    assert(isinstance(split_lr, list))\n","    assert(len(split_lr) > 0)\n","    assert(not ((split == False) and (len(split_lr) > 1)))\n","\n","    # NB: We don't use the AwdLstmCB while doing Classification and we use accuracy not accuracy_flat\n","    callbs=[CudaCB(device = torch.device('cuda',0)), GradientClipping(clip=0.1), ProgressCallback(), MetricsCB({\"acc\": accuracy})]\n","    if (one_cycle):\n","      one_cycle_callbs = create_OneCycleCB(split_lr, phases=[0.5, 0.5], mom_start=0.8, mom_mid=0.7, mom_end=0.8)\n","      callbs = callbs + one_cycle_callbs\n","\n","    model = self._arch.model\n","    self._arch.freeze(freeze)\n","\n","    print ('BEFORE Hyper parameters')\n","    opt_func=adam_opt_func\n","    lr = split_lr[0]\n","    if (split and (len(split_lr) == 1)):\n","      hypers_group = [{}] * self._arch.n_splits\n","      opt_groups=(self._arch.splitter, hypers_group, {'lr': lr})\n","    elif (split and (len(split_lr) > 1)):\n","      hypers_group = [{'lr': lr_g} for lr_g in split_lr]\n","      opt_groups=(self._arch.splitter, hypers_group, {})\n","    else:\n","      opt_groups = None\n","    opt = get_optimiser(model, lr, opt_func, opt_groups)\n","\n","    loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs)\n","    loop.fit(num_epochs=num_epochs)\n","\n","    # TODO !!!!!!! Make _print_opt() a public function, maybe using repr\n","    print ('AFTER Hyper parameters')\n","    loop.opt._print_opt()\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def run_predict(self):\n","    x,y = next(iter(self.tcdb.valid_dl))\n","    pred_batch = self._arch.model.eval()(x.cuda())\n","    return (pred_batch)\n","\n","    # Predicting on the padded batch or on the individual unpadded samples give the same results.\n","    pred_ind = []\n","    for inp in x:\n","      length = x.size(1) - (inp == self.awd_lstm_enc.pad_idx).long().sum()\n","      inp = inp[:length]\n","      pred_ind.append(self.arch.eval()(inp[None].cuda()))\n","    assert near(pred_batch, torch.cat(pred_ind))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdYqMH0e4iHW","colab_type":"code","outputId":"aef45bef-ab71-45f6-9642-b8c4eb743734","executionInfo":{"status":"ok","timestamp":1585044666033,"user_tz":-330,"elapsed":7689,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["tc_app = AppTextClassifier()\n","tc_app.load_data(path_imdb, path_imdb_tuned/'vocab_lm.pkl')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--------- IMDB Classification DataBundle init /root/.fastai/data/imdb\n","FolderItemContainer loaded 50001 items of type TextFileItemList\n","Split using split_random into 1500, 1000 and 0 items of type TextFileItemList\n","Extracted 1500 items of type SentenceItemList using extract_doc\n","Extracted 1500 items of type ClassNameItemList using extract_custom\n","Converted 1500 items to type SentenceWordItemList using SentenceToWord\n","Converted 1500 items to type SentenceWordIdItemList using WordToWordId\n","Converted 1500 items to type ClassIdItemList using NameToId\n","Extracted 1000 items of type SentenceItemList using extract_doc\n","Extracted 1000 items of type ClassNameItemList using extract_custom\n","Converted 1000 items to type SentenceWordItemList using SentenceToWord\n","Converted 1000 items to type SentenceWordIdItemList using WordToWordId\n","Converted 1000 items to type ClassIdItemList using NameToId\n","Final SentenceWordIdItemList (1500 items)\n","[[2, 7, 8, 198, 81, 1805, 120, 19, 11, 2705, 844, 12, 11, 485, 717, 8, 1529, 148, 24, 0, 24, 3668, 24, 44, 24, 8, 24, 75, 24, 30, 24, 1836, 74, 24, 17, 134, 24, 544, 13, 7, 359, 5783, 1042, 124, 249, 9, 25, 18, 572, 61, 35, 157, 28, 165, 544, 13, 138, 12, 20, 5546, 752, 30, 19, 42, 23, 2274, 10, 268, 14, 126, 10, 12, 0, 26, 1230, 9, 25, 7, 2603, 7, 1514, 12, 7, 3283, 7, 10629, 325, 143, 317, 99, 89, 12, 120, 11, 72, 2663, 389, 9, 7, 8, 683, 198, 15, 1281, 24, 50, 8, 233, 24, 1582, 7, 3208, 7, 17279, 14, 8, 996, 7, 2179, 7, 8145, 9, 25, 7, 1384, 7, 2110, 97, 304, 42, 13, 8, 108, 245, 122, 18, 162, 140, 137, 98, 61, 17, 40, 576, 9, 7, 39, 346, 8, 31, 12, 587, 16, 222, 103, 11, 237, 288, 1334, 28, 4716, 70, 161, 968, 621, 14, 98, 9, 7, 20, 21, 960, 21, 15, 168, 329, 19, 31, 6569, 717, 429, 13, 114, 259, 9, 25, 7, 296, 178, 10, 70, 11, 31, 56, 11, 65, 1427, 13, 10538, 10, 16, 572, 2559, 9, 7, 19, 15, 11, 102, 500, 13, 20, 9, 7, 17, 19, 29, 10, 16, 22, 1042, 10, 249, 12, 495, 12, 16, 22, 89, 248, 9, 18, 261, 202, 19, 144, 8, 816, 24, 120, 21, 7, 32, 162, 7, 201, 7, 3446, 10, 21, 118, 271, 9, 7, 73, 1963, 9, 3], [2, 7, 37, 131, 11, 352, 13, 109, 410, 0, 18, 23, 72, 0, 14, 325, 19, 391, 9, 7, 115, 267, 270, 8, 14720, 208, 18, 23, 2673, 9, 21, 7, 9316, 21, 81, 1086, 14, 1834, 32, 17, 8, 391, 12, 43, 177, 13, 8, 1995, 20, 147, 123, 15, 17, 9, 7, 32, 41, 19, 1652, 16667, 10, 454, 8, 21, 7, 0, 7, 2011, 21, 20, 56, 322, 1561, 12, 9721, 32, 13, 792, 643, 59, 200, 14, 184, 45, 9, 7, 26, 11, 322, 4782, 21, 8, 0, 21, 32, 76, 85, 689, 200, 20, 776, 71, 43, 3947, 14, 32, 9, 7, 1270, 17, 11, 13406, 32, 1101, 942, 8, 2685, 2011, 0, 93, 21, 7, 0, 8, 2011, 11696, 45, 28, 500, 11, 6783, 9, 7, 12, 117, 755, 8, 0, 4782, 32, 76, 85, 67, 987, 12, 1236, 541, 33, 8, 6783, 9, 7, 306, 87, 377, 32, 14, 2173, 161, 51, 20, 15, 602, 27, 586, 9, 7, 101, 49, 487, 14, 1610, 19, 391, 17, 11, 190, 601, 76, 779, 57, 20, 78, 26, 8, 371, 13, 11868, 9, 7, 19, 391, 5694, 33, 8, 123, 755, 16009, 12, 1270, 3486, 222, 9, 7, 107, 51, 17, 118, 205, 29, 147, 5522, 38, 572, 194, 460, 9, 7, 12, 3670, 12, 723, 991, 45, 1481, 70, 32, 61, 35, 487, 119, 12, 76, 491, 32, 72, 1021, 9, 7, 54, 15, 19, 42, 154, 133, 32, 215, 14, 3316, 11, 659, 133, 32, 38, 3727, 160, 26, 167, 102, 929, 20, 16, 346, 432, 28, 32, 14, 930, 64, 560, 9, 7, 19, 23, 11, 154, 20, 105, 41, 243, 607, 58, 13, 8, 205, 365, 21, 7, 483, 7, 412, 21, 52, 7, 14, 648, 161, 51, 19, 15, 11, 158, 8171, 9, 7, 54, 38, 11, 182, 13, 722, 20, 202, 21, 7, 9316, 21, 14, 8, 549, 13, 8, 129, 365, 205, 116, 140, 902, 9, 7, 30, 1187, 18, 41, 14, 3909, 20, 54, 38, 67, 1885, 9, 7, 28, 42, 169, 8, 1995, 32, 38, 416, 17, 15, 654, 9, 7, 32, 372, 58, 17, 11, 211, 3750, 26, 44, 2530, 13, 1621, 10, 765, 6221, 12, 78, 11, 1621, 586, 4808, 454, 21, 7, 0, 21, 9, 7, 20, 15, 44, 527, 17, 8, 496, 9, 7, 30, 26, 44, 8, 11498, 221, 12, 67, 829, 3780, 17, 225, 20, 76, 530, 4768, 9, 7, 12, 54, 15, 73, 12514, 9, 7, 8, 391, 3740, 32, 0, 147, 5522, 9, 7, 53, 6771, 15, 2799, 9, 7, 97, 54, 38, 67, 3772, 5201, 20, 32, 41, 14, 61, 776, 32, 199, 35, 4045, 9, 18, 61, 35, 375, 3772, 33, 0, 17, 1081, 410, 2755, 9, 7, 30, 17, 109, 410, 4780, 20, 76, 43, 48, 632, 2313, 9, 7, 3296, 32, 76, 561, 45, 7437, 12, 0, 9, 7, 12, 1900, 87, 32, 110, 376, 16, 9, 7, 462, 21, 7, 9316, 21, 15, 48, 3845, 109, 410, 0, 20, 986, 14, 43, 270, 46, 118, 205, 59, 391, 352, 9, 3], [2, 7, 1147, 18, 163, 1187, 3777, 10, 18, 1874, 11, 7, 1672, 33, 7, 5587, 7, 1008, 401, 564, 173, 647, 12, 54, 23, 11, 231, 420, 10, 18, 163, 194, 283, 79, 23, 454, 7, 963, 7, 5369, 10, 45, 8, 150, 13, 16, 10, 12, 18, 1874, 19, 42, 33, 274, 1415, 20, 8, 420, 71, 43, 54, 107, 16, 23, 42, 13, 8, 1700, 200, 18, 162, 140, 137, 9, 7, 16, 22, 515, 106, 541, 50, 8, 1823, 15, 53, 193, 164, 10, 30, 185, 15, 3172, 92, 1823, 1977, 4291, 9, 7, 54, 23, 19, 1977, 470, 1087, 20, 685, 7, 963, 7, 5369, 33, 8, 1545, 307, 13, 19, 29, 115, 8, 738, 10, 12, 47, 209, 44, 1526, 51, 13371, 13566, 26, 73, 2812, 8841, 12, 811, 4067, 1452, 3944, 20, 38, 44, 0, 45, 8, 8034, 51, 7, 11129, 7, 9712, 9, 7, 64, 105, 120, 11, 1977, 470, 184, 69, 6068, 18, 199, 35, 783, 9, 25, 36, 1181, 34, 7, 45, 118, 1128, 10, 18, 23, 760, 17, 276, 28, 20, 420, 33, 8, 274, 307, 10, 53, 44, 18, 88, 23, 19, 1643, 4003, 17, 8, 7, 2417, 7, 8973, 207, 9, 7, 8, 29, 433, 13, 509, 143, 26, 8, 183, 334, 27, 177, 258, 10, 26, 8, 300, 123, 12846, 44, 3583, 13, 2665, 1764, 12, 117, 433, 13, 567, 58, 13, 11, 13651, 12, 1287, 355, 26, 8, 1779, 988, 9, 7, 13967, 7, 4257, 36, 7, 6165, 7, 7423, 34, 56, 11, 1672, 57, 8, 2872, 343, 20, 7, 3421, 7, 6194, 332, 14, 468, 17, 10, 117, 1065, 14, 8, 5246, 10, 8, 4166, 22, 4055, 533, 103, 7, 2417, 22, 988, 12, 1392, 60, 10, 12, 117, 68, 3779, 74, 2296, 17, 60, 5246, 267, 0, 60, 226, 0, 9, 7, 50, 54, 10, 8, 29, 587, 103, 8, 603, 2506, 1850, 4003, 9, 25, 1667, 7, 3812, 63, 7, 9993, 10, 1594, 13, 233, 416, 8, 96, 223, 17, 40, 581, 10, 23, 682, 14, 202, 33, 8, 217, 17, 19, 31, 27, 11, 370, 24, 223, 0, 10, 7296, 30, 0, 70, 8, 7528, 215, 14, 5871, 8, 1761, 11, 139, 9, 7, 37, 1293, 10, 39, 587, 17, 64, 15, 46, 8453, 12, 0, 8, 129, 241, 17, 8, 29, 9, 7, 7423, 297, 342, 33, 14, 530, 48, 4186, 290, 10, 30, 88, 37, 9601, 60, 125, 1559, 70, 68, 3162, 17, 19, 31, 9, 7, 8, 122, 17, 8, 29, 38, 641, 44, 7528, 45, 8, 2506, 1850, 20, 7, 13967, 15, 2397, 103, 115, 2064, 60, 0, 9, 7, 44, 13, 119, 38, 3237, 12, 0, 10, 239, 610, 3678, 20, 47, 81, 3928, 54, 45, 44, 9, 7, 947, 10, 47, 930, 12, 38, 456, 14, 2744, 8, 4329, 20, 47, 38, 44, 267, 1849, 57, 8, 183, 148, 12, 16, 22, 37, 55, 67, 259, 13, 470, 10131, 9, 25, 7, 10426, 7, 0, 56, 2702, 17, 60, 772, 217, 27, 7, 3421, 7, 6194, 10, 19, 75, 2367, 74, 103, 11, 963, 10333, 27, 11, 920, 13, 60, 1727, 2014, 1484, 7, 2417, 7, 8973, 9, 7, 37, 1293, 10, 68, 15, 456, 14, 1021, 1692, 14, 8, 3314, 7, 13967, 12, 8, 400, 13, 8, 7528, 10, 246, 68, 88, 3430, 690, 64, 47, 209, 180, 181, 9, 7, 54, 38, 67, 245, 1764, 17, 19, 4003, 10, 12, 8, 2548, 332, 28, 8, 322, 312, 41, 737, 11, 654, 2187, 9, 7, 54, 15, 11, 15472, 10, 6226, 24, 51, 7, 2417, 20, 543, 14, 8663, 7, 13967, 237, 10, 54, 15, 11, 154, 133, 11, 630, 587, 103, 7, 2417, 12, 26, 6518, 2818, 39, 3232, 74, 42, 13, 8, 7528, 54, 28, 67, 531, 327, 709, 12, 0, 60, 28, 1169, 113, 590, 14, 8, 253, 10, 30, 54, 15, 97, 48, 3191, 157, 24, 1318, 154, 133, 11, 389, 457, 41, 11, 545, 26, 7, 2417, 7, 8973, 22, 7797, 10, 79, 56, 100, 14898, 17, 8, 0, 13, 67, 575, 17, 11, 575, 0, 9, 7, 12, 42, 13, 8, 69, 9004, 24, 4906, 159, 23, 42, 133, 7, 2417, 3036, 42, 13, 8, 7528, 17, 40, 963, 36, 8, 42, 772, 28, 17415, 34, 10, 5889, 8, 12641, 12, 0, 58, 13, 40, 2818, 12, 4324, 12, 1030, 98, 222, 51, 11, 4160, 9, 7, 12072, 9, 25, 7, 91, 104, 11, 12987, 103, 7, 2417, 22, 534, 17, 19, 4003, 9, 7, 37, 80, 61, 91, 1004, 40, 419, 10, 30, 91, 97, 175, 58, 8, 2487, 20, 1374, 14, 7, 2417, 131, 0, 46, 69, 92, 1797, 0, 9, 7, 17, 1214, 7, 2417, 10, 8, 7528, 44, 1092, 326, 12, 10, 17, 82, 1849, 10, 371, 82, 322, 1561, 36, 108, 13, 79, 7443, 82, 5425, 17, 158, 135, 34, 14, 545, 98, 9, 7, 42, 1194, 10, 2924, 14, 11, 6779, 10, 15, 456, 14, 1212, 17, 40, 1849, 10, 168, 56, 8, 645, 1561, 13, 64, 39, 1631, 8, 1667, 7, 5133, 7, 1043, 10, 63, 168, 15, 1667, 337, 12, 96, 63, 36, 68, 56, 704, 13, 3768, 10, 60, 965, 1994, 74, 17, 11, 2813, 24, 294, 7, 0, 10, 12, 68, 56, 7190, 34, 9, 7, 955, 10, 30, 8, 29, 0, 70, 16, 56, 80, 42, 13, 8, 7528, 10, 8, 42, 49, 199, 35, 1126, 10, 37, 41, 118, 1561, 17, 40, 1849, 358, 8, 1436, 13, 8, 29, 10, 70, 39, 1101, 2241, 20, 39, 76, 804, 17, 40, 1849, 36, 45, 55, 8, 232, 639, 14, 561, 8, 271, 34, 9, 7, 40, 963, 571, 23, 11, 139, 113, 624, 14, 41, 100, 361, 58, 28, 20, 206, 10, 30, 9586, 10, 164, 32, 85, 156, 8, 29, 23, 11464, 7, 963, 7, 5369, 9, 25, 7, 4604, 10, 19, 15, 37, 48, 1135, 922, 3690, 103, 8, 207, 9, 7, 8, 125, 15, 194, 5647, 10, 30, 16, 22, 188, 194, 65, 28, 11, 205, 29, 9, 7, 3812, 7, 9993, 6316, 0, 8, 400, 13, 8, 198, 10, 6352, 425, 125, 1559, 438, 33, 17, 40, 576, 10, 12, 8, 29, 15, 37, 387, 11, 7548, 13, 382, 13, 8, 109, 128, 435, 11, 449, 20, 17594, 8, 7, 1752, 8, 5882, 116, 95, 69, 92, 8, 7, 5587, 7, 1008, 138, 9, 7, 8, 122, 38, 132, 1313, 240, 14, 1835, 28, 8, 297, 3723, 13, 95, 1191, 10, 79, 15, 156, 108, 13, 8, 2053, 243, 775, 69, 27, 1837, 713, 14, 491, 143, 309, 17, 11, 205, 31, 92, 8, 1315, 2089, 13, 8, 135, 13, 42, 13, 8, 122, 20, 91, 162, 243, 14, 142, 12, 5898, 28, 82, 3734, 144, 483, 9, 7, 30, 117, 178, 10, 37, 11, 182, 13, 205, 116, 202, 8, 75, 14, 81, 2680, 82, 122, 14, 8, 238, 133, 32, 1349, 74, 147, 988, 17, 4544, 70, 47, 38, 512, 10, 59, 38, 33, 8, 1487, 13, 147, 1613, 27, 47, 510, 28, 82, 484, 9, 7, 30, 16, 22, 637, 14, 825, 20, 8, 190, 205, 138, 20, 188, 61, 20, 38, 239, 0, 8, 129, 664, 1712, 3], [2, 7, 8, 738, 243, 50, 8, 7, 3825, 7, 1384, 0, 275, 20, 23, 112, 14, 533, 19, 29, 103, 7, 4225, 7, 0, 28, 816, 24, 674, 9, 7, 164, 20, 20, 22, 0, 74, 94, 422, 52, 7, 12417, 10, 803, 24, 0, 2029, 9, 7, 5199, 7, 1043, 8453, 8134, 33, 8, 1964, 197, 0, 0, 739, 46, 8, 7, 3533, 1542, 94, 646, 8, 0, 132, 361, 8, 6604, 539, 9, 7, 27, 167, 10, 19, 7, 1146, 577, 15, 11, 5409, 293, 14, 1829, 24, 143, 11, 891, 1057, 5409, 10, 26, 827, 1791, 9, 7, 208, 10, 19, 71, 212, 113, 139, 890, 14, 8, 166, 12, 945, 10, 49, 120, 73, 874, 1707, 14, 5161, 11, 5244, 130, 10, 2365, 10, 118, 9871, 13, 1036, 24, 1658, 2365, 9, 7, 2271, 0, 15, 73, 1030, 148, 10, 164, 59, 1538, 36, 7, 8827, 7, 8, 7, 4830, 7, 148, 1925, 20, 10, 62, 7, 0, 86, 35, 34, 9, 7, 55, 48, 4778, 408, 12, 11, 11196, 14, 126, 9, 3], [2, 18, 242, 155, 19, 15, 8, 129, 29, 18, 41, 140, 137, 9, 18, 142, 20, 33, 19, 4489, 16, 22, 1831, 80, 1855, 889, 10, 30, 28, 296, 18, 315, 51, 14, 43, 8, 65, 410, 49, 427, 8, 3482, 129, 29, 11, 65, 619, 9, 18, 175, 8, 29, 14, 43, 628, 193, 10, 12, 16, 56, 8, 402, 1427, 13, 280, 9, 16, 186, 87, 247, 44, 2530, 13, 1130, 70, 18, 126, 16, 10, 12, 18, 199, 35, 104, 240, 13, 16, 9, 8, 12300, 585, 9, 18, 134, 16, 10, 18, 421, 156, 101, 254, 37, 134, 16, 27, 95, 27, 18, 61, 10, 30, 18, 61, 35, 421, 106, 101, 105, 778, 16, 9, 7, 8, 29, 15, 1797, 1500, 4090, 9, 7, 13199, 15, 515, 9, 18, 132, 10273, 20, 7, 6441, 116, 105, 43, 53, 65, 10, 12, 18, 97, 132, 10273, 18, 105, 126, 42, 29, 10, 53, 121, 221, 17, 11, 697, 271, 234, 411, 1122, 13, 16, 9, 18, 127, 20, 1223, 149, 126, 16, 10, 758, 114, 14, 104, 11, 247, 13, 64, 135, 23, 51, 17, 8, 1222, 474, 10, 59, 62, 114, 14, 85, 64, 147, 17, 28, 70, 32, 2220, 8, 150, 13, 319, 1664, 9, 3], [2, 7, 0, 15, 48, 2226, 10, 30, 39, 15, 1896, 57, 16, 12, 1450, 17, 16130, 9, 7, 40, 135, 15, 525, 357, 12, 39, 90, 35, 142, 106, 14, 634, 16, 9, 7, 70, 40, 227, 464, 10, 7, 0, 10, 2108, 9, 7, 39, 721, 525, 1636, 12, 39, 876, 14, 157, 58, 14, 530, 2390, 9, 7, 70, 39, 15, 3397, 353, 10, 39, 4405, 26, 11, 264, 20, 1848, 12394, 9, 7, 305, 68, 7415, 98, 10, 39, 0, 17, 134, 12, 509, 14, 894, 60, 10, 13731, 9, 7, 89, 8, 496, 13, 8, 31, 15, 11, 236, 542, 10, 12, 76, 920, 357, 28, 108, 101, 9, 7, 208, 10, 27, 218, 698, 10, 16, 15, 11, 139, 145, 36, 37, 95, 52, 10, 301, 10, 8, 129, 177, 15, 8, 378, 256, 13, 363, 36, 239, 957, 288, 159, 57, 16, 34, 20, 32, 76, 85, 17, 8, 831, 13, 8, 29, 141, 16, 15, 37, 65, 28, 8, 0, 10, 30, 28, 8, 193, 12, 2478, 20, 15, 8, 154, 9, 7, 281, 16, 15, 8, 129, 13, 8, 31, 9, 7, 967, 8, 170, 1018, 8, 771, 15, 65, 10, 12, 8, 1791, 15, 11, 357, 31, 20, 20, 76, 920, 193, 28, 67, 101, 36, 37, 28, 87, 34, 9, 7, 44, 8, 31, 15, 465, 33, 1435, 1243, 36, 2226, 10, 27, 8, 31, 565, 34, 10, 20, 41, 251, 245, 9, 18, 51, 7, 7659, 7, 0, 10, 30, 18, 41, 14, 1185, 20, 19, 15, 37, 42, 13, 40, 129, 138, 36, 301, 40, 279, 52, 34, 10, 39, 800, 14, 61, 16, 145, 9, 3], [2, 18, 86, 35, 215, 14, 1001, 19, 29, 143, 33, 8, 889, 12, 1662, 17, 8, 911, 203, 10, 18, 398, 106, 340, 41, 47, 100, 57, 7, 2052, 507, 20, 56, 164, 530, 11, 8591, 7884, 13, 8, 7, 359, 1329, 723, 66, 7, 860, 7, 5032, 6921, 15, 194, 2253, 604, 10, 12, 39, 22, 112, 67, 245, 116, 17, 7, 4939, 9, 7, 53, 26, 48, 1045, 375, 18, 23, 1293, 0, 58, 46, 19, 29, 9, 25, 7, 54, 38, 65, 770, 9, 7, 8, 83, 15, 72, 955, 10, 755, 1708, 24, 7, 4809, 27, 11, 4328, 14, 716, 918, 8, 130, 20, 54, 38, 2209, 0, 317, 26, 82, 226, 2167, 13, 3482, 10, 12, 108, 1547, 32, 9, 7, 317, 75, 42, 13, 32, 15, 1899, 8, 400, 1742, 8, 1810, 12, 571, 3044, 119, 9, 7, 8, 334, 20, 309, 260, 390, 12, 12939, 530, 8, 80, 307, 13, 559, 17, 44, 8, 7, 0, 14, 175, 58, 62, 47, 530, 11, 7, 477, 9, 25, 7, 54, 22, 97, 7, 5032, 7, 8872, 10, 12, 39, 22, 37, 11, 96, 290, 12, 194, 0, 27, 11, 1634, 2117, 10, 860, 7, 1319, 7, 0, 49, 15, 48, 44, 1461, 65, 290, 9, 7, 27, 28, 8, 322, 312, 10, 67, 13, 119, 38, 81, 604, 10, 11, 5667, 13, 4440, 75, 10, 12, 542, 1318, 26, 1529, 2560, 10, 72, 604, 14, 126, 17, 1093, 9, 25, 7, 8, 96, 770, 66, 7, 89, 7, 0, 22, 1116, 15, 2914, 10, 12, 67, 13, 8, 312, 38, 35, 27, 7857, 27, 429, 10, 53, 16, 22, 228, 1689, 20, 330, 23, 1064, 33, 67, 13, 8, 300, 781, 12, 37, 33, 429, 20, 84, 281, 16954, 27, 113, 405, 33, 277, 59, 47, 55, 1356, 2710, 58, 13, 339, 9, 25, 11, 211, 0, 238, 28, 87, 15, 8, 590, 0, 356, 218, 20, 7, 359, 56, 206, 0, 10, 161, 20, 7, 3773, 7, 3455, 56, 453, 3409, 57, 9, 7, 1565, 545, 159, 590, 74, 2176, 128, 1437, 9, 7, 16, 427, 3459, 3814, 33, 11, 697, 2688, 59, 2681, 10, 255, 16, 184, 2628, 12, 69, 158, 92, 16, 81, 15, 10, 12, 16, 97, 7996, 64, 15, 180, 33, 222, 8, 356, 10216, 9, 7, 28, 500, 101, 2477, 11, 1170, 2986, 59, 8, 443, 13, 11, 2793, 1224, 10, 537, 9, 25, 7, 64, 7, 3455, 233, 320, 23, 20, 39, 895, 14, 1045, 58, 8, 356, 12, 136, 8, 318, 64, 23, 180, 33, 10, 306, 119, 85, 8, 101, 1214, 2703, 269, 92, 11, 590, 74, 13, 11, 443, 12, 11, 4973, 10, 2064, 14, 309, 1749, 103, 1872, 9, 7, 839, 8, 237, 408, 15, 69, 1247, 10, 12, 16, 22, 69, 1260, 9, 25, 7, 53, 8, 590, 0, 781, 84, 55, 69, 632, 92, 251, 10, 32, 1320, 14, 85, 8, 0, 13, 7, 8872, 22, 1214, 1559, 10, 12, 32, 265, 16, 252, 14, 85, 67, 13, 8, 218, 9, 7, 542, 8, 2283, 196, 12, 3442, 58, 8, 356, 7, 359, 7, 954, 52, 7, 8, 1027, 449, 23, 8, 83, 179, 10, 446, 267, 167, 11, 552, 4328, 33, 79, 14, 1803, 10, 47, 475, 14, 2550, 8, 303, 13, 8, 130, 12, 4538, 33, 8, 218, 159, 9, 7, 54, 38, 67, 554, 130, 1546, 12, 16807, 432, 20, 38, 55, 525, 3285, 12, 2846, 17, 8, 810, 13, 11, 190, 9322, 10, 266, 151, 105, 41, 5547, 67, 321, 12, 1364, 123, 1050, 9, 25, 7, 16, 55, 44, 1360, 326, 113, 15841, 10, 12, 37, 240, 23, 112, 13, 8, 83, 9, 7, 44, 17, 44, 10, 37, 11, 102, 29, 12, 16, 22, 1109, 23, 948, 3337, 9, 3], [2, 18, 440, 172, 19, 27, 11, 486, 17, 8, 2048, 10, 10930, 46, 8, 83, 12, 7, 5782, 7, 3618, 22, 3013, 9, 7, 91, 71, 804, 57, 185, 344, 45, 395, 8, 397, 271, 9, 18, 783, 8, 2883, 28, 8, 109, 839, 84, 654, 9, 7, 19, 15, 228, 387, 8, 129, 108, 16157, 591, 207, 446, 8, 213, 47, 88, 14, 485, 8, 83, 196, 14, 8, 5535, 47, 1369, 14, 2168, 53, 95, 9, 7, 64, 15, 245, 15, 20, 8, 3960, 13, 7, 3301, 12, 7, 2404, 7, 3903, 84, 9106, 266, 38, 6505, 0, 27, 167, 46, 7, 1275, 9, 7, 8, 207, 2890, 8, 7067, 225, 7, 0, 12, 8, 7, 299, 17600, 10, 8, 1753, 13, 7, 1132, 4921, 33, 8, 7, 4365, 962, 10, 8, 0, 13, 7, 969, 3266, 13, 295, 46, 7, 299, 4921, 9, 7, 8, 1517, 13, 8, 2864, 112, 87, 10, 11, 757, 24, 7, 2509, 7, 2009, 10, 247, 1538, 2761, 57, 8, 2023, 13, 8, 7, 2509, 101, 9, 18, 61, 35, 142, 156, 19, 15, 37, 638, 42, 17, 8, 1077, 0, 9, 7, 394, 47, 38, 839, 82, 6800, 458, 2459, 9, 7, 1180, 24, 91, 1347, 9, 3], [2, 18, 134, 7, 1062, 24, 7, 116, 12, 18, 134, 1998, 24, 2361, 9, 7, 12, 7, 2972, 7, 2225, 258, 509, 81, 1916, 26, 11, 370, 4203, 14, 7, 0, 7, 365, 21, 7, 0, 21, 9, 25, 7, 53, 18, 0, 160, 12, 8561, 14, 43, 1552, 9, 7, 843, 10, 67, 13, 8, 0, 38, 102, 28, 167, 11, 73, 339, 29, 12, 67, 170, 36, 8, 0, 12, 8, 2466, 34, 81, 917, 58, 10, 30, 344, 15130, 185, 9, 25, 18, 0, 487, 11, 171, 7, 4530, 146, 30, 54, 15, 37, 42, 756, 17, 8, 237, 29, 79, 56, 78, 11, 139, 236, 13, 833, 59, 1663, 1144, 9, 7, 8, 166, 283, 800, 106, 14, 815, 759, 0, 12, 245, 2092, 12585, 10, 30, 164, 36, 115, 128, 472, 34, 7688, 75, 14, 972, 161, 69, 9, 25, 7, 17, 0, 7, 667, 15068, 26, 2707, 24, 28, 87, 0, 37, 240, 24, 795, 9, 25, 195, 195, 58, 13, 4, 378, 195, 3], [2, 7, 799, 19, 62, 32, 51, 923, 10, 1374, 0, 59, 55, 215, 48, 515, 648, 9, 203, 669, 52, 18, 134, 19, 1092, 10, 12, 8, 274, 23, 78, 145, 92, 18, 88, 6667, 28, 10, 278, 87, 10, 62, 32, 536, 8, 75, 14, 388, 19, 6656, 10, 32, 376, 19, 274, 9, 875, 9, 3]...] \n"," ClassIdItemList (1500 items)\n","[1, 1, 0, 0, 1, 0, 0, 1, 0, 1...]\n","Final SentenceWordIdItemList (1000 items)\n","[[2, 7, 2622, 15, 515, 178, 94, 18, 41, 522, 530, 11, 7, 2622, 352, 9, 7, 19, 29, 15, 8, 7586, 7, 2622, 83, 9, 7, 114, 2699, 15, 812, 9, 18, 3919, 69, 172, 7, 8232, 30, 54, 38, 67, 801, 770, 9, 7, 14, 1324, 8, 1144, 13, 8, 250, 187, 1914, 16, 12, 14, 43, 456, 14, 155, 63, 18, 134, 32, 63, 38, 11, 190, 13, 8, 108, 1260, 5734, 17, 135, 94, 7, 16, 15, 1156, 10, 16, 15, 158, 94, 12, 188, 54, 15, 11, 158, 1724, 13, 19, 1056, 10, 7, 8907, 9, 18, 61, 35, 127, 168, 166, 105, 212, 151, 1130, 17, 167, 11, 111, 9, 25, 7, 5115, 12, 7, 0, 38, 97, 515, 94, 7, 5115, 15, 42, 13, 77, 3278, 246, 7, 1882, 9, 7, 61, 35, 487, 1054, 59, 95, 1103, 854, 94, 7, 62, 32, 38, 1789, 14, 443, 8, 6567, 13, 135, 10, 61, 35, 692, 19, 29, 9, 3], [2, 11, 182, 13, 200, 17, 19, 4490, 2203, 38, 69, 0, 193, 92, 188, 193, 36, 179, 16, 90, 41, 67, 501, 24, 58, 24, 1261, 432, 34, 30, 11, 182, 13, 20, 15, 107, 16, 220, 14, 41, 100, 485, 46, 8, 1159, 14, 145, 1444, 14, 690, 8, 2970, 16, 22, 0, 9, 7, 121, 1243, 38, 35, 1682, 14, 2680, 10, 54, 22, 624, 0, 13, 0, 857, 10, 12, 279, 13, 44, 11, 2955, 4679, 280, 36, 108, 13, 79, 742, 73, 2090, 34, 10, 281, 107, 67, 4970, 318, 86, 35, 421, 64, 23, 180, 33, 9, 7, 17, 99, 681, 10, 11, 29, 57, 0, 196, 56, 100, 94, 32, 8334, 16, 9, 25, 7, 42, 1794, 20, 11, 1228, 10, 145, 307, 13, 19, 249, 110, 947, 2352, 33, 274, 10, 12, 16, 110, 530, 8, 1234, 15396, 16, 986, 14, 43, 10, 30, 78, 17, 19, 0, 12, 708, 0, 1025, 24, 16653, 811, 16, 22, 42, 13, 8, 69, 862, 138, 13, 8, 0, 11, 0, 13, 3576, 458, 314, 1246, 12, 44, 8, 713, 16, 22, 530, 0, 10, 0, 12, 0, 17, 8, 392, 13, 2563, 14, 8, 0, 9, 18, 310, 16, 232, 115, 7, 1536, 13, 8, 7, 412, 10, 7, 740, 7, 4530, 22, 2327, 0, 13, 8, 697, 334, 20, 0, 5593, 1485, 10, 79, 15, 710, 8, 183, 238, 7, 1979, 15, 255, 141, 266, 133, 7, 4530, 22, 0, 0, 36, 164, 1485, 5593, 0, 20, 742, 14, 15916, 458, 8, 791, 34, 220, 5742, 58, 13, 1136, 10, 7, 1979, 22, 202, 15, 1161, 10, 412, 24, 33, 12, 263, 69, 1591, 9, 7, 55, 2473, 14, 8, 0, 17, 147, 29, 318, 0, 16, 74, 28, 7, 1783, 7, 0, 22, 7, 1263, 13, 8, 7, 3403, 55, 51, 82, 8680, 33, 277, 10, 12, 32, 261, 142, 20, 91, 209, 44, 3121, 9, 3], [2, 7, 19, 29, 15, 37, 72, 65, 9, 7, 17, 213, 10, 16, 15, 8, 279, 7, 4311, 29, 18, 41, 137, 9, 7, 16, 56, 72, 139, 130, 10, 641, 0, 10, 2845, 3510, 12, 1214, 9, 7, 0, 7, 14667, 12, 7, 1340, 7, 8431, 38, 993, 9, 18, 61, 35, 142, 156, 47, 86, 19, 29, 9, 7, 32, 105, 155, 7, 4311, 23, 993, 27, 89, 10, 39, 15, 95, 10, 95, 145, 17, 21, 7, 894, 7, 20, 7, 963, 9, 21, 3], [2, 7, 521, 7, 13960, 428, 27, 7, 1304, 7, 8350, 11, 1634, 2117, 49, 1886, 6295, 49, 38, 3090, 2987, 50, 8, 17667, 7, 1554, 10, 17, 19, 414, 266, 2695, 6446, 1634, 1765, 10578, 10, 79, 15, 0, 46, 113, 121, 206, 12402, 234, 218, 10, 30, 1532, 46, 67, 294, 1810, 218, 0, 27, 89, 27, 7, 1083, 7, 657, 63, 241, 9, 3], [2, 21, 7, 1231, 16, 33, 7, 8284, 21, 15, 11, 711, 249, 1145, 22, 406, 10, 26, 69, 92, 48, 803, 393, 13, 363, 481, 114, 8996, 995, 611, 75, 9, 25, 7, 8, 130, 3404, 128, 831, 24, 2217, 354, 17, 3946, 10, 42, 13, 921, 15, 0, 181, 40, 3291, 10, 8, 99, 1867, 26, 8, 4461, 13, 20, 183, 12502, 9, 7, 224, 65, 374, 10, 47, 924, 14, 202, 11, 3555, 17, 5039, 7, 8284, 794, 7, 0, 10, 317, 26, 11, 566, 45, 82, 598, 9, 7, 8199, 302, 17, 70, 42, 13, 119, 214, 547, 26, 8, 99, 22, 566, 9, 25, 7, 19, 1109, 4559, 13, 11, 83, 15, 898, 193, 12, 54, 38, 67, 65, 409, 10, 208, 16, 132, 81, 506, 645, 10, 27, 16, 105, 41, 9, 7, 118, 553, 45, 7371, 8, 1586, 1845, 790, 90, 35, 174, 382, 10, 12, 394, 166, 7, 0, 149, 41, 1460, 14, 8, 1290, 13, 8, 828, 9, 25, 7, 37, 11, 96, 31, 10, 30, 64, 81, 6497, 16, 15, 7, 2778, 7, 1599, 22, 373, 241, 27, 8, 12239, 139, 12399, 7, 2385, 9, 7, 153, 68, 1250, 60, 759, 14, 393, 2619, 10, 16, 22, 8, 80, 169, 68, 22, 201, 9, 7, 2778, 22, 125, 9719, 985, 11, 102, 859, 14, 43, 4308, 9, 7, 73, 656, 91, 41, 35, 137, 60, 17, 251, 344, 9, 25, 7, 1752, 10, 7, 0, 824, 10, 5454, 24, 7, 420, 3], [2, 7, 19, 31, 15, 69, 57, 106, 471, 120, 303, 13, 8, 203, 222, 119, 10, 12, 106, 47, 36, 12, 91, 34, 371, 7290, 14, 120, 303, 13, 16, 44, 9, 18, 127, 16, 22, 100, 0, 10, 324, 180, 17, 1022, 11, 0, 489, 35, 426, 16, 30, 62, 32, 215, 11, 2308, 83, 10, 16, 22, 146, 4, 824, 9, 3], [2, 7, 8, 253, 6840, 1631, 19, 29, 11, 867, 9, 7, 64, 15, 11, 867, 14, 87, 15, 106, 15, 16, 612, 20, 11, 1246, 20, 76, 2433, 167, 7693, 12, 1364, 8859, 231, 12, 585, 0, 76, 37, 2433, 11, 697, 31, 20, 71, 1934, 717, 8, 5540, 1420, 549, 19, 31, 53, 818, 2378, 9, 7, 19, 15, 7, 1477, 45, 114, 129, 124, 279, 10, 18, 1308, 76, 37, 377, 8, 1646, 9, 7, 0, 974, 10, 12083, 1541, 33, 192, 549, 10, 83, 24, 364, 10, 1243, 10, 672, 10, 231, 12, 5367, 9, 7, 14, 298, 16, 0, 10, 32, 242, 43, 11, 15191, 14, 426, 16, 9, 18, 310, 16, 14, 4298, 77, 4029, 3826, 10, 30, 54, 84, 221, 70, 18, 88, 14, 1212, 273, 50, 16, 10, 107, 18, 105, 37, 202, 16, 118, 69, 9, 7, 8, 80, 1780, 582, 13, 8, 29, 15, 8, 0, 1082, 13, 8, 1030, 1375, 9, 25, 3], [2, 7, 19, 15, 11, 81, 245, 29, 9, 7, 16, 15, 48, 218, 29, 26, 249, 1574, 17, 9, 7, 3252, 7409, 74, 26, 3656, 7, 0, 17, 19, 29, 14, 212, 16, 11, 1825, 4409, 9, 7, 16, 110, 383, 32, 1506, 64, 877, 180, 14, 636, 14, 7, 3252, 397, 9, 7, 16, 23, 11, 89, 336, 29, 10, 8, 166, 332, 8, 232, 3659, 17, 19, 0, 1268, 3659, 34, 14, 212, 16, 8, 232, 259, 13, 247, 9, 7, 0, 7, 16378, 97, 3162, 17, 19, 29, 12, 16, 15, 233, 11, 1814, 14, 85, 60, 33, 8, 211, 277, 9, 7, 68, 304, 60, 217, 89, 9, 7, 78, 7, 2140, 7, 2931, 15, 17, 19, 29, 9, 7, 16, 22, 285, 345, 16, 32, 41, 35, 137, 16, 9, 7, 16, 22, 413, 285, 267, 62, 32, 38, 11, 7, 2140, 7, 3252, 352, 9, 7, 16, 986, 69, 890, 92, 16, 15, 188, 366, 9, 3], [2, 7, 14, 1126, 2206, 10, 62, 42, 84, 80, 14, 85, 164, 7, 15356, 7, 0, 22, 21, 7, 10318, 21, 36, 3718, 10, 7, 0, 27, 21, 10953, 24, 336, 21, 34, 10, 115, 267, 137, 228, 11, 638, 13, 138, 20, 859, 26, 1008, 471, 10, 3838, 10242, 10, 351, 17, 1117, 36, 7, 14920, 22, 21, 7, 8, 0, 7, 3413, 21, 10, 7, 8597, 22, 21, 7, 2937, 7, 0, 21, 10, 7, 794, 7, 0, 22, 21, 7, 0, 21, 10, 7, 0, 22, 21, 7, 0, 10, 7, 14190, 52, 21, 10, 7, 0, 22, 21, 7, 0, 21, 10, 7, 0, 22, 21, 7, 0, 33, 11, 7, 11887, 21, 10, 537, 9, 34, 10, 42, 260, 43, 1637, 20, 8, 5181, 13, 8, 535, 1035, 17, 8, 31, 260, 37, 7288, 42, 1255, 10, 267, 100, 21, 794, 24, 0, 21, 522, 115, 180, 181, 8, 926, 4221, 24, 4832, 1241, 298, 17, 1318, 46, 8, 2606, 0, 138, 9, 25, 7, 3264, 10, 20, 489, 35, 43, 8, 439, 9, 7, 28, 7, 0, 15092, 40, 31, 17, 167, 11, 548, 24, 13, 24, 213, 1411, 36, 21, 0, 21, 10, 27, 42, 31, 2426, 298, 16, 10, 17, 11, 1103, 701, 34, 12, 20, 40, 1367, 486, 6625, 10, 7, 13991, 7, 0, 6733, 7, 0, 36, 1775, 173, 176, 45, 20, 75, 12, 11, 1008, 535, 355, 34, 10, 427, 167, 11, 73, 24, 0, 10, 3813, 241, 10, 2768, 17, 114, 6336, 36, 20, 7716, 443, 10, 165, 3602, 12, 10136, 644, 34, 20, 42, 15, 252, 298, 37, 14, 43, 0, 17, 118, 111, 9, 36, 7, 167, 11, 608, 254, 2379, 167, 11, 11797, 1978, 70, 42, 2718, 20, 8, 486, 88, 80, 1232, 30, 11, 405, 135, 10, 267, 100, 547, 17, 1008, 2450, 115, 8, 31, 12, 10109, 1795, 9, 34, 25, 7, 17, 57, 109, 335, 13, 8, 31, 10, 7, 10318, 12, 40, 1285, 1008, 351, 12, 10242, 1225, 82, 75, 17, 11, 0, 1263, 24, 510, 0, 395, 10, 133, 0, 12, 9008, 10, 1540, 12, 617, 10, 38, 8, 8760, 12, 0, 47, 38, 901, 14, 5822, 14, 82, 21, 2450, 21, 10, 33, 8, 8608, 5909, 20, 445, 8, 7, 0, 1069, 10, 0, 0, 38, 37, 21, 0, 21, 28, 82, 0, 9, 7, 28, 151, 351, 10, 8, 5810, 2600, 3590, 46, 8, 2047, 15, 69, 16426, 92, 8, 3219, 0, 2571, 46, 151, 463, 89, 24, 1370, 5135, 9, 7, 733, 8, 4076, 13, 19, 463, 15672, 12617, 10, 151, 216, 3539, 38, 631, 14, 2613, 20, 134, 12, 490, 50, 9240, 2098, 38, 9862, 1297, 14, 43, 265, 10, 62, 37, 14, 11, 2281, 0, 9, 25, 36, 7, 28, 7, 10318, 10, 8, 80, 811, 13, 1186, 272, 50, 0, 5769, 12, 16048, 9435, 10, 4539, 0, 1202, 8, 0, 9, 34, 25, 7, 70, 8, 351, 7377, 559, 103, 11, 380, 24, 2334, 21, 15916, 21, 14, 444, 2842, 12, 117, 10158, 82, 7070, 2267, 14, 104, 160, 14, 8, 932, 203, 189, 82, 21, 0, 2910, 12, 811, 38, 13, 167, 11, 259, 20, 42, 199, 35, 766, 513, 13, 8, 0, 63, 15916, 17, 7, 1452, 7, 0, 22, 10360, 21, 7, 1800, 28, 7, 5397, 21, 9, 7, 16, 22, 80, 20, 17, 21, 7, 10318, 21, 10, 8, 21, 0, 21, 15, 112, 14, 991, 33, 11, 11585, 549, 9, 25, 7, 296, 7, 10318, 12, 40, 380, 470, 38, 160, 33, 8, 2047, 36, 8, 31, 22, 349, 335, 34, 10, 47, 4671, 17, 5301, 10, 0, 12, 1040, 24, 1867, 14, 0, 28, 559, 10, 379, 79, 47, 104, 14, 1004, 7, 0, 10, 11, 12861, 30, 10138, 4582, 9, 7, 0, 10576, 0, 8, 731, 436, 3539, 10, 17, 167, 11, 111, 20, 68, 2762, 60, 8753, 14, 43, 8526, 46, 119, 12, 20, 68, 1581, 69, 92, 0, 490, 36, 45, 257, 14, 42, 13, 8, 471, 34, 9, 25, 7, 42, 71, 41, 204, 20, 8, 1008, 351, 41, 45, 206, 227, 265, 8, 42, 410, 49, 76, 1457, 119, 8, 134, 12, 6718, 20, 41, 100, 11631, 2318, 17, 82, 484, 9, 7, 30, 27, 5810, 27, 8, 2600, 20, 151, 351, 278, 8, 2047, 38, 3509, 10, 19, 171, 24, 265, 21, 0, 851, 21, 76, 37, 30, 842, 1538, 9, 25, 7, 6207, 10, 0, 10, 3835, 10, 12, 617, 41, 80, 302, 8, 351, 880, 189, 12, 28, 65, 9, 7, 12, 347, 20, 7241, 154, 133, 7, 10318, 10, 13605, 46, 8, 9460, 10, 214, 14, 1812, 37, 55, 7, 0, 22, 5471, 7, 299, 15296, 30, 97, 40, 1285, 1008, 0, 7, 8106, 36, 69, 92, 11, 528, 14, 7, 0, 34, 10, 39, 16217, 1555, 342, 160, 14, 21, 0, 21, 10, 27, 39, 2056, 50, 8, 232, 7546, 13, 8, 0, 262, 10, 232, 54, 12, 117, 0, 40, 12051, 2267, 28, 9240, 7300, 10, 8, 1585, 431, 224, 600, 12, 5203, 9, 25, 7, 16, 15, 53, 20, 7, 0, 10, 17, 11, 12405, 567, 160, 14, 60, 21, 4801, 21, 10, 0, 5286, 160, 7, 10318, 50, 40, 21, 0, 21, 2596, 12, 10250, 98, 10, 28, 65, 9, 7, 1446, 10, 17, 11, 5008, 5010, 639, 10, 7, 10318, 10, 26, 20, 216, 24, 176, 443, 12, 165, 10617, 644, 36, 37, 1135, 0, 10, 179, 17, 11, 288, 2565, 10, 14, 8, 216, 442, 22, 0, 17, 7, 0, 7, 0, 22, 7021, 21, 7, 243, 12, 7, 85, 21, 34, 10, 214, 355, 74, 10, 1848, 33, 40, 9155, 12, 346, 40, 1108, 36, 424, 10, 11, 1108, 52, 34, 10, 12, 706, 143, 14, 1297, 10, 1270, 379, 8, 992, 4890, 12, 26, 8, 1936, 701, 55, 496, 14, 136, 74, 9, 7, 26, 20, 154, 10, 7, 0, 254, 55, 43, 492, 48, 4203, 36, 3044, 121, 99, 0, 265, 17, 288, 138, 52, 34, 14, 8, 0, 466, 154, 17, 7, 14920, 22, 21, 7, 8, 0, 7, 3413, 21, 9, 25, 7, 30, 3043, 91, 201, 14, 142, 64, 56, 530, 13, 7, 0, 7, 0, 308, 173, 297, 17, 8, 405, 31, 21, 7, 0, 4736, 7, 0, 21, 36, 27, 89, 27, 17, 308, 99, 826, 138, 17, 8, 173, 16217, 34, 10, 91, 38, 361, 0, 17, 8, 460, 27, 14, 64, 1704, 1445, 28, 7, 10318, 115, 39, 444, 6110, 50, 8, 227, 1872, 10, 20, 131, 8, 227, 75, 20, 91, 261, 104, 14, 85, 19, 158, 24, 135, 1008, 486, 36, 7405, 8, 2416, 20, 947, 0, 98, 17, 12733, 34, 9, 25, 21, 7, 10318, 21, 254, 37, 43, 27, 868, 27, 14971, 27, 21, 7, 8, 0, 7, 3413, 21, 59, 27, 0, 27, 21, 7, 2937, 7, 0, 21, 10, 30, 16, 152, 1994, 58, 819, 138, 13, 801, 784, 12, 9561, 107, 13, 114, 680, 10, 2768, 571, 9, 3], [2, 7, 47, 320, 16, 71, 43, 11, 31, 3459, 92, 7, 17810, 7, 5767, 9, 7, 106, 4028, 25, 47, 66, 7, 16, 22, 37, 78, 191, 1500, 13, 19, 365, 9, 7, 96, 125, 9, 7, 8, 80, 123, 18, 494, 2438, 28, 23, 8, 42, 25, 270, 46, 7, 6516, 7, 0, 9, 7, 60, 125, 23, 8, 129, 17, 8, 237, 31, 9, 7, 8, 83, 105, 162, 100, 245, 10, 30, 16, 23, 35, 9, 7, 67, 159, 84, 72, 337, 746, 36, 1632, 12, 356, 34, 10, 36, 8, 25, 730, 154, 28, 500, 34, 10, 30, 8, 96, 125, 112, 8, 1377, 25, 8656, 9, 25, 18, 81, 61, 35, 421, 156, 53, 121, 101, 9016, 19, 31, 53, 25, 65, 9, 3]...] \n"," ClassIdItemList (1000 items)\n","[1, 1, 0, 0, 0, 1, 0, 1, 1, 0...]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-r4-NwBaBj1A","colab_type":"text"},"source":["### Step 8 - Build Classification architecture"]},{"cell_type":"markdown","metadata":{"id":"mJkCCFEmkP7M","colab_type":"text"},"source":["**Pooling Classifier Module**"]},{"cell_type":"code","metadata":{"id":"XHZa555go370","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Create a Linear Classifier with Pooling, which sits after the AWD-LSTM as a classification head of the model\n","#\n","# Instead of using only the last hidden value from the (last LSTM layer of the) AWD-LSTM for classification, Concat Pooling uses three \n","# things (from the last LSTM layer) viz. the last hidden value, the average of all hidden values and the maximum of all the hidden values \n","#----------------------------------------------------\n","class PoolingLinearClassifier(nn.Module):\n","\n","  # ----------------------------\n","  # The architecture consists of one or more sequential 'blocks' of layers\n","  # Each 'block' consists of a BatchNorm, Dropout, Linear and Relu layers\n","  # except the last 'block' which has no Relu\n","  #\n","  # 'layers_sz' is a list of input sizes of each block (and the output size of the last block). \n","  #     The output size of a block is the same as the input size of the next block.\n","  #     eg. [900, 50, 2] means two layer blocks ie. layer_1 is (900, 50) and layer_2 is (50, 2)\n","  #\n","  # 'drops_p' is a list of the dropout percentages of each block eg. [0.1, 0.1]\n","  # ----------------------------\n","  def __init__(self, layers_sz, drops_p):\n","    super().__init__()\n","\n","    # Number of blocks is one less than the length of the layer_sz list\n","    n_layer_blocks = len(layers_sz) - 1\n","    assert(n_layer_blocks > 0)\n","    assert(n_layer_blocks == len(drops_p))\n","    \n","    # Create the layers of each block\n","    layers = []\n","    for i in range(n_layer_blocks):\n","      n_in, n_out = layers_sz[i], layers_sz[i + 1]\n","      is_last = (i == (n_layer_blocks - 1))\n","      layers += self._create_block(n_in, n_out, drops_p[i], is_last)\n","\n","    # Build a single Sequential with all layers\n","    self.layers = nn.Sequential(*layers)\n","\n","  # ----------------------------\n","  # Apply Concat Pooling on the output value of the last LSTM layer. Then pass that to the linear\n","  # classifier layers\n","  # ----------------------------\n","  def forward(self, input):\n","    x = self._concat_pool(input)\n","\n","    # Now pass this through the classification layers\n","    x = self.layers(x)\n","    return x\n","\n","  # ----------------------------\n","  # From the output value of the last LSTM layer, find the last hidden value, average \n","  # hidden value and max hidden value and concatenate them.\n","  # ----------------------------\n","  def _concat_pool(self, input):\n","    out_vals, out_dp_vals, pad_mask = input\n","\n","    # Get the output value from the last LSTM layer, with shape [samples, timesteps, embedding size]\n","    lastlstm_out_dp = out_dp_vals[-1]\n","    # Calculate the actual sequence lengths without padding\n","    # The mask has shape [samples, timesteps] and \n","    # the sequence lengths has shape [samples]\n","    seq_lengths = lastlstm_out_dp.size(1) - pad_mask.long().sum(dim=1)\n","\n","    # Make sure that we ignore the padding in the last state/average/maximum.\n","    # Expand the mask to a third embedding dimension so it has shape [samples, timesteps, embedding size]\n","    fill_mask_3d = pad_mask[:, :, None]\n","\n","    # Use the fill mask to fill the pad values with 0, so they get ignored in the average calculation\n","    # Then sum all the values in each row, and divide by the number of values in each row (ie. the sequence length)\n","    # to get the average value for each row (ie. sample)\n","    # The average has shape [samples, embedding size]\n","    zero_fill_out_dp = lastlstm_out_dp.masked_fill(fill_mask_3d, 0)\n","    sum_out_dp = zero_fill_out_dp.sum(dim=1)\n","    numval_out_dp = seq_lengths.type(sum_out_dp.dtype)\n","    numval_out_dp = numval_out_dp[:,None]\n","    avg_out_dp = sum_out_dp.div(numval_out_dp)\n","\n","    # Use the fill mask to fill the pad values with neg-infinity, so they get ignored in the max calculation\n","    # Then get the max values in each row\n","    # The max has shape [samples, embedding size]\n","    neginf_fill_out_dp = lastlstm_out_dp.masked_fill(fill_mask_3d, -float('inf'))\n","    max_out_dp, _ = neginf_fill_out_dp.max(dim=1)\n","\n","    # Get the hidden value from the column for the last elements of each sequence\n","    # The last hidden value has shape [samples, embedding size]\n","    n_samples = lastlstm_out_dp.size(0)\n","    all_rowidxs = torch.arange(0, n_samples)\n","    lastval_colidxs = seq_lengths - 1\n","    lastval_out_dp = lastlstm_out_dp[all_rowidxs, lastval_colidxs]\n","\n","    # Concatenate the last value, average value and max value along axis 1\n","    # The concat pool has shape [samples, embedding size * 3]\n","    c_pool = torch.cat([lastval_out_dp, max_out_dp, avg_out_dp], 1)\n","\n","    return (c_pool)\n","\n","  # ----------------------------\n","  # Create the layers of a block\n","  # ----------------------------\n","  def _create_block (self, n_in, n_out, drop_p, is_last):\n","    layers = []\n","    layers.append(nn.BatchNorm1d(n_in))\n","    layers.append(nn.Dropout(drop_p))\n","    layers.append(nn.Linear(n_in, n_out))\n","    if (not is_last):\n","      layers.append(nn.ReLU(inplace=True))\n","\n","    return layers"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFivcUwskMpC","colab_type":"text"},"source":["**Sentence Encoder Module**"]},{"cell_type":"code","metadata":{"id":"iEKijsDfUENu","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Create a module which feeds our text sequences to the AWD-LSTM in chunks of bptt length\n","# so that we don't run out of memory by feeding all the data at once.\n","#----------------------------------------------------\n","class SentenceEncoder(nn.Module):\n","  def __init__(self, lstm_module, bptt, pad_idx=1):\n","    super().__init__()\n","    self.lstm_module = lstm_module\n","    self.bptt, self.pad_idx = bptt, pad_idx\n","    \n","  # ----------------------------\n","  # Wrapper over the AWD-LSTM to break the input sequences into chunks of bptt timesteps each\n","  # and then merge all the result values from each chunk, before returning them\n","  # ----------------------------\n","  def forward(self, input):\n","    bs, full_seq_len = input.size()\n","    self.lstm_module.bs = bs\n","    self.lstm_module.reset_state()\n","\n","    out_vals_lol, out_dp_vals_lol, pad_masks = [],[],[]\n","    for i in range(0, full_seq_len, self.bptt):\n","      # Get a chunk at a time, with all samples and bptt timesteps columns\n","      chunk = input[:, i: min(i+self.bptt, full_seq_len)]\n","\n","      # Feed a chunk to the LSTM, and append the outputs to result lists\n","      # The outputs ovs and odps are a list of values from each LSTM layer\n","      # So out_vals_lol and out_dp_vals_lol are list-of-lists (lol)\n","      # The LSTM may have truncated empty rows, so extend the outputs so they are all\n","      # of the required size\n","      ovs, odps, pm = self.lstm_module(chunk)\n","      pad_masks.append(self._pad_extend_rows(pm, bs, 1))\n","      out_vals_lol.append([self._pad_extend_rows(ov, bs, 0) for ov in ovs])\n","      out_dp_vals_lol.append([self._pad_extend_rows(odp, bs, 0) for odp in odps])\n","\n","    # Now convert the return value to the same format as returned by the AWD-LSTM ie.\n","    # outputs are a flat list of LSTM layer values, and a single pad mask\n","    return (self._val_flatten(out_vals_lol, bs),\n","           self._val_flatten(out_dp_vals_lol, bs),\n","           torch.cat(pad_masks, dim=1))\n","\n","  # ----------------------------\n","  # Flatten the list-of-lists (lol)\n","  #\n","  # The outer list is the chunks eg three chunks 'a', 'b', 'c'. The inner list is the LSTM layers eg. two layers '1', '2'\n","  #\n","  # We will flatten the list by concatenating all values for each LSTM layer from \n","  #    [[l1_a, l2_a], [l1_b, l2_b], [l1_c, l2_c]] to\n","  #    [l1_a + l1_b + l1_c, l2_a + l2_b + l2_c]                         \n","  # ----------------------------\n","  def _val_flatten(self, val_lol, bs):\n","    # Get the number of LSTM layers from the inner list\n","    n_inner = len(val_lol[0])\n","    # For each layer index, pick out the corresponding value for that layer from each inner list and concatenate them\n","    val_flat = [torch.cat([inner[i_inner] for inner in val_lol], dim = 1) for i_inner in range(n_inner)]\n","\n","    return (val_flat)\n","\n","  # ----------------------------\n","  # If an output tensor has fewer rows than the sample batch size (because the LSTM truncated \n","  # empty rows), add additional rows filled with 'val'\n","  # ----------------------------\n","  def _pad_extend_rows(self, ot, bs, val):\n","    if (ot.shape[0] < bs):\n","      # Create additional rows with the right shape filled with 'val'\n","      add_rows = ot.new_zeros(bs - ot.shape[0], *ot.shape[1:]) + val\n","\n","      # Concat the additional rows to the output tensor\n","      return torch.cat([ot, add_rows])\n","    else:\n","      return (ot)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VALkaT58kBFn","colab_type":"text"},"source":["**Classifier Architecture**"]},{"cell_type":"code","metadata":{"id":"zJbLWUEsj60-","colab_type":"code","colab":{}},"source":["\n","#----------------------------------------------------\n","# Create the end-to-end classification architecture with the AWD-LSTM (and Sentence \n","# Encoder) followed by the Pooling Classifier\n","#----------------------------------------------------\n","class ArchTextClassifier():\n","\n","  def __init__(self, vocab_sz, emb_sz, n_h, n_layers, pad_idx, n_out, bptt, out_p=0.4, hidden_p=0.2, \n","                        inp_p=0.6, emb_p=0.1, weight_p=0.5, layers_sz=None, drops_p=None):\n","    # Create the AWD-LSTM and Sentence Encoder\n","    self.awd_lstm_enc = AWD_LSTM(vocab_sz, emb_sz, n_h, n_layers, pad_idx, \n","                       emb_p, inp_p, weight_p, hidden_p, apply_packing=True)\n","    self.enc = SentenceEncoder(self.awd_lstm_enc, bptt)\n","\n","    # Initialise the layers_sz and drops_p for the Pooling Classifier\n","    if layers_sz is None: layers_sz = [50]\n","    if drops_p is None:  drops_p = [0.1] * len(layers_sz)\n","    n_in = 3 * emb_sz\n","    layers_sz = [n_in] + layers_sz + [n_out] \n","    drops_p = [out_p] + drops_p\n","\n","    self.plc = PoolingLinearClassifier(layers_sz, drops_p)\n","    self.model = SequentialRNN(self.enc, self.plc)\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def load_weights(self, weights_path):\n","    self.awd_lstm_enc.load_state_dict(torch.load(weights_path))\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def freeze(self, type):\n","    if (type == \"FREEZE_LSTM\"):\n","      for rnn in self.awd_lstm_enc.lstm_layers:\n","        for p in rnn.parameters(): p.requires_grad_(False)\n","    elif (type == \"UNFREEZE_LSTM\"):\n","      for rnn in self.awd_lstm_enc.lstm_layers:\n","        for p in rnn.parameters(): p.requires_grad_(True)\n","    elif (type == \"FREEZE_ENCODER\"):\n","      for p in self.enc.parameters(): p.requires_grad_(False)\n","    elif (type == \"UNFREEZE_ENCODER\"):\n","      for p in self.enc.parameters(): p.requires_grad_(True)\n","    elif (type == \"UNFREEZE_LAST_LSTM\"):\n","      for p in self.awd_lstm_enc.lstm_layers[-1].parameters(): p.requires_grad_(True)\n","\n","  # ----------------------------\n","  # ----------------------------\n","  n_splits = 4\n","  def splitter(self, type):\n","    awd_lstm_enc = self.awd_lstm_enc\n","\n","    # Embedding and Input Dropouts\n","    groups = [nn.Sequential(awd_lstm_enc.emb, awd_lstm_enc.emb_dp, awd_lstm_enc.inp_dp)]\n","\n","    # One group for each LSTM/Hidden Dropout pair\n","    for i in range(len(awd_lstm_enc.lstm_layers)): \n","      groups.append(nn.Sequential(awd_lstm_enc.lstm_layers[i], awd_lstm_enc.hidden_dps[i]))\n","\n","    # One group for the Pooling Classifier\n","    groups.append(self.plc)\n","\n","    # List of list of parameters by group\n","    return [list(o.parameters()) for o in groups]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SxvVA2WJhcbW","colab_type":"text"},"source":["### Step 9 - Train the Classifier with IMDB data"]},{"cell_type":"code","metadata":{"id":"aHBASuUI7zRE","colab_type":"code","outputId":"17ed0b1a-3422-4e25-d8b6-07d3b30f7830","executionInfo":{"status":"ok","timestamp":1585044676855,"user_tz":-330,"elapsed":14120,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":807}},"source":["torch.manual_seed(0)\n","\n","# These are the dropout probabilities\n","dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25\n","_tc_arch = tc_app.create_arch(*dps)\n","\n","# Load pre-trained AWD-LSTM weights\n","_tc_arch.load_weights(path_imdb_tuned/'finetuned_enc.pth')\n","\n","tc_app.run_train(split_lr=[1e-2], split=True, one_cycle=True, freeze=\"FREEZE_ENCODER\", num_epochs=1)\n","\n","lr_tmp = 5e-3\n","split_lr=[lr_tmp/2., lr_tmp/2., lr_tmp/2., lr_tmp]\n","tc_app.run_train(split_lr=split_lr, split=True, one_cycle=True, freeze=\"UNFREEZE_LAST_LSTM\", num_epochs=1)\n","\n","lr_tmp = 1e-3\n","split_lr=[lr_tmp/8., lr_tmp/4., lr_tmp/2., lr_tmp]\n","tc_app.run_train(split_lr=split_lr, split=True, one_cycle=True, freeze=\"UNFREEZE_ENCODER\", num_epochs=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","1 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n","8 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.592240</td>\n","      <td>0.682571</td>\n","      <td>0.634573</td>\n","      <td>0.668359</td>\n","      <td>00:01</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","1 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","5 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","5 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","8 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","BEFORE Hyper parameters\n","1 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0025}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0025}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0025}\n","8 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.471380</td>\n","      <td>0.778646</td>\n","      <td>0.532593</td>\n","      <td>0.727148</td>\n","      <td>00:02</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","1 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.261729121149329e-05}\n","4 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.261729121149329e-05}\n","4 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.261729121149329e-05}\n","8 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 8.523458242298658e-05}\n","BEFORE Hyper parameters\n","1 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.000125}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00025}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0005}\n","8 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.001}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.385024</td>\n","      <td>0.833984</td>\n","      <td>0.472160</td>\n","      <td>0.760547</td>\n","      <td>00:03</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.353268</td>\n","      <td>0.844215</td>\n","      <td>0.452019</td>\n","      <td>0.783398</td>\n","      <td>00:03</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","1 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.359408171751905e-07}\n","4 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 1.071881634350381e-06}\n","4 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 2.143763268700762e-06}\n","8 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.287526537401524e-06}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tlUoEmuhkeTN","colab_type":"text"},"source":["### Predictions"]},{"cell_type":"code","metadata":{"id":"bAThxPBOAEu2","colab_type":"code","colab":{}},"source":["tc_app.run_predict()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUisTN2gBoFR","colab_type":"code","outputId":"6de147fb-f6fa-49b5-92be-c86aadaf012c","executionInfo":{"status":"ok","timestamp":1585044721808,"user_tz":-330,"elapsed":1767,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":595}},"source":["_tc_arch.model"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SequentialRNN(\n","  (0): SentenceEncoder(\n","    (lstm_module): AWD_LSTM(\n","      (emb): Embedding(17965, 300, padding_idx=1)\n","      (emb_dp): EmbeddingDropout(\n","        (emb): Embedding(17965, 300, padding_idx=1)\n","      )\n","      (inp_dp): RNNDropout()\n","      (lstm_layers): ModuleList(\n","        (0): WeightDropout(\n","          (module): LSTM(300, 300, batch_first=True)\n","        )\n","        (1): WeightDropout(\n","          (module): LSTM(300, 300, batch_first=True)\n","        )\n","      )\n","      (hidden_dps): ModuleList(\n","        (0): RNNDropout()\n","        (1): Identity()\n","      )\n","    )\n","  )\n","  (1): PoolingLinearClassifier(\n","    (layers): Sequential(\n","      (0): BatchNorm1d(900, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (1): Dropout(p=0.10000000149011612, inplace=False)\n","      (2): Linear(in_features=900, out_features=50, bias=True)\n","      (3): ReLU(inplace=True)\n","      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): Dropout(p=0.1, inplace=False)\n","      (6): Linear(in_features=50, out_features=2, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"ywGHyIlY9NoV","colab_type":"text"},"source":["## Obsolete"]},{"cell_type":"code","metadata":{"id":"4-Knvz_VLVf_","colab_type":"code","colab":{}},"source":["path_imdb = datasets.untar_data(datasets.URLs.IMDB)\n","lmds_params = {'target_ds': FastaiLMDataset, 'bs': 64, 'bptt': 70}\n","lmdb = LanguageModelFolderDataBundle(path_imdb, lmds_params)\n","lmdb.do()\n","lm_imdb_vocab = lmdb.convert_state['vocab_i2w']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a_bNtpZNq4Tz","colab_type":"code","outputId":"373b4adf-a84b-47c1-8a8e-9fe508427e5f","executionInfo":{"status":"ok","timestamp":1584705440411,"user_tz":-330,"elapsed":109811,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["print (len(lmdb.convert_state['vocab_i2w']), len(lmdb.convert_state['vocab_w2i'].keys()))\n","lmdb.convert_state['vocab_i2w'][500], lmdb.convert_state['vocab_w2i']['history']"],"execution_count":0,"outputs":[{"output_type":"stream","text":["18189 41168\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["('son', 426)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rEsxiYlky63W","colab":{}},"source":["#----------------------------------------------------\n","#----------------------------------------------------\n","class ArchLanguageModel():\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def test_train(self, test_data, opt_func, split_lr, split, one_cycle, freeze, num_epochs=1):\n","    train_dl = test_data.train_dl\n","    valid_dl = test_data.valid_dl\n","    # NB: Use cross_entropy_flat for Language Model\n","    loss_func = cross_entropy_flat\n","\n","    # split_lr is a list:\n","    #   1. a single-element list [0.01] - same LR for all groups. \n","    #        If 'Split' is False, there is only one group. \n","    #        If 'Split' is True, there are multiple groups.\n","    #   2. a multi-element list [0.01, 0.03, 0.05] - discriminative LR for different groups. \n","    #        'Split' cannot be False\n","    assert(isinstance(split_lr, list))\n","    assert(len(split_lr) > 0)\n","    assert(not ((split == False) and (len(split_lr) > 1)))\n","\n","    # NB: Use AwdLstmCB for Language Model and accuracy_flat\n","    callbs=[CudaCB(device = torch.device('cuda',0)), AwdLstmCB(alpha=2., beta=1.), GradientClipping(clip=0.1), ProgressCallback(), MetricsCB({\"acc\": accuracy_flat})]\n","    if (one_cycle):\n","      one_cycle_callbs = create_OneCycleCB(split_lr, phases=[0.5, 0.5], mom_start=0.8, mom_mid=0.7, mom_end=0.8)\n","      callbs = callbs + one_cycle_callbs\n","\n","    self.freeze(freeze)\n","\n","    print ('BEFORE Hyper parameters')\n","    lr = split_lr[0]\n","    if (split and (len(split_lr) == 1)):\n","      hypers_group = [{}] * self.n_splits\n","      opt_groups=(self.splitter, hypers_group, {'lr': lr})\n","    elif (split and (len(split_lr) > 1)):\n","      hypers_group = [{'lr': lr_g} for lr_g in split_lr]\n","      opt_groups=(self.splitter, hypers_group, {})\n","    else:\n","      opt_groups = None\n","    opt = get_optimiser(self.arch, lr, opt_func, opt_groups)\n","\n","    loop = Trainer(train_dl, valid_dl, self.arch, opt, loss_func, callbs)\n","    #db.set_trace()\n","    loop.fit(num_epochs=num_epochs)\n","\n","    # TODO !!!!!!! Make _print_opt() a public function, maybe using repr\n","    print ('AFTER Hyper parameters')\n","    loop.opt._print_opt()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAwKeDM4tZNX","colab_type":"code","outputId":"26ff526d-22a7-429b-d73f-41c0b625b99b","executionInfo":{"status":"ok","timestamp":1584705470201,"user_tz":-330,"elapsed":139562,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["test_vocab = lm_imdb_vocab\n","test_data = lmdb\n","#test_data = subset_data\n","tok_pad = test_vocab.index(PAD)\n","\n","torch.manual_seed(0)\n","test_language_model = ArchLanguageModel(test_vocab, 300, 300, 2, tok_pad)\n","\n","test_language_model.test_train(test_data, opt_func=adam_opt_func, split_lr=[5e-3], split=False, one_cycle=False, freeze=\"UNFREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","12 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>6.506213</td>\n","      <td>0.079433</td>\n","      <td>6.039642</td>\n","      <td>0.085496</td>\n","      <td>00:18</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","12 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k2RC3CkVtlQy","colab_type":"code","outputId":"e0fbec18-58ec-4237-fc21-bfb44544eb20","executionInfo":{"status":"ok","timestamp":1584705501340,"user_tz":-330,"elapsed":12019,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":496}},"source":["#----------------------------------------------------\n","# The pre-trained weights are named after the module names in Fastai's model, which are slightly\n","# different than the module names in my KD model\n","#----------------------------------------------------\n","# These are the dropout probabilities \n","dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5\n","\n","arch_language_model = ArchLanguageModel(lm_imdb_vocab, 300, 300, 2, tok_pad, *dps)\n","arch_language_model.arch, arch_language_model.arch.state_dict().keys(), matched_wgts.keys()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(SequentialRNN(\n","   (0): AWD_LSTM(\n","     (emb): Embedding(18189, 300, padding_idx=1)\n","     (emb_dp): EmbeddingDropout(\n","       (emb): Embedding(18189, 300, padding_idx=1)\n","     )\n","     (inp_dp): RNNDropout()\n","     (lstm_layers): ModuleList(\n","       (0): WeightDropout(\n","         (module): LSTM(300, 300, batch_first=True)\n","       )\n","       (1): WeightDropout(\n","         (module): LSTM(300, 300, batch_first=True)\n","       )\n","     )\n","     (hidden_dps): ModuleList(\n","       (0): RNNDropout()\n","       (1): Identity()\n","     )\n","   )\n","   (1): LinearDecoder(\n","     (out_dp): RNNDropout()\n","     (decoder): Linear(in_features=300, out_features=18189, bias=True)\n","   )\n"," ),\n"," odict_keys(['0.emb.weight', '0.emb_dp.emb.weight', '0.lstm_layers.0.weight_hh_l0_raw', '0.lstm_layers.0.module.weight_ih_l0', '0.lstm_layers.0.module.weight_hh_l0', '0.lstm_layers.0.module.bias_ih_l0', '0.lstm_layers.0.module.bias_hh_l0', '0.lstm_layers.1.weight_hh_l0_raw', '0.lstm_layers.1.module.weight_ih_l0', '0.lstm_layers.1.module.weight_hh_l0', '0.lstm_layers.1.module.bias_ih_l0', '0.lstm_layers.1.module.bias_hh_l0', '1.decoder.weight', '1.decoder.bias']),\n"," odict_keys(['0.emb.weight', '0.emb_dp.emb.weight', '0.rnns.0.weight_hh_l0_raw', '0.rnns.0.module.weight_ih_l0', '0.rnns.0.module.weight_hh_l0', '0.rnns.0.module.bias_ih_l0', '0.rnns.0.module.bias_hh_l0', '0.rnns.1.weight_hh_l0_raw', '0.rnns.1.module.weight_ih_l0', '0.rnns.1.module.weight_hh_l0', '0.rnns.1.module.bias_ih_l0', '0.rnns.1.module.bias_hh_l0', '1.decoder.weight', '1.decoder.bias']))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"XbnorezyufMv","colab_type":"code","outputId":"af59135c-564d-418b-ffa3-f8306f40b35f","executionInfo":{"status":"ok","timestamp":1584705519324,"user_tz":-330,"elapsed":29949,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["arch_language_model.test_train(test_data, opt_func=adam_opt_func, split_lr=[5e-3], split=True, one_cycle=False, freeze=\"FREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.703729</td>\n","      <td>0.231917</td>\n","      <td>4.291876</td>\n","      <td>0.256696</td>\n","      <td>00:17</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7u3o718Ou0qg","colab_type":"code","outputId":"9fbb2551-ed01-40cc-d115-c23f94ab2b12","executionInfo":{"status":"ok","timestamp":1584705536739,"user_tz":-330,"elapsed":47339,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["arch_language_model.test_train(test_data, opt_func=adam_opt_func, split_lr=[2e-2], split=True, one_cycle=True, freeze=\"FREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.02}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.02}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.02}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.430527</td>\n","      <td>0.245284</td>\n","      <td>4.219647</td>\n","      <td>0.261421</td>\n","      <td>00:17</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","4 {'momentum': 0.7999748271191593, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.234525822385427e-06}\n","4 {'momentum': 0.7999748271191593, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.234525822385427e-06}\n","2 {'momentum': 0.7999748271191593, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.234525822385427e-06}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CBOsH6Q7vO2e","colab_type":"code","outputId":"0b4a30f7-a986-4ec8-80ff-1d7e127bc865","executionInfo":{"status":"ok","timestamp":1584705555924,"user_tz":-330,"elapsed":66502,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":236}},"source":["lr_tmp = 2e-3\n","split_lr = [lr_tmp/2., lr_tmp/2., lr_tmp]\n","arch_language_model.test_train(test_data, opt_func=adam_opt_func, split_lr=split_lr, split=True, one_cycle=True, freeze=\"UNFREEZE_LSTM\", num_epochs=1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.001}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.001}\n","2 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.002}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.229662</td>\n","      <td>0.257942</td>\n","      <td>4.181029</td>\n","      <td>0.265248</td>\n","      <td>00:19</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","4 {'momentum': 0.7999748271191593, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 2.617262911193581e-07}\n","4 {'momentum': 0.7999748271191593, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 2.617262911193581e-07}\n","2 {'momentum': 0.7999748271191593, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.234525822387162e-07}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qo7aXpMrOkE8","colab_type":"code","outputId":"7ceddbb4-77a9-4db3-f5d5-0d5fe6038ee4","executionInfo":{"status":"ok","timestamp":1584705683619,"user_tz":-330,"elapsed":7612,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["lm_imdb_vocab = pickle.load(open(path_imdb_tuned/'vocab_lm.pkl', 'rb'))\n","tlmdb = TextClassificationFolderDataBundle(path_imdb, bs=64, vocab_i2w=lm_imdb_vocab)\n","tlmdb.do()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--------- IMDB Classification DataBundle init /root/.fastai/data/imdb\n","FolderItemContainer loaded 50001 items of type TextFileItemList\n","Split using split_random into 1500, 1000 and 0 items of type TextFileItemList\n","Extracted 1500 items of type SentenceItemList using extract_doc\n","Extracted 1500 items of type ClassNameItemList using extract_custom\n","Converted 1500 items to type SentenceWordItemList using SentenceToWord\n","Converted 1500 items to type SentenceWordIdItemList using WordToWordId\n","Converted 1500 items to type ClassIdItemList using NameToId\n","Extracted 1000 items of type SentenceItemList using extract_doc\n","Extracted 1000 items of type ClassNameItemList using extract_custom\n","Converted 1000 items to type SentenceWordItemList using SentenceToWord\n","Converted 1000 items to type SentenceWordIdItemList using WordToWordId\n","Converted 1000 items to type ClassIdItemList using NameToId\n","Final SentenceWordIdItemList (1500 items)\n","[[2, 7, 8, 381, 13, 8, 7, 48, 22, 6149, 15236, 10, 7, 5185, 7, 1843, 10, 24, 617, 7, 1389, 9, 18, 276, 51, 130, 4877, 231, 7, 6502, 7, 1114, 11, 7, 5185, 7, 1843, 16, 19, 28, 9, 7, 6252, 7, 6891, 10, 48, 608, 14851, 10, 88, 216, 12, 67, 287, 16, 19, 43, 10, 11, 24, 475, 14, 483, 3365, 36, 7, 6502, 7, 1114, 16, 188, 483, 9, 7, 484, 10, 18, 930, 10, 7, 1976, 7, 281, 3201, 448, 188, 158, 9, 7, 17, 24, 12, 68, 21, 288, 21, 28, 10, 0, 8, 573, 14, 40, 12, 3827, 98, 166, 131, 9, 7, 91, 412, 361, 14, 1434, 12, 13814, 13, 19, 28, 36, 6381, 9, 7, 544, 386, 7, 6502, 7, 1114, 142, 19, 24, 165, 49, 693, 28, 9, 7, 94, 2499, 11, 5182, 213, 14, 8, 1187, 11, 13489, 127, 18036, 9, 7, 8, 516, 2245, 231, 7, 1114, 11, 7, 1976, 7, 281, 15, 261, 4289, 9, 3], [2, 7, 540, 267, 53, 7, 2107, 7, 0, 48, 58, 237, 145, 11, 145, 29, 188, 228, 9, 7, 26, 580, 39, 305, 8, 183, 13, 12, 68, 0, 123, 48, 192, 32, 221, 14, 2428, 614, 156, 0, 16, 4941, 27, 108, 702, 10, 30, 20, 970, 64, 7, 0, 15, 44, 60, 9, 25, 7, 8, 971, 83, 1999, 11, 690, 0, 11, 329, 13, 115, 3425, 46, 7, 2880, 9, 7, 8, 28, 24, 68, 5285, 285, 325, 137, 188, 123, 305, 12, 68, 2335, 11, 629, 183, 29, 7, 2880, 132, 8, 81, 1814, 749, 85, 15, 12, 653, 1046, 14, 8, 7, 182, 7, 582, 7, 0, 9, 25, 7, 11, 18, 206, 1069, 289, 19, 43, 162, 219, 12, 1703, 2558, 13, 10891, 14, 7, 16187, 7, 2110, 48, 0, 533, 9, 7, 559, 1274, 14, 44, 9, 3], [2, 18, 206, 34, 216, 1289, 157, 18, 190, 19, 51, 15947, 11, 21, 7, 8, 7, 0, 21, 37, 142, 17, 230, 42, 110, 8, 1000, 153, 15, 36, 44, 20, 24, 92, 29, 20, 31, 10, 161, 6179, 390, 20, 7, 14229, 78, 11696, 51, 17443, 9, 7, 52, 20, 31, 10, 17, 63, 12, 0, 543, 898, 10, 30, 19, 76, 17, 15, 34, 4949, 9, 7, 17, 22, 8, 252, 13, 31, 137, 180, 531, 30, 1987, 227, 4366, 52, 20, 9, 18, 129, 20, 212, 42, 606, 16, 183, 14, 8, 112, 8, 31, 1975, 29, 1967, 20, 449, 26, 84, 214, 2367, 23, 82, 9, 7, 52, 21, 7, 8, 7, 0, 10, 21, 17, 63, 12, 2435, 497, 48, 917, 61, 8, 164, 252, 13, 7277, 8801, 167, 168, 7, 0, 22, 267, 15, 69, 1510, 14, 985, 394, 14, 132, 5131, 10, 270, 16, 176, 103, 39, 63, 12, 79, 396, 12897, 20, 22, 741, 14, 2647, 9, 7, 17, 22, 412, 216, 353, 16, 8, 1939, 390, 9, 7, 17, 22, 8, 0, 13, 7, 14229, 22, 119, 10, 8, 7702, 13, 8, 2396, 152, 10, 20, 18, 190, 2039, 10, 26, 45, 5500, 2415, 13785, 29, 3480, 11382, 9, 7, 30, 153, 10, 20, 15, 183, 13, 8, 265, 167, 7, 0, 3727, 47, 12, 1125, 0, 9, 25, 7, 8, 112, 16, 85, 8, 2717, 13, 7, 0, 22, 4077, 15, 1680, 14, 226, 15, 216, 616, 167, 8, 371, 291, 12, 240, 10, 240, 10, 240, 10, 116, 19, 353, 10, 6902, 23, 2562, 407, 9, 7, 17, 22, 52, 49, 0, 14, 7, 2940, 22, 0, 10, 11, 17, 89, 34, 259, 1601, 23, 2989, 9, 35, 7, 17, 22, 37, 373, 8, 371, 3583, 7, 0, 1445, 10, 2201, 174, 11, 91, 80, 41, 198, 10, 0, 2546, 10, 20, 91, 1030, 39, 15, 37, 12, 407, 10, 30, 49, 4885, 167, 46, 104, 91, 202, 870, 14, 888, 7, 0, 22, 3671, 16, 49, 2450, 112, 142, 7, 14229, 212, 40, 579, 20, 10, 11, 39, 89, 34, 3971, 256, 53, 17, 9, 33, 7, 14229, 15, 667, 3817, 29, 7, 0, 142, 91, 105, 12, 293, 13, 8, 141, 10, 91, 144, 108, 9, 7, 17, 22, 12, 1837, 1207, 267, 10, 11, 71, 39, 89, 985, 41, 2010, 17, 22, 14, 15845, 6098, 142, 75, 566, 39, 504, 8, 11066, 10, 985, 23, 2562, 407, 14, 1353, 310, 1571, 9, 7, 54, 22, 12, 1220, 29, 7, 0, 35, 91, 140, 144, 108, 10, 58, 26, 91, 140, 144, 12, 3356, 13, 7, 7756, 0, 142, 30, 20, 22, 110, 41, 131, 15, 166, 10, 117, 214, 31, 33, 10, 30, 17, 22, 37, 26, 17816, 26, 17, 22, 101, 102, 14, 304, 35, 54, 15, 43, 158, 10, 207, 10, 137, 39, 0, 189, 12, 5025, 20, 22, 113, 94, 33, 9, 7, 0, 15, 34, 2792, 1089, 108, 36, 167, 39, 22, 292, 46, 108, 142, 7, 14229, 58, 11000, 16, 15, 44, 9, 25, 7, 8, 31, 89, 34, 1689, 7, 0, 26, 132, 1052, 167, 7, 0, 15, 34, 74, 79, 958, 10, 39, 22, 12, 0, 340, 13, 12, 407, 35, 5221, 10, 7286, 10, 0, 10, 29, 2204, 0, 33, 20, 329, 211, 14, 135, 35, 597, 43, 1806, 10, 975, 984, 215, 141, 48, 39, 0, 29, 33, 9, 7, 8, 4136, 45, 162, 14, 792, 64, 97, 42, 2706, 26, 12, 3967, 14, 7, 0, 22, 8282, 15, 8, 913, 10163, 158, 10, 85, 0, 41, 6327, 8410, 9, 25, 7, 17, 15, 34, 424, 1331, 167, 8, 235, 2015, 29, 7, 4582, 15, 12, 461, 10, 11, 66, 1594, 152, 10, 99, 5322, 9484, 16, 198, 344, 10, 97, 40, 101, 12643, 9, 7, 66, 517, 38, 1601, 23, 2989, 10, 165, 26, 71, 7, 0, 22, 1233, 2201, 16, 11, 0, 934, 1428, 108, 47, 167, 8, 328, 36, 41, 403, 38, 113, 587, 9, 35, 7, 30, 298, 8, 164, 158, 7, 0, 2670, 12, 1231, 10, 1794, 12, 191, 0, 10, 160, 51, 0, 20, 17, 192, 72, 27, 17, 9, 33, 7, 30, 54, 38, 66, 152, 167, 1382, 27, 8, 104, 76, 167, 20, 1848, 72, 12, 2491, 10, 1939, 4925, 9, 7, 0, 4910, 12, 1890, 621, 184, 11, 184, 10, 8, 2866, 0, 16, 8, 727, 10, 230, 42, 8, 136, 474, 13, 8, 9359, 8, 31, 63, 9, 7, 54, 15, 43, 6728, 20, 1159, 12, 6675, 18, 172, 270, 862, 103, 7, 78, 574, 42, 4501, 59, 17, 15487, 8, 3632, 10, 57, 89, 17, 40, 14, 162, 709, 115, 65, 35, 7, 91, 78, 1173, 7, 8541, 10, 48, 0, 210, 7, 14229, 11, 7, 11919, 10, 14, 42, 8, 2380, 474, 13, 319, 185, 709, 2420, 12442, 0, 9, 33, 18, 129, 19, 43, 860, 14, 56, 210, 9, 865, 127, 197, 3], [2, 18, 485, 19, 28, 107, 81, 196, 36, 99, 48, 83, 44, 585, 143, 47, 8, 1161, 4127, 13, 7, 2097, 13, 8, 7, 3870, 9, 7, 90, 10, 51, 18, 24, 9, 7, 11, 51, 18, 79, 135, 19, 28, 9, 7, 270, 18, 52, 44, 161, 5162, 0, 53, 12622, 132, 51, 911, 11, 0, 0, 9, 7, 111, 2225, 15, 8, 9620, 13, 7, 10429, 35, 8, 2987, 11, 1401, 11, 362, 0, 144, 23, 17, 23, 44, 4623, 33, 103, 7, 0, 10, 8, 17188, 11, 1343, 365, 0, 9, 7, 0, 22, 9620, 35, 21, 7, 0, 21, 53, 7, 3962, 7, 2041, 10, 13, 268, 33, 15, 1255, 12, 356, 10333, 29, 690, 1272, 26, 12, 8338, 9, 7, 0, 5103, 4728, 15, 266, 166, 1696, 6146, 2215, 48, 538, 14, 3396, 41, 2043, 2835, 35, 21, 7, 0, 21, 8, 1530, 33, 29, 0, 1652, 9, 7, 11, 21, 7, 0, 21, 35, 8, 9620, 13, 7, 0, 33, 858, 63, 66, 643, 3807, 29, 7, 2615, 7, 0, 50, 7, 11, 56, 34, 811, 7, 0, 11, 7, 0, 93, 3697, 23, 3697, 3], [2, 7, 19, 15, 43, 13, 8, 249, 121, 18, 40, 138, 126, 9, 7, 207, 10, 8, 139, 5516, 240, 10, 7, 3458, 11, 7, 0, 7, 2756, 15129, 7, 1749, 7, 9997, 15, 64, 192, 19, 28, 427, 9, 7, 3458, 22, 2517, 10, 2059, 11, 8199, 112, 13, 792, 72, 71, 55, 415, 15, 433, 15, 677, 11, 217, 9, 18, 40, 14, 6064, 7, 0, 27, 41, 7, 1749, 7, 9997, 5444, 10, 11, 7, 1749, 7, 9997, 27, 3377, 19, 14, 42, 16, 8, 28, 9, 3], [2, 7, 89, 264, 144, 64, 252, 13, 0, 7, 281, 2873, 5997, 65, 18, 328, 52, 12, 1978, 14, 623, 1352, 22, 7, 1667, 9, 7, 19, 28, 15, 77, 512, 26, 90, 26, 77, 307, 22, 9, 7, 17, 24, 8, 104, 1029, 28, 91, 245, 26, 12, 1157, 351, 9, 7, 8, 1224, 72, 15, 13, 576, 26, 17, 15, 662, 14, 8, 104, 2703, 18, 5997, 11, 1250, 276, 166, 52, 17, 9, 18, 70, 52, 14, 9795, 8, 1224, 72, 18, 40, 14, 5203, 8, 36, 16, 8, 28, 9, 7, 106, 8, 244, 24, 1226, 10, 11, 8, 134, 24, 92, 9, 7, 137, 11, 64, 15, 8, 770, 48, 1056, 7, 281, 22, 4627, 65, 7, 106, 88, 7, 281, 40, 12, 2684, 1338, 27, 8, 158, 36, 8, 8975, 71, 39, 238, 1646, 65, 7, 106, 64, 331, 13, 7, 0, 88, 7, 13329, 2604, 1095, 16, 19, 150, 9, 7, 17, 563, 52, 12, 1823, 22, 1565, 9, 7, 1218, 10, 3], [2, 18, 245, 19, 31, 275, 1944, 3147, 14, 185, 36, 12, 10680, 6011, 10, 51, 27, 86, 17, 24, 79, 58, 14, 105, 77, 358, 16, 5936, 27, 77, 6011, 9, 7, 8, 31, 6104, 66, 1155, 10680, 1759, 10, 533, 8, 119, 16, 8, 203, 152, 9, 7, 149, 390, 83, 92, 10, 12, 250, 52, 178, 1325, 2908, 1191, 9, 7, 1126, 8, 82, 24, 216, 1404, 378, 10, 8, 134, 24, 1286, 3581, 10, 29, 12, 1037, 417, 205, 9, 7, 19, 31, 15, 648, 222, 10, 46, 232, 17, 538, 14, 42, 3544, 10, 30, 17, 211, 52, 8, 222, 11, 8, 809, 24, 420, 47, 12, 2378, 9, 7, 8, 82, 1249, 118, 1099, 57, 1456, 11, 8, 222, 3974, 14, 42, 255, 69, 96, 12, 417, 0, 12, 201, 241, 298, 8, 28, 9, 7, 17, 258, 42, 294, 2826, 58, 14, 80, 8, 10680, 203, 10, 30, 116, 184, 54, 38, 1035, 13, 121, 61, 54, 20, 38, 424, 4154, 14, 10680, 2547, 11, 56, 34, 852, 12, 2870, 23, 169, 995, 420, 82, 9, 3], [2, 18, 40, 124, 68, 740, 5702, 3082, 19, 31, 9, 25, 370, 9, 7, 20, 77, 926, 1877, 1214, 14567, 64, 15, 68, 654, 12, 2046, 2414, 1464, 16, 8, 21, 664, 21, 1856, 9, 35, 7, 87, 18, 579, 64, 17, 24, 10, 18, 70, 37, 40, 791, 77, 76, 2826, 17, 16, 8, 104, 309, 9, 7, 11, 484, 10, 19, 28, 15, 12, 2046, 2414, 1464, 9, 33, 25, 275, 9, 7, 8, 419, 63, 180, 14, 56, 29, 8, 28, 9, 7, 75, 43, 16, 19, 28, 89, 255, 20, 15, 385, 10761, 57, 7564, 10, 289, 626, 12, 2365, 13, 8, 124, 9, 25, 7, 186, 10, 114, 20, 323, 740, 5702, 103, 25, 367, 9, 7, 20, 18, 27, 66, 321, 317, 8, 28, 198, 225, 14, 1832, 265, 633, 124, 36, 19, 872, 9, 7, 407, 56, 18, 2240, 20, 9, 7, 711, 256, 53, 19, 28, 9, 7, 878, 53, 77, 2054, 9, 7, 19, 28, 15, 0, 36, 2135, 188, 673, 9, 3], [2, 8606, 63, 14, 40, 12, 197, 61, 13, 197, 27, 19, 28, 26, 17, 63, 320, 9, 92, 82, 10, 92, 1223, 10, 92, 122, 11, 92, 4208, 9, 25, 19, 31, 1300, 2278, 0, 224, 36, 2101, 10367, 11, 17201, 1826, 9, 8606, 318, 36, 10017, 0, 12059, 16, 12, 6071, 10, 8606, 1921, 8, 639, 30, 64, 12, 639, 17, 15, 383, 13, 336, 10640, 203, 11, 646, 13, 222, 1093, 16, 9, 25, 66, 92, 11, 1382, 390, 16, 8, 31, 10, 0, 17200, 14035, 6114, 72, 16, 12, 2170, 11, 237, 550, 143, 23, 0, 9, 25, 8, 146, 722, 107, 40, 32, 5893, 27, 8, 10485, 1984, 10, 26, 117, 43, 13, 8, 136, 146, 1223, 18, 155, 126, 9, 8606, 318, 36, 3896, 13, 434, 11, 635, 72, 0, 72, 14, 2101, 10367, 9, 3], [2, 18, 172, 1914, 20, 44, 8, 21, 1709, 17, 21, 3648, 38, 0, 11, 0, 10, 148, 605, 954, 9, 7, 14, 86, 10, 19, 211, 12, 404, 13, 21, 8, 3230, 63, 75, 2022, 21, 9, 18, 414, 19, 31, 24, 937, 36, 12, 68, 392, 348, 16, 8, 441, 14926, 93, 7, 3213, 10, 17, 771, 12, 1881, 14, 1106, 170, 11, 128, 9, 7, 8, 335, 18, 245, 88, 40, 66, 2788, 10, 30, 60, 6514, 1398, 13, 8, 1916, 15, 37, 12875, 9, 7, 66, 13, 17, 15, 253, 14, 414, 9, 7, 8, 7, 13348, 0, 24, 564, 14, 889, 10, 30, 32, 1881, 37, 14, 62, 4581, 61, 62, 120, 4608, 9, 7, 66, 13, 8, 781, 83, 359, 10, 11, 8, 1472, 24, 54, 10, 74, 59, 66, 13, 8, 388, 83, 37, 0, 7, 4302, 88, 12, 67, 287, 33, 7, 8, 125, 15, 37, 95, 10, 30, 216, 834, 9, 7, 16, 8, 0, 31, 10, 8, 2587, 38, 7, 2273, 0, 33, 1134, 27, 12, 158, 10, 11, 12, 351, 13, 1055, 23, 5790, 11, 12, 639, 9, 7, 254, 1230, 227, 57, 51, 9, 7, 8, 386, 15, 195, 339, 9, 10304, 10, 802, 8, 478, 13, 8, 31, 10, 54, 38, 66, 977, 2070, 29, 8, 7, 7185, 123, 20, 304, 52, 12, 724, 2054, 10, 85, 102, 86, 491, 27, 12, 250, 9, 7, 43, 2783, 290, 19, 31, 63, 101, 603, 51, 130, 241, 10, 20, 54, 38, 201, 6210, 13, 8, 223, 5903, 1690, 340, 61, 54, 9, 7, 8, 286, 24, 252, 13, 217, 10, 792, 109, 8, 1559, 53, 12, 215, 535, 10597, 99, 10, 17, 97, 106, 42, 12, 796, 742, 36, 8, 443, 141, 22, 127, 0, 22, 2976, 36, 7, 6743, 9, 7, 100, 299, 1887, 38, 5281, 10, 5149, 10, 2284, 10, 9080, 93, 7, 16, 2456, 10, 8, 1017, 15, 480, 3685, 10, 8, 31, 94, 423, 9, 3]...] \n"," ClassIdItemList (1500 items)\n","[1, 1, 1, 1, 0, 1, 0, 0, 1, 0...]\n","Final SentenceWordIdItemList (1000 items)\n","[[2, 7, 19, 24, 12, 674, 150, 20, 18, 317, 298, 8, 3053, 26, 12, 562, 11, 24, 8912, 47, 17, 9, 7, 16, 8, 130, 173, 20, 40, 2805, 10, 18, 40, 412, 229, 60, 19, 150, 11, 109, 67, 17, 70, 42, 14, 128, 17, 184, 10, 30, 132, 0, 20, 204, 13, 8, 523, 38, 37, 239, 26, 67, 26, 32, 496, 115, 9, 7, 19, 24, 12, 92, 150, 16, 8, 3053, 11, 63, 406, 180, 10, 74, 171, 16, 4169, 9, 7, 271, 54, 38, 12, 201, 1241, 357, 3135, 20, 32, 80, 71, 178, 177, 291, 10, 11, 17, 318, 12, 351, 13, 939, 14, 105, 327, 14, 8, 2009, 2840, 10, 30, 269, 247, 20, 10, 18, 24, 26, 8912, 171, 26, 18, 24, 174, 16, 8, 3053, 9, 7, 8, 7, 2828, 15, 284, 47, 7, 0, 7, 0, 10, 48, 305, 8, 228, 16, 12, 68, 5699, 1424, 9, 18, 496, 298, 8, 3878, 10, 606, 14, 8, 5554, 13, 8, 150, 153, 16, 7, 2213, 10, 39, 0, 4808, 2835, 301, 569, 10, 11, 24, 9431, 47, 8, 576, 16, 8, 150, 11, 16, 108, 9, 7, 17, 24, 92, 20, 43, 13, 77, 44, 76, 1714, 291, 148, 1525, 8, 1592, 20, 17, 88, 298, 77, 2557, 9, 12, 212, 80, 218, 9, 3], [2, 18, 40, 34, 5917, 255, 36, 153, 27, 216, 4608, 10, 30, 120, 269, 8, 7552, 14, 1106, 170, 19, 1488, 0, 26, 869, 18, 87, 14, 285, 905, 77, 2767, 9, 25, 7, 1720, 10, 207, 95, 160, 15, 10, 54, 15, 43, 2151, 183, 10, 775, 17, 15, 49, 314, 48, 24, 784, 10, 12, 158, 20, 24, 3848, 10, 49, 1705, 205, 1597, 10, 57, 12, 796, 0, 592, 9, 7, 489, 10, 20, 15, 34, 302, 153, 10, 26, 8, 31, 509, 13, 0, 10, 29, 12, 641, 8020, 10, 85, 24, 51, 398, 10, 18, 229, 17, 24, 12, 3602, 10, 163, 124, 0, 10, 1101, 579, 26, 8, 934, 16, 948, 9175, 5151, 12, 402, 9, 25, 7, 43, 13, 149, 4932, 24, 7, 3257, 7, 5116, 10, 48, 37, 81, 2654, 16, 19, 10, 30, 106, 186, 7, 561, 19, 1738, 10, 163, 1044, 41, 1051, 78, 37, 449, 27, 0, 9, 7, 37, 20, 39, 24, 36, 41, 214, 153, 10, 26, 329, 16, 17, 431, 14, 3882, 14, 8, 134, 402, 13, 12, 4027, 11727, 9, 25, 7, 3649, 10, 451, 27, 86, 10, 18, 81, 317, 49, 488, 13, 19, 881, 10, 26, 8, 335, 70, 34, 196, 10, 163, 24, 254, 102, 47, 8, 164, 5815, 48, 937, 19, 9, 7, 51, 59, 32, 40, 180, 14, 56, 30, 128, 2429, 1786, 56, 128, 19, 26, 17, 15, 58, 26, 339, 10, 163, 4711, 27, 391, 2631, 13, 7904, 2608, 9, 3], [2, 7, 8, 7, 526, 7, 649, 7, 488, 2562, 124, 3872, 103, 7, 521, 7, 336, 11, 7, 0, 9, 289, 22, 366, 29, 7, 521, 7, 336, 9, 25, 7, 521, 7, 336, 24, 1685, 14, 42, 12, 191, 7, 0, 1082, 10, 1926, 84, 2442, 11950, 103, 7, 0, 8, 7, 927, 35, 7, 0, 33, 10, 7, 1238, 7, 521, 35, 7, 0, 33, 10, 7, 0, 35, 7, 2476, 33, 10, 11, 7, 0, 35, 7, 16353, 33, 9, 7, 207, 10, 7, 0, 97, 34, 105, 8, 2939, 14, 56, 17, 51, 45, 0, 17, 12, 250, 11, 389, 72, 29, 19, 9, 7, 153, 22, 8, 0, 13, 122, 103, 7, 1238, 7, 1820, 35, 7, 0, 33, 10, 7, 12956, 7, 6063, 35, 7, 2476, 33, 10, 7, 0, 35, 7, 0, 33, 10, 7, 1423, 7, 0, 35, 7, 17909, 33, 10, 7, 8246, 7, 6006, 35, 7, 16353, 33, 10, 7, 3783, 7, 0, 35, 7, 1651, 0, 33, 11, 7, 763, 7, 0, 35, 7, 763, 7, 0, 33, 9, 7, 8, 150, 2562, 453, 203, 7221, 10, 26, 90, 26, 1082, 2711, 9, 7, 8, 486, 24, 8, 767, 1015, 7, 0, 762, 10, 30, 46, 232, 17, 87, 12, 293, 13, 262, 60, 17, 9, 7, 17, 24, 427, 225, 27, 346, 10, 168, 37, 216, 72, 14, 839, 2002, 9, 25, 7, 8, 100, 1951, 24, 7, 0, 50, 10, 85, 24, 68, 3039, 14, 8, 3056, 10, 1044, 8, 839, 453, 203, 150, 9, 7, 44, 13, 8, 740, 1627, 102, 3690, 10, 26, 90, 26, 8, 432, 7, 10280, 7, 277, 35, 597, 7, 0, 7, 10280, 10, 16, 46, 232, 43, 477, 33, 9, 7, 1308, 7, 6502, 87, 8, 2160, 0, 7, 2339, 624, 11, 12199, 123, 10, 26, 90, 26, 41, 7376, 134, 72, 21, 7, 0, 21, 9, 7, 14616, 7, 0, 10, 12, 2196, 13, 1352, 22, 303, 291, 10, 52, 7, 3398, 10, 5819, 8, 7, 220, 7, 916, 7, 2653, 10, 154, 7, 3460, 7, 0, 35, 7, 2772, 7, 3343, 16, 8, 441, 853, 7, 1526, 7, 3044, 7, 141, 939, 11, 624, 13, 7, 0, 16, 7, 39, 23, 7, 141, 33, 2642, 7, 1072, 7, 0, 9, 25, 7, 8, 7, 0, 87, 239, 87, 12, 293, 13, 0, 14, 115, 10, 51, 139, 0, 24, 2679, 27, 84, 2711, 10, 14, 1001, 8, 7, 3207, 7, 2002, 11, 7, 16862, 9949, 35, 8, 0, 33, 9, 7, 26, 165, 10, 8, 529, 83, 68, 4171, 11, 4698, 9, 7, 3291, 8, 1627, 48, 2129, 83, 103, 7, 1072, 7, 0, 10, 7, 308, 7, 1668, 10, 7, 520, 7, 13152, 10, 7, 520, 7, 358, 10, 7, 4627, 7, 0, 10, 7, 0, 10, 11, 8, 5574, 1927, 9, 7, 520, 7, 0, 7, 0, 106, 102, 41, 36, 279, 1845, 9, 25, 7, 521, 7, 336, 15, 606, 14, 42, 773, 53, 0, 10, 30, 7, 0, 50, 15, 148, 16, 14095, 10, 26, 90, 26, 8, 453, 23, 203, 150, 9, 7, 2988, 10, 8, 15489, 28, 107, 405, 3263, 115, 1672, 36, 335, 9, 7, 17, 15, 90, 294, 178, 11, 8217, 13, 335, 2421, 9, 3], [2, 7, 64, 147, 40, 101, 12, 3311, 17887, 13313, 46, 12, 17233, 2155, 315, 554, 98, 12, 369, 13, 2400, 10, 26, 336, 402, 1553, 7, 4916, 7, 2947, 35, 7, 0, 7, 11870, 33, 4081, 6511, 1647, 1899, 53, 12, 3839, 3554, 10, 154, 295, 14, 711, 43, 1397, 1691, 13, 108, 9, 25, 7, 8, 104, 976, 227, 13, 8, 223, 31, 83, 195, 67, 30, 17, 24, 44, 4780, 53, 54, 9, 7, 8, 891, 318, 161, 104, 976, 227, 11, 13156, 115, 98, 49, 1096, 798, 852, 31, 9, 7, 20, 22, 12, 67, 374, 110, 117, 16702, 320, 20, 102, 8, 223, 95, 9, 7, 207, 10, 59, 45, 519, 19, 31, 14, 196, 69, 2632, 116, 45, 147, 40, 2739, 12, 145, 497, 506, 10, 145, 182, 10, 596, 569, 9, 7, 54, 22, 75, 664, 10, 320, 78, 42, 2376, 61, 198, 175, 17, 531, 11, 17, 22, 12, 68, 684, 31, 257, 37, 94, 531, 9, 7, 46, 232, 54, 15, 34, 94, 14, 1106, 170, 257, 117, 423, 96, 1251, 227, 9, 25, 7, 59, 19, 866, 83, 14, 196, 10, 116, 8, 497, 506, 63, 14, 233, 12, 844, 267, 9, 7, 0, 7, 11870, 505, 43, 13, 8, 249, 388, 18, 40, 138, 126, 11, 533, 8, 235, 28, 10, 73, 431, 14, 42, 942, 55, 457, 9, 7, 32, 105, 12, 497, 228, 16, 12, 7, 354, 31, 20, 107, 42, 2478, 47, 3445, 13, 99, 11, 32, 233, 75, 761, 46, 44, 50, 7, 157, 88, 45, 4937, 19, 240, 65, 7, 271, 10, 73, 22, 195, 30, 73, 206, 34, 449, 46, 44, 266, 18, 1185, 19, 545, 34, 558, 14, 8, 2432, 343, 48, 107, 111, 1184, 1512, 19, 31, 72, 9, 7, 8, 386, 13, 8, 205, 15, 1774, 11, 2282, 270, 8, 278, 48, 305, 8, 3788, 10, 7, 0, 9, 7, 74, 8, 3963, 24, 1015, 11, 41, 457, 36, 8, 1647, 83, 37, 1306, 46, 44, 9, 25, 7, 19, 28, 2371, 86, 13, 236, 173, 1483, 194, 31, 7, 0, 9, 7, 20, 28, 24, 12, 783, 13, 757, 2252, 11, 1989, 10191, 11, 7, 71, 12, 7, 3963, 7, 1899, 15, 195, 94, 8, 164, 9, 7, 4916, 5633, 12, 568, 110, 73, 6884, 12, 3955, 30, 117, 58, 12, 1989, 4211, 52, 12, 1196, 57, 8, 3788, 9, 7, 19, 524, 13, 158, 531, 133, 11, 133, 184, 373, 473, 120, 60, 1543, 227, 10, 8, 3963, 800, 9, 7, 39, 63, 14, 42, 43, 13, 8, 11334, 2226, 138, 9, 7, 39, 3353, 75, 2874, 11, 88, 34, 304, 14, 7348, 94, 13, 12, 2778, 9, 7, 8, 286, 15, 95, 30, 17, 3742, 8, 386, 13, 8, 31, 51, 17, 89, 34, 79, 558, 9, 7, 8, 31, 15, 561, 47, 7, 2107, 7, 1177, 11, 39, 15, 79, 95, 46, 1107, 72, 664, 9, 7, 39, 24, 776, 188, 1451, 39, 97, 129, 13, 11, 8, 1728, 83, 34, 68, 67, 9, 7, 8, 315, 24, 540, 11, 18, 274, 233, 8, 31, 1046, 27, 20, 9, 7, 17, 24, 49, 4418, 315, 51, 17, 24, 195, 1004, 30, 20, 22, 60, 8, 81, 67, 179, 19, 31, 63, 14, 1394, 9, 7, 16, 8, 146, 10, 59, 32, 202, 37, 12, 1367, 240, 116, 32, 147, 3296, 8, 28, 9, 7, 730, 275, 127, 197, 3], [2, 12, 515, 13, 645, 215, 345, 10, 7429, 12, 15661, 1863, 402, 10, 3882, 14, 49, 177, 0, 20, 24, 6085, 47, 84, 17258, 298, 8, 76, 13, 8, 2900, 15071, 13, 8, 623, 0, 9, 7, 45, 260, 17914, 12, 649, 20, 2716, 115, 2017, 45, 382, 17, 11, 45, 2238, 20, 12, 191, 526, 46, 8, 2508, 258, 42, 53, 12, 277, 20, 24, 229, 14, 40, 11021, 198, 615, 9, 7, 19, 191, 246, 504, 329, 22, 649, 27, 310, 9, 25, 7, 19, 37, 12, 194, 28, 26, 17, 538, 14, 1009, 394, 26, 9, 7, 484, 10, 54, 15, 1592, 11, 44, 20, 2342, 501, 10, 30, 19, 15, 79, 58, 49, 203, 28, 9, 12, 68, 1629, 203, 28, 9, 7, 5541, 238, 4669, 36, 132, 564, 11, 9883, 11, 2313, 81, 3008, 655, 14, 8, 125, 10, 85, 63, 1035, 13, 1126, 0, 568, 27, 217, 1281, 9, 7, 30, 588, 83, 285, 16, 11, 74, 175, 8, 28, 15, 324, 133, 32, 274, 144, 64, 15, 132, 284, 11, 47, 959, 9, 25, 7, 54, 38, 12, 201, 67, 152, 153, 11, 54, 10, 111, 4170, 49, 6477, 578, 20, 15, 5277, 0, 120, 0, 29, 12, 220, 7876, 10, 30, 20, 22, 60, 17, 9, 7, 64, 22, 479, 15, 20, 182, 7, 10967, 7, 13663, 10, 48, 63, 66, 68, 427, 59, 37, 1208, 121, 14, 41, 1046, 10, 4662, 3027, 36, 12437, 4020, 244, 11, 2037, 1448, 946, 14, 1664, 20, 31, 11, 20, 192, 17, 216, 854, 8, 0, 21, 194, 21, 28, 257, 8, 5230, 21, 7, 626, 16, 8, 7, 518, 9, 21, 7, 17, 15, 37, 26, 95, 26, 21, 7, 626, 16, 8, 7, 518, 21, 257, 8, 201, 9883, 152, 38, 186, 9883, 11, 37, 4578, 3155, 10, 30, 16, 8, 146, 10, 8, 4479, 19, 31, 78, 435, 27, 15, 12, 359, 11, 10212, 372, 36, 623, 369, 1873, 9, 367, 127, 197, 25, 7, 1245, 5159, 103, 1316, 203, 3], [2, 7, 225, 15, 225, 93, 702, 45, 58, 400, 14, 611, 251, 121, 466, 36, 12, 1179, 20, 15, 198, 365, 9, 7, 8, 104, 7, 0, 28, 24, 92, 9, 7, 8, 360, 43, 24, 641, 9, 7, 8, 815, 43, 24, 6308, 9, 7, 8, 303, 218, 24, 0, 378, 9, 7, 11, 19, 28, 58, 3680, 8, 11577, 2950, 9, 25, 7, 692, 166, 1269, 27, 757, 1405, 332, 11, 0, 10, 171, 91, 40, 8, 218, 283, 16, 8, 7, 1370, 7, 1177, 10, 16, 8, 0, 22, 10, 11, 45, 639, 0, 9, 7, 52, 12, 7734, 13, 8, 104, 43, 10, 45, 40, 14, 878, 109, 14, 1315, 115, 44, 133, 184, 9, 7, 2080, 427, 18, 1185, 9, 7, 1101, 19, 704, 23, 14, 23, 425, 863, 10, 58, 52, 7, 0, 275, 11, 367, 10, 15, 58, 185, 112, 113, 242, 9, 7, 482, 11, 18, 1905, 14, 566, 109, 54, 15, 140, 118, 1589, 13, 149, 795, 652, 309, 93, 88, 45, 58, 356, 811, 14, 1589, 19, 12408, 1737, 65, 18, 129, 160, 52, 19, 70, 42, 426, 23, 251, 10, 51, 300, 9227, 16, 8, 104, 31, 70, 34, 42, 51, 14072, 9, 25, 7, 121, 52, 19, 20, 2965, 8, 223, 58, 114, 86, 1040, 9, 7, 753, 19, 1738, 9, 3], [2, 200, 0, 200, 56, 37, 364, 118, 1049, 59, 32, 40, 34, 126, 19, 28, 25, 7, 2869, 72, 120, 8, 2322, 21, 7, 3568, 1210, 21, 1154, 142, 7, 8, 7, 2425, 7, 141, 5855, 7, 1880, 10, 154, 7, 16353, 11, 191, 526, 7, 2612, 1303, 111, 13, 8, 386, 13, 8, 28, 295, 14, 105, 108, 174, 10, 11, 37, 146, 72, 26, 3023, 36, 8, 7, 2425, 7, 141, 22, 21, 7, 916, 7, 1409, 21, 9, 7, 19, 43, 237, 79, 764, 103, 8, 4101, 13, 4566, 16, 8, 2023, 3753, 0, 83, 81, 54, 27, 687, 2165, 10, 11, 8, 308, 3430, 3030, 35, 206, 34, 496, 55, 415, 33, 24, 51, 1982, 18, 97, 34, 765, 14, 80, 59, 8, 7, 2425, 7, 141, 550, 55, 123, 9, 7, 19, 43, 723, 17, 72, 14, 238, 176, 52, 17, 24, 185, 14, 42, 7, 2612, 22, 123, 10, 48, 296, 16, 623, 11, 1169, 15, 8, 521, 10, 30, 93, 17, 89, 34, 216, 196, 61, 20, 112, 9, 7, 47, 8, 68, 146, 13, 19, 82, 7, 8955, 15, 18126, 47, 8, 551, 0, 10, 7, 1880, 237, 12, 4615, 16797, 18066, 98, 41, 6610, 10, 8, 0, 105, 7, 16353, 11, 8, 14291, 105, 7, 2612, 10, 11, 54, 22, 75, 43, 338, 14, 611, 7, 8, 7, 2425, 7, 141, 9, 7, 8, 95, 246, 2842, 23, 171, 109, 22, 20, 27, 12, 1075, 896, 46, 8, 146, 65, 25, 7, 19, 24, 700, 16, 6465, 11, 0, 373, 62, 5777, 10, 11, 8, 286, 13, 19, 43, 24, 46, 8, 76, 469, 10, 11, 297, 17, 22, 1483, 11, 17097, 17, 24, 106, 12, 2806, 11, 2460, 286, 27, 19, 977, 139, 218, 9, 7, 30, 8, 2774, 43, 1427, 19, 286, 11, 1617, 180, 191, 30, 69, 95, 762, 10, 11, 49, 74, 479, 11, 69, 6512, 286, 93, 25, 200, 370, 127, 275, 61, 13, 4, 209, 200, 3], [2, 18, 147, 40, 966, 219, 330, 8, 28, 120, 942, 8, 691, 153, 9, 18, 245, 8, 235, 28, 47, 675, 12441, 11, 1154, 16, 2451, 227, 9, 168, 117, 12, 392, 348, 28, 17, 97, 40, 101, 102, 145, 9, 25, 7, 8, 28, 509, 69, 52, 12, 739, 11, 16, 201, 227, 17, 830, 32, 20, 32, 147, 6598, 143, 1076, 9, 7, 11, 157, 24, 8, 28, 617, 21, 2193, 5324, 21, 71, 17, 24, 5852, 51, 90, 16, 165, 12, 95, 9285, 9, 25, 7, 219, 118, 134, 10, 44, 8, 122, 137, 58, 656, 57, 447, 52, 346, 9, 25, 7, 11, 147, 37, 811, 14, 742, 36, 8, 4508, 23, 8, 299, 0, 48, 70, 40, 101, 69, 5412, 59, 19, 28, 24, 12, 383, 76, 222, 9, 3], [2, 7, 2043, 7, 3392, 57, 21, 7, 1215, 7, 476, 7, 0, 33, 103, 7, 1259, 7, 0, 7, 6419, 10, 7, 969, 7, 0, 10, 7, 1922, 7, 0, 10, 7, 4373, 7, 4945, 7, 4099, 10, 7, 0, 7, 0, 10, 7, 1239, 7, 0, 10, 7, 0, 7, 0, 10, 7, 3351, 7, 0, 10, 7, 636, 7, 0, 10, 7, 1063, 7, 7566, 93, 7, 182, 7, 2567, 7, 0, 10, 7, 1108, 7, 9743, 7, 0, 10, 7, 1158, 7, 0, 10, 7, 0, 7, 1742, 9, 25, 7, 19, 15, 166, 7, 3051, 1191, 31, 10, 102, 12873, 27, 674, 10, 773, 16, 4560, 10, 561, 47, 198, 23, 76, 303, 218, 182, 7, 2567, 3324, 7, 0, 9, 7, 1310, 47, 302, 795, 10, 17, 22, 12, 187, 52, 8, 2426, 13, 7, 3051, 121, 10, 12, 0, 1008, 27, 487, 10, 4119, 7582, 13, 8, 3723, 7115, 712, 8, 277, 35, 12, 408, 10, 381, 10, 307, 10, 604, 33, 11, 7544, 35, 1522, 10, 16, 19, 404, 1298, 2708, 33, 9, 7, 8, 215, 11, 139, 579, 506, 7, 0, 7, 6419, 35, 73, 284, 8, 417, 240, 16, 7, 6993, 7, 0, 12, 559, 1166, 267, 26, 336, 402, 1298, 16210, 7, 3511, 7, 10708, 10, 48, 15, 4122, 47, 55, 2708, 10, 7, 2464, 7, 0, 35, 7, 969, 7, 0, 33, 11, 212, 2943, 16, 3392, 26, 75, 43, 2581, 55, 82, 10, 619, 27, 10, 13, 268, 10, 55, 214, 408, 35, 7, 1922, 7, 0, 33, 9, 7, 325, 10, 408, 11, 595, 639, 14, 285, 7, 2464, 7, 0, 614, 5764, 9, 7, 8, 31, 2887, 36, 216, 12, 250, 10, 15, 1169, 834, 10, 46, 241, 242, 113, 4826, 27, 8, 2325, 13, 450, 394, 10, 30, 15, 2409, 927, 16, 8, 146, 9, 7, 0, 7, 6419, 62, 267, 15, 13, 8, 9243, 1797, 252, 10, 30, 73, 15, 8, 5341, 16, 8, 432, 205, 9, 7, 55, 2761, 2001, 10, 665, 1068, 11, 428, 134, 15, 844, 16, 996, 13, 109, 73, 10, 26, 49, 5197, 16210, 10, 0, 55, 2708, 10, 15, 16, 483, 16121, 47, 108, 11, 212, 171, 453, 29, 8, 766, 10, 4830, 11, 1049, 10, 639, 108, 16, 2352, 9, 7, 969, 7, 0, 305, 8, 183, 29, 12, 1670, 445, 13, 0, 10, 168, 39, 15, 242, 53, 1637, 9, 7, 39, 63, 4215, 100, 337, 0, 175, 10, 48, 40, 5201, 1207, 11, 102, 17, 14, 8, 333, 10, 11, 800, 0, 1485, 9, 7, 969, 7, 0, 10, 12, 3477, 23, 579, 181, 52, 8, 413, 10, 63, 63, 140, 247, 12, 228, 52, 19, 30, 30, 39, 89, 12, 861, 11, 643, 267, 428, 9, 7, 17, 22, 270, 1400, 14, 80, 108, 16, 19, 228, 110, 39, 15, 12, 1448, 497, 314, 16, 320, 342, 39, 89, 597, 66, 3477, 579, 119, 11, 36, 8, 1797, 1109, 21, 7, 9182, 7, 583, 21, 9, 7, 8, 81, 159, 409, 18, 87, 29, 19, 31, 15, 8, 1424, 16, 85, 8, 31, 15, 9361, 9, 7, 26, 8, 31, 2306, 10, 91, 128, 7, 2708, 7, 0, 60, 14, 2381, 7, 3511, 10, 3068, 226, 144, 234, 256, 20, 19, 246, 15, 75, 67, 9, 7, 30, 19, 192, 27, 701, 123, 895, 11, 82, 9, 7, 59, 8, 104, 158, 87, 322, 101, 8, 722, 29, 8, 570, 788, 16, 85, 91, 80, 7, 3511, 575, 127, 0, 16, 8, 510, 682, 12, 2155, 10, 91, 70, 40, 145, 123, 895, 110, 91, 56, 34, 144, 20, 7, 2708, 7, 0, 10, 154, 1739, 862, 16, 8, 1058, 13, 41, 337, 16210, 10, 15, 79, 12, 1670, 459, 13, 196, 11, 91, 70, 40, 126, 7, 3511, 0, 12, 422, 959, 73, 229, 73, 651, 11, 116, 1892, 12, 4448, 3531, 71, 73, 2854, 73, 24, 397, 60, 108, 9, 7, 182, 7, 2567, 7, 0, 15, 75, 3963, 14, 450, 27, 303, 35, 7, 0, 22, 7, 5324, 10, 7, 337, 7, 2216, 10, 7, 11437, 7, 3681, 10, 7, 8, 7, 3560, 7, 3355, 10, 7, 15957, 7, 11, 7, 2997, 10, 7, 685, 7, 469, 7, 1679, 11, 121, 52, 7, 884, 7, 9967, 7, 0, 9, 7, 16, 7644, 10, 7, 0, 87, 561, 166, 7, 3051, 28, 10, 662, 14, 19, 10, 21, 7, 751, 7, 476, 7, 1327, 21, 35, 7644, 33, 9, 7, 39, 860, 14, 2977, 8, 9215, 13, 8, 1737, 9, 7, 91, 38, 2409, 3145, 47, 8, 2708, 7115, 189, 8, 10185, 137, 39, 11986, 41, 214, 1553, 9, 7, 149, 152, 38, 2395, 11, 3805, 14, 42, 2478, 47, 2602, 1919, 10, 30, 117, 745, 15, 799, 103, 747, 0, 11, 0, 38, 37, 239, 12, 3963, 11, 78, 2243, 288, 4994, 10, 11, 84, 0, 9543, 78, 74, 1603, 12, 336, 402, 9, 7, 19, 31, 9496, 8, 1190, 13, 1167, 14, 4784, 601, 476, 487, 11, 0, 487, 48, 40, 101, 1207, 2294, 14, 0, 11, 639, 51, 20, 0, 107, 1190, 75, 1049, 9222, 14, 413, 9, 7, 467, 100, 1344, 859, 10, 19, 31, 89, 12, 92, 287, 16, 10466, 117, 745, 11, 3805, 14, 42, 377, 14, 8470, 10, 3385, 10, 336, 402, 1294, 35, 597, 210, 958, 11, 642, 33, 9, 7, 26, 95, 26, 2381, 15, 10, 3273, 1207, 71, 17, 531, 15, 74, 479, 9, 3], [2, 7, 36, 330, 19, 28, 10, 18, 88, 34, 567, 94, 9, 18, 24, 4893, 793, 9, 7, 297, 8, 507, 24, 0, 10, 26, 66, 13, 8, 631, 431, 1369, 23, 61, 11, 1751, 10, 8, 248, 88, 40, 3434, 390, 9, 35, 7, 77, 512, 347, 10, 21, 7, 131, 15, 52, 12, 1187, 0, 10, 49, 23, 11, 18, 172, 14266, 10, 51, 18, 206, 34, 380, 17, 93, 21, 9, 33, 7, 8, 125, 24, 191, 11, 2092, 26, 4539, 14, 66, 13, 8, 4597, 69, 21, 299, 23, 6191, 21, 194, 20, 111, 13, 226, 38, 35, 1126, 33, 1763, 327, 14, 9, 18, 56, 272, 66, 13, 8, 152, 97, 35, 147, 33, 40, 101, 1171, 35, 11, 280, 66, 5321, 33, 9, 7, 8, 8085, 382, 13, 547, 24, 2416, 30, 431, 14, 114, 293, 16, 8, 146, 4, 209, 9, 7, 3409, 10, 8, 146, 9, 7, 8, 146, 70, 40, 101, 12, 139, 145, 59, 45, 87, 1719, 157, 64, 534, 10, 534, 9, 35, 18, 56, 34, 3045, 3035, 17, 9, 33, 7, 17, 258, 40, 106, 1341, 14, 8, 28, 76, 10, 85, 15, 16, 8, 1486, 13, 21, 17, 22, 81, 49, 488, 198, 10, 109, 67, 97, 17, 42, 65, 21, 7, 0, 7, 216, 67, 9, 3]...] \n"," ClassIdItemList (1000 items)\n","[1, 0, 1, 0, 0, 0, 0, 0, 1, 1...]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oiCRp2LICLmm","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","class ArchTextClassifier():\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def OLD_test_predict(self, test_data):\n","    x,y = next(iter(test_data.valid_dl))\n","    pred_batch = self.arch.eval()(x.cuda())\n","    return (pred_batch)\n","\n","    # Predicting on the padded batch or on the individual unpadded samples give the same results.\n","    pred_ind = []\n","    for inp in x:\n","      length = x.size(1) - (inp == self.awd_lstm_enc.pad_idx).long().sum()\n","      inp = inp[:length]\n","      pred_ind.append(self.arch.eval()(inp[None].cuda()))\n","    assert near(pred_batch, torch.cat(pred_ind))\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def OLD_test_train(self, test_data, opt_func, split_lr, split, one_cycle, freeze, num_epochs=1):\n","    train_dl = test_data.train_dl\n","    valid_dl = test_data.valid_dl\n","    # NB: We don't use cross_entropy_flat for Classification\n","    loss_func = F.cross_entropy\n","\n","    # split_lr is a list:\n","    #   1. a single-element list [0.01] - same LR for all groups. \n","    #        If 'Split' is False, there is only one group. \n","    #        If 'Split' is True, there are multiple groups.\n","    #   2. a multi-element list [0.01, 0.03, 0.05] - discriminative LR for different groups. \n","    #        'Split' cannot be False\n","    assert(isinstance(split_lr, list))\n","    assert(len(split_lr) > 0)\n","    assert(not ((split == False) and (len(split_lr) > 1)))\n","\n","    # NB: We don't use the AwdLstmCB while doing Classification and we use accuracy not accuracy_flat\n","    callbs=[CudaCB(device = torch.device('cuda',0)), GradientClipping(clip=0.1), ProgressCallback(), MetricsCB({\"acc\": accuracy})]\n","    if (one_cycle):\n","      one_cycle_callbs = create_OneCycleCB(split_lr, phases=[0.5, 0.5], mom_start=0.8, mom_mid=0.7, mom_end=0.8)\n","      callbs = callbs + one_cycle_callbs\n","\n","    self.freeze(freeze)\n","\n","    print ('BEFORE Hyper parameters')\n","    lr = split_lr[0]\n","    if (split and (len(split_lr) == 1)):\n","      hypers_group = [{}] * self.n_splits\n","      opt_groups=(self.splitter, hypers_group, {'lr': lr})\n","    elif (split and (len(split_lr) > 1)):\n","      hypers_group = [{'lr': lr_g} for lr_g in split_lr]\n","      opt_groups=(self.splitter, hypers_group, {})\n","    else:\n","      opt_groups = None\n","    opt = get_optimiser(self.arch, lr, opt_func, opt_groups)\n","\n","    loop = Trainer(train_dl, valid_dl, self.arch, opt, loss_func, callbs)\n","    #db.set_trace()\n","    loop.fit(num_epochs=num_epochs)\n","\n","    # TODO !!!!!!! Make _print_opt() a public function, maybe using repr\n","    print ('AFTER Hyper parameters')\n","    loop.opt._print_opt()\n","\n","# eg. opt_func = optim.SGD, sgd_opt_func, adam_opt_func\n","# eg. split_lr = [lr/2., lr/2., lr] for one cycle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"USLlyzen-uvc","colab_type":"code","outputId":"fc24cf6a-3000-41c6-c243-8bf092dd64ca","executionInfo":{"status":"ok","timestamp":1584706102817,"user_tz":-330,"elapsed":13770,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":807}},"source":["test_vocab = lm_imdb_vocab\n","test_data = tlmdb\n","#test_data = subset_data\n","tok_pad = test_vocab.index(PAD)\n","bptt = 70\n","\n","torch.manual_seed(0)\n","dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25\n","arch_classifier = ArchTextClassifier(len(test_vocab), 300, 300, 2, tok_pad, 2, bptt, *dps)\n","\n","# Load pre-trained AWD-LSTM weights\n","arch_classifier.load_weights(path_imdb_tuned/'finetuned_enc.pth')\n","\n","arch_classifier.test_train(test_data, opt_func=adam_opt_func, split_lr=[1e-2], split=True, one_cycle=True, freeze=\"FREEZE_ENCODER\", num_epochs=1)\n","\n","lr_tmp = 5e-3\n","split_lr=[lr_tmp/2., lr_tmp/2., lr_tmp/2., lr_tmp]\n","arch_classifier.test_train(test_data, opt_func=adam_opt_func, split_lr=split_lr, split=True, one_cycle=True, freeze=\"UNFREEZE_LAST_LSTM\", num_epochs=1)\n","\n","lr_tmp = 1e-3\n","split_lr=[lr_tmp/8., lr_tmp/4., lr_tmp/2., lr_tmp]\n","arch_classifier.test_train(test_data, opt_func=adam_opt_func, split_lr=split_lr, split=True, one_cycle=True, freeze=\"UNFREEZE_ENCODER\", num_epochs=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","1 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n","5 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n","8 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.631232</td>\n","      <td>0.658575</td>\n","      <td>0.632412</td>\n","      <td>0.634570</td>\n","      <td>00:02</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","1 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","5 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","5 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","8 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00017046916484597316}\n","BEFORE Hyper parameters\n","1 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0025}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0025}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0025}\n","8 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.494811</td>\n","      <td>0.756417</td>\n","      <td>0.543253</td>\n","      <td>0.723437</td>\n","      <td>00:02</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","1 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.261729121149329e-05}\n","4 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.261729121149329e-05}\n","4 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.261729121149329e-05}\n","8 {'momentum': 0.7982962913144535, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 8.523458242298658e-05}\n","BEFORE Hyper parameters\n","1 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.000125}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.00025}\n","4 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.0005}\n","8 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.001}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>0.412415</td>\n","      <td>0.807199</td>\n","      <td>0.482806</td>\n","      <td>0.760938</td>\n","      <td>00:03</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>0.369527</td>\n","      <td>0.838914</td>\n","      <td>0.449211</td>\n","      <td>0.794141</td>\n","      <td>00:03</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","1 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 5.359408171751905e-07}\n","4 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 1.071881634350381e-06}\n","4 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 2.143763268700762e-06}\n","8 {'momentum': 0.7995722430686906, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 4.287526537401524e-06}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SIXV2NPqeEu_","colab_type":"code","colab":{}},"source":["arch_classifier.test_predict(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtYExeLy_WSL","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# End-to-end run - Train using KD Trainer (as opposed to Fastai Learner)\n","# Change the 'model' variable to switch between KD Model and Fastai Model\n","# Change the 'test_data' variable to switch between full 'data' and 'subset_data'\n","# Change the 'opt' variable to switch between the Pytorch SGD optimiser, KD's SGD optimiser and KD's Adam optimiser\n","#----------------------------------------------------\n","\n","#----------------------------------------------------\n","# Note that I run various combinations of training to compare results. Those results are in an email on fal_thu\n","# This tested KD model and Fastai model, with KD Trainer and Fastai Learner, full data and subset data and different optimisers\n","# The results came out more or less consistent in all these scenarios showing that the KD Trainer, KD model, KD callbacks and KD optimisers\n","# were equivalent to the ones from the Fastai lesson.\n","#----------------------------------------------------\n","\n","\n","#----------------------------------------------------\n","# Create the end-to-end architecture with the AWD-LSTM followed by the Decoder\n","#----------------------------------------------------\n","def get_language_model(vocab_sz, emb_sz, n_h, n_layers, pad_idx, out_p=0.4, hidden_p=0.2, inp_p=0.6, \n","                       emb_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n","  awd_lstm_enc = AWD_LSTM(vocab_sz, emb_sz, n_h, n_layers, pad_idx, emb_p, inp_p, weight_p, hidden_p)\n","\n","  # Get the embedding layer from the AWD-LSTM to enable weight-tying with the\n","  # Decoder\n","  enc = awd_lstm_enc.emb if tie_weights else None\n","\n","  # The input of the Decoder has embedding size to match the output of the \n","  # last AWD-LSTM layer. The output of the Decoder has vocab size to produce\n","  # a probability for each word in the vocab\n","  awd_lstm_dec = LinearDecoder(emb_sz, vocab_sz, out_p, tie_encoder=enc)\n","\n","  return SequentialRNN(awd_lstm_enc, awd_lstm_dec)\n","\n","#----------------------------------------------------\n","# Freeze the LSTM layers\n","#----------------------------------------------------\n","def freeze_lstm(model, freeze):\n","  do_grad = not freeze\n","  for rnn in model[0].lstm_layers:\n","    for p in rnn.parameters(): p.requires_grad_(do_grad)\n","\n","#----------------------------------------------------\n","# Now run the model end to end using same hyperparameters for the 3 parameter groups\n","# !!!! Using my code only (ie. KD Trainer, KD Optimiser and KD Model) and no Fastai\n","#----------------------------------------------------\n","\n","# !!!!! The first thing was to implement the param group splitter and make sure it works with the KD Adam optimiser. That is done.\n","\n","def test_awd_lstm(model, test_data, lr, opt_type, one_cycle, freeze):\n","  train_dl = test_data.train_dl\n","  valid_dl = test_data.valid_dl\n","  loss_func = cross_entropy_flat\n","\n","  callbs=[CudaCB(device = torch.device('cuda',0)), AwdLstmCB(alpha=2., beta=1.), GradientClipping(clip=0.1), ProgressCallback(), MetricsCB({\"acc\": accuracy_flat})]\n","  if (one_cycle):\n","    # TODO !!!!!!! Using freeze as the if condition to differentiate between Step 2 and Step 3 below is a hack\n","    if (freeze):\n","      one_cycle_callbs = create_OneCycleCB([lr], phases=[0.5, 0.5], mom_start=0.8, mom_mid=0.7, mom_end=0.8)\n","    else:\n","      one_cycle_callbs = create_OneCycleCB([lr/2., lr/2., lr], phases=[0.5, 0.5], mom_start=0.8, mom_mid=0.7, mom_end=0.8)\n","    callbs = callbs + one_cycle_callbs\n","\n","  freeze_lstm(model, freeze)\n","\n","  print ('BEFORE Hyper parameters')\n","  if (opt_type == \"py_sgd\"):\n","    opt = optim.SGD(model.parameters(), lr=lr)\n","  elif (opt_type == \"lib_sgd\"):\n","    opt = sgd_opt_func(model.parameters(), lr=lr)\n","  elif (opt_type == \"lib_adam\"):\n","    opt = adam_opt_func(model.parameters(), lr=lr)\n","  elif (opt_type == \"lib_adam_split\"):\n","    opt_groups=(lm_splitter, [{}, {}, {}], {'lr': lr})\n","    opt = get_optimiser(model, lr, adam_opt_func, opt_groups)\n","  else:\n","    opt = None\n","\n","  loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs)\n","  loop.fit(num_epochs=1)\n","\n","  # TODO !!!!!!! Make _print_opt() a public function, maybe using repr\n","  print ('AFTER Hyper parameters')\n","  loop.opt._print_opt()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u61ROXJB-Dyy","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Create the model based on KD code\n","# Do this after importing Lib ie. the first 4 cells of this current notebook\n","#----------------------------------------------------\n","torch.manual_seed(0)\n","tok_pad = vocab.index(PAD)\n","kd_model_cpu = get_language_model(len(vocab), 300, 300, 2, tok_pad)\n","kd_model = kd_model_cpu.cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mG33OmAaqDrp","colab_type":"code","outputId":"1f076be8-b4ac-4237-ef76-38f2224ad272","executionInfo":{"status":"ok","timestamp":1584685790455,"user_tz":-330,"elapsed":966073,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"colab":{"base_uri":"https://localhost:8080/","height":168}},"source":["test_awd_lstm(kd_model, subset_data, lr=5e-3, opt_type=\"lib_adam\", one_cycle=False, freeze=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["BEFORE Hyper parameters\n","12 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>6.837101</td>\n","      <td>0.081921</td>\n","      <td>6.570950</td>\n","      <td>0.087704</td>\n","      <td>00:43</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["AFTER Hyper parameters\n","12 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.005}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vF6GuljktOkS","colab_type":"text"},"source":["**Split the model into param groups**"]},{"cell_type":"code","metadata":{"id":"t9CQsdsxa6Hz","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Split into three groups - two for each rnn/corresponding dropout, then one last \n","# group that contains the embeddings/decoder. This is the one that needs to be\n","# trained the most as we may have new embeddings vectors.\n","#----------------------------------------------------\n","\n","def lm_splitter(m):\n","    groups = []\n","    for i in range(len(m[0].lstm_layers)): \n","      groups.append(nn.Sequential(m[0].lstm_layers[i], m[0].hidden_dps[i]))\n","    groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].inp_dp, m[1])]\n","    return [list(o.parameters()) for o in groups]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pIYe48DU7Ywj","colab_type":"code","colab":{}},"source":["test_awd_lstm(kd_model, subset_data, lr=5e-3, opt_type=\"lib_adam_split\", one_cycle=False, freeze=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0xpMXFp3pf8o","colab_type":"code","colab":{}},"source":["test_awd_lstm(kd_model, subset_data, lr=2e-2, opt_type=\"lib_adam_split\", one_cycle=True, freeze=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cP9LhXWTp4om","colab_type":"code","colab":{}},"source":["test_awd_lstm(kd_model, subset_data, lr=2e-3, opt_type=\"lib_adam_split\", one_cycle=True, freeze=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jC3yZ0CrlFj","colab_type":"code","colab":{}},"source":["  def temp_forward(self, input):\n","    # If the batch size changes, then re-initialise the hidden state (whose shape depends on batch size)\n","    bs, _ = input.size()\n","    if bs!=self.bs:\n","      self.bs=bs\n","      self.reset_state()\n","\n","    # Input goes through Embedding layer with dropout\n","    # Input has shape [samples, timesteps]\n","    # Emb_val has shape [samples, timesteps, embedding size]\n","    emb_val = self.emb_dp(input)\n","\n","    # Now apply Input dropout to the result of the embedding\n","    # This will then be fed to the LSTM layers\n","    # Inp_val has shape [samples, timesteps, embedding size]\n","    inp_val = self.inp_dp(emb_val)\n","\n","    #print ('KD input and inp val', input.float().mean(), inp_val.mean())\n","\n","    # Keep a list of hidden state, raw output values and output values post dropout for each LSTM layer\n","    new_states, out_vals, out_dp_vals = [], [], []\n","\n","    # Go through each LSTM layer and its corresponding Hidden Dropout and Hidden State\n","    for lstm_dp, hidden_dp, state in zip(self.lstm_layers, self.hidden_dps, self.state):\n","      # Apply the LSTM\n","      out_val, new_state = lstm_dp(inp_val, state)\n","      # Apply the Hidden Dropout to the LSTM output\n","      out_dp_val = hidden_dp(out_val)\n","\n","      #print ('KD layer', out_val.mean(), state[0].mean(), state[1].mean(), new_state[0].mean(), new_state[1].mean())\n","      #print ('KD hidden', out_dp_val.mean())\n","\n","      # Add the state, raw output value and output value post dropout for this layer, to the lists\n","      # [hidden state layer 1, hidden state layer 2, ....]\n","      # [raw output layer 1, raw output layer 2, ...]\n","      # [(post dropout) output layer 1, output layer 2, ...]\n","      new_states.append(new_state)\n","      out_vals.append(out_val)\n","      out_dp_vals.append(out_dp_val)\n","\n","      # The post-dropout output will become the input to the next layer\n","      inp_val = out_dp_val\n","\n","    # Save the new hidden states\n","    self.state = self._to_detach(new_states)\n","\n","    # Return ([list of raw outputs for each layer], [list of outputs for each layer])\n","    return (out_vals, out_dp_vals)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nFLP_x2-ZAL","colab_type":"code","colab":{}},"source":["\n","\n","model = kd_model\n","test_data = subset_data\n","#test_data = data\n","py_sgd = optim.SGD(model.parameters(), lr=5e-3)\n","lib_sgd = sgd_opt_func(model.parameters(), lr=5e-3)\n","lib_adam = adam_opt_func(model.parameters(), lr=5e-3)\n","\n","train_dl = test_data.train_dl\n","valid_dl = test_data.valid_dl\n","loss_func = cross_entropy_flat\n","opt = lib_adam\n","\n","callbs=[CudaCB(device = torch.device('cuda',0)), AwdLstmCB(alpha=2., beta=1.), GradientClipping(clip=0.1), ProgressCallback(), MetricsCB({\"acc\": accuracy_flat})]\n","\n","loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs)\n","loop.fit(num_epochs=1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bPfzlQblpXqN","colab_type":"code","colab":{}},"source":["\n","\n","model = kd_model\n","test_data = subset_data\n","#test_data = data\n","py_sgd = optim.SGD(model.parameters(), lr=5e-3)\n","lib_sgd = sgd_opt_func(model.parameters(), lr=5e-3)\n","lib_adam = adam_opt_func(model.parameters(), lr=5e-3)\n","\n","opt_groups=(lm_splitter, [{}, {}, {}], {'lr': 1e-2})\n","lib_adam_split = get_optimiser(model, 5e-3, adam_opt_func, opt_groups)\n","\n","train_dl = test_data.train_dl\n","valid_dl = test_data.valid_dl\n","loss_func = cross_entropy_flat\n","opt = lib_adam_split\n","\n","callbs=[CudaCB(device = torch.device('cuda',0)), AwdLstmCB(alpha=2., beta=1.), GradientClipping(clip=0.1), ProgressCallback(), MetricsCB({\"acc\": accuracy_flat})]\n","\n","loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs)\n","loop.fit(num_epochs=1)"],"execution_count":0,"outputs":[]}]}