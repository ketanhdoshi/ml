{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Image Segmentation.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["oDqvMEF25fhd","9aAvrYcEmgCU","lysWvQv4TnkY","7nw-l32zTIRo","snnMHTZRiL1L","hFB0Uiq-4RUN","HsjygTKEfwvV","l0s6fY5mK4hf","8_wpoU7DpT2b","CUMZfq3WymJN","VcCsDyCn5WqX","ftGBDhEo9WSO","fVftt3fdW8yT","bmztacepXdwf","w1808VDE6E8a","4m3XpDco-iqS","DWlmzDr26SII","gfUKwGdIR3MX","oVSxR0bFRQ-m","Lu07Z5kmwVqC"],"authorship_tag":"ABX9TyNgo4VlB0nkDZZ9DWgdJNbf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_udsuq_1R9q5","colab_type":"text"},"source":["### Image Segmentation with Unet - [notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/carvana.ipynb), [video](https://youtu.be/nG3tT31nPmQ), [notes](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-14-e0d23c7a0add)"]},{"cell_type":"markdown","metadata":{"id":"233jN_MC1KLk","colab_type":"text"},"source":["**Todos**\n","* Image augmentation as per Fastai - rotate images\n","* Use lr find\n","* Move ISDataBundle into data_lib\n","* DONE Freeze Resnet first, then unfreeze\n","* DONE Build Unet\n","* DONE Show results after predict\n","* DONE Standard template for every application - App, Arch, DTR, display batch/results, LR Find, Metrics, Progress, Tensor board results, Debugging layers, One Cycle scheduler, Databundle, Plot hyperparameters, Parameter groups\n","* DONE Try with bs = 64\n","* DONE Save the first model and load it when trying 512\n","* DONE Then save the 512 model and load it when trying 1024\n","* DONE When freezing, unfreeze the Batch Norm layers in the backbone. Only in Resnet, not the head.\n","* DONE Use one cycle, recorder\n","* DONE Use discriminative learning rates\n","* DONE add an arch.summary()\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oDqvMEF25fhd","colab_type":"text"},"source":["### Import Libraries"]},{"cell_type":"code","metadata":{"id":"EHLPROiP6Lpk","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kI5e1uoLdVMQ","colab_type":"code","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nx-Xm5ShB3A7","colab_type":"code","colab":{}},"source":["import IPython.core.debugger as db\n","from pathlib import Path\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","from functools import partial\n","from concurrent.futures import ThreadPoolExecutor\n","import shutil\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import tensor, nn\n","from torch.utils.data import DataLoader, Sampler, SequentialSampler, RandomSampler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QwN17d7_DKe","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","gd_path = 'gdrive/My Drive/Colab Data/fastai-v3'  #change dir to your project folder\n","gn_path = 'gdrive/My Drive/Colab Notebooks'  #change dir to your project folder\n","\n","import sys\n","sys.path.insert(1, gn_path + '/exp')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcLxkgTy_P1L","colab_type":"code","colab":{}},"source":["from nb_util import accuracy, dice, accuracy_thresh\n","from nb_data import DataBundle, CSVItemContainer, DfItemList, ImageFileItemList, ImageItemList, ILPairedDataset, kaggle_data\n","from nb_training import Trainer, CudaCB, ProgressCallback, MetricsCB, DebugTracker, DebugYhatLossCB\n","from nb_optimiser import Recorder, HyperParams, adam_opt_func\n","from nb_arch import FuncLayer, ArchBase\n","from nb_image import ShowImg"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9aAvrYcEmgCU","colab_type":"text"},"source":["### Define Data File Paths"]},{"cell_type":"code","metadata":{"id":"x6w6DBZvfFoR","colab_type":"code","colab":{}},"source":["root_path = Path.cwd()\n","data_path = root_path/'carvana'\n","data_path.mkdir(exist_ok=True)\n","\n","full_metadata_file_path = data_path/'metadata.csv'\n","full_masks_file_path = data_path/'train_masks.csv'\n","orig_masks_path = data_path/'train_masks'\n","orig_imgs_path = data_path/'train'\n","\n","full_masks_path = data_path/'train_masks_128'\n","full_imgs_path = data_path/'train_128'\n","full_masks_path.mkdir(exist_ok=True)\n","full_imgs_path.mkdir(exist_ok=True)\n","\n","temp_subset_masks_path = data_path/'train_masks_subset'\n","temp_subset_imgs_path = data_path/'train_subset'\n","\n","g_data_path = Path(gd_path) / 'data'\n","subset_masks_path = g_data_path/'train_masks_subset'\n","subset_imgs_path = g_data_path/'train_subset'\n","subset_masks_path.mkdir(exist_ok=True)\n","subset_imgs_path.mkdir(exist_ok=True)\n","\n","basic_model_128_path = data_path/'basic_model_128.pth'\n","basic_model_512_path = data_path/'basic_model_512.pth'\n","basic_model_1024_path = data_path/'basic_model_1024.pth'\n","unet_model_128_path = data_path/'unet_model_128.pth'\n","unet_model_512_path = data_path/'unet_model_512.pth'\n","unet_model_1024_path = data_path/'unet_model_1024.pth'\n","\n","list(data_path.iterdir())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lysWvQv4TnkY","colab_type":"text"},"source":["### Fetch Dataset from Kaggle"]},{"cell_type":"code","metadata":{"id":"8fRLXbHGaw3h","colab_type":"code","colab":{}},"source":["kaggle_data('competitions', 'carvana-image-masking-challenge')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRZk58g5QUUu","colab_type":"code","colab":{}},"source":["def carvana_extract(zip_file, out_path):\n","  # It is a gigantic 24GB zip file, so delete some of the largest unnecessary files\n","  # to reduce it to 8.7GB and save some disk space\n","  !zip -d {zip_file} test_hq.zip\n","  !zip -d {zip_file} train_hq.zip\n","  !zipinfo {zip_file}\n","\n","  !unzip -j {zip_file} metadata.csv.zip train_masks.csv.zip train.zip train_masks.zip -d {out_path}\n","  !unzip \"carvana/*.zip\" -d {out_path} >> /dev/null\n","  !rm {out_path}/*.zip\n","  !ls -l {out_path}\n","\n","carvana_extract('carvana-image-masking-challenge.zip', data_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snnMHTZRiL1L","colab_type":"text"},"source":["### Explore data"]},{"cell_type":"code","metadata":{"id":"AxuwE6JIiKZJ","colab_type":"code","colab":{}},"source":["meta_df = pd.read_csv(full_metadata_file_path)\n","meta_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rOJnX8q3l9Q4","colab_type":"code","colab":{}},"source":["masks_df = pd.read_csv(full_masks_file_path)\n","masks_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n67GNKrtn_O_","colab_type":"code","colab":{}},"source":["ex_mask_files = list(orig_masks_path.iterdir())[:10]\n","ex_mask_files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z1SZ4Y79ooGm","colab_type":"code","colab":{}},"source":["Image.open(ex_mask_files[0]).resize((300,200))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DKd8eO4ruovT","colab_type":"code","colab":{}},"source":["ex_masks = [Image.open(mask) for mask in ex_mask_files]\n","ShowImg.show_grid(ex_masks, [mask_file.name for mask_file in ex_mask_files])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VX7jLM9cujv1","colab_type":"code","colab":{}},"source":["ex_img_files = list(orig_imgs_path.iterdir())[:10]\n","ex_img_files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o09L6eWdvHSw","colab_type":"code","colab":{}},"source":["Image.open(ex_img_files[0]).resize((300,200))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bKBjNza3vNCt","colab_type":"code","colab":{}},"source":["ex_imgs = [Image.open(img) for img in ex_img_files]\n","ShowImg.show_grid(ex_imgs, [img_file.name for img_file in ex_img_files])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4oBr8biC0JyB","colab_type":"code","colab":{}},"source":["CAR_ID = '154ee2b6d27a'\n","num_angles = 16\n","car_imgs = [Image.open(orig_imgs_path/f'{CAR_ID}_{i+1:02d}.jpg') for i in range(num_angles)]\n","car_masks = [Image.open(orig_masks_path/f'{CAR_ID}_{i+1:02d}_mask.gif') for i in range(num_angles)]\n","\n","ShowImg.show_grid(car_imgs, car_masks, y_method=ShowImg.show_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFxwadslO2xg","colab_type":"code","colab":{}},"source":["((len(masks_df)//16)//5)*16"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFB0Uiq-4RUN","colab_type":"text"},"source":["### Build Unet Architecture"]},{"cell_type":"code","metadata":{"id":"CSHB38pzv3Vt","colab_type":"code","colab":{}},"source":["import torchvision.models as models\n","\n","#----------------------------------------------------\n","# Block used by Trivial model\n","#----------------------------------------------------\n","class BasicUpsample(nn.Module):\n","  def __init__(self, ch_in, ch_out):\n","    super().__init__()\n","    self.conv = nn.ConvTranspose2d(ch_in, ch_out, kernel_size=2, stride=2)\n","    self.bn = nn.BatchNorm2d(ch_out)\n","    self.relu = nn.ReLU()\n","        \n","  def forward(self, x):\n","    return self.bn(self.relu(self.conv(x)))\n","\n","#----------------------------------------------------\n","# Create the Unet architecture\n","#----------------------------------------------------\n","class ArchImageSegmentationBasic(ArchBase):\n","  # ----------------------------\n","  # Show summary of the model with output sizes of each layer, for a given\n","  # input size\n","  # ----------------------------\n","  def summary(self, sz=128):\n","    super().summary(input_sz=(3, sz, sz))\n","\n","  # ----------------------------\n","  # Freeze the resnet layers of the model\n","  # ----------------------------\n","  def freeze(self, on=False):\n","    freeze_module = self.encoder\n","    super().freeze(module=freeze_module, on=on)\n","\n","  # ----------------------------\n","  # Define the module-layer groups to split the model for discriminative LRs. \n","  # The parameters from each module group will be put into separate parameter \n","  # groups.\n","  # ----------------------------\n","  def module_groups(self):\n","    lr_groups = [self.encoder[:6], self.encoder[6:], self.head]\n","    return lr_groups\n","\n","  # ----------------------------\n","  # Create a trivial model to upsample the encoded image\n","  # ----------------------------\n","  def create_model(self):\n","    resnet = models.resnet34(pretrained=True)\n","    # Take all Resnet layers except the last two (Adaptive Pool and FC layers)\n","    self.encoder = nn.Sequential(*list(resnet.children())[:-2])\n","    self.head = nn.Sequential(\n","        nn.ReLU(),\n","        BasicUpsample(512,256),\n","        BasicUpsample(256,256),\n","        BasicUpsample(256,256),\n","        BasicUpsample(256,256),\n","        nn.ConvTranspose2d(256, 1, 2, stride=2),\n","        # Flatten the channel dimension to go from shape (bs, 1, width, height) to \n","        # (bs, width, height)\n","        FuncLayer(lambda x: x[:,0])\n","    )        \n","    self.model = nn.Sequential(self.encoder, self.head)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wakkKy2kY19","colab_type":"code","colab":{}},"source":["\n","#----------------------------------------------------\n","# Decoder Block for Unet model. It consists of two input blocks and one \n","# output block:\n","#   1. Cross input from the matching encoder level. This input is processed by a \n","#       cross conv block\n","#   2. Upward input from the decoder level below it. This input is processed by an \n","#       upsample conv transpose block\n","#   3. Output block concatenates the processed inputs from the two blocks above\n","#       and processed it through a batch norm and relu block.\n","#----------------------------------------------------\n","class UnetUpConv(nn.Module):\n","  def __init__(self, cross_ch_in, up_ch_in, up_ch_out):\n","    super().__init__()\n","\n","    # Conv layer for the cross input\n","    cross_ch_out = up_ch_out // 2\n","    self.cross_conv = nn.Conv2d(cross_ch_in, cross_ch_out, 1)\n","\n","    # Conv Transpose layer for the upsample input\n","    upsample_out = up_ch_out // 2\n","    self.upsample_conv = nn.ConvTranspose2d(up_ch_in, upsample_out, 2, stride=2)\n","    \n","    # Batch norm for the output\n","    self.bn = nn.BatchNorm2d(up_ch_out)\n","\n","  def forward(self, cross_in, up_in):\n","    # Process the two inputs, cross and upsample, via their respective blocks\n","    cross_out = self.cross_conv (cross_in)\n","    upsample_out = self.upsample_conv (up_in)\n","\n","    # Concat the processed inputs and pass them through the output block\n","    concat_cross_upsample = torch.cat([cross_out, upsample_out], dim=1)\n","    up_out = self.bn(F.relu(concat_cross_upsample))\n","    return up_out\n","\n","#----------------------------------------------------\n","# Unet model consisting of a downward Encoder layer stack and an upward Decoder \n","# layer stack with cross connections between them.\n","#\n","# The Encoder consists of four levels (top level_0 to level_3) plus a fifth bottom\n","# level_4, that reduce the image size going downward. \n","#\n","# The Decoder consists of four levels (lowest level_3 to top level_0) that\n","# increase the image size going upward.\n","#\n","# There are cross connections between the four matching Encoder and Decoder levels.\n","# So Encoder level_0 has a cross connection to Decoder level_0 and so on.\n","# \n","# Therefore, each Encoder level sends the same output to two places - one going down \n","# to the Encoder level below it and another going cross-wise to the matching Decoder\n","# level.\n","#\n","# Similarly, each Decoder level gets two inputs - one coming up from the Decoder\n","# level below it and one coming cross-wise from the matching Encoder level.\n","# \n","# The bottom Encoder level_4 is different because it has no matching Decoder \n","# level. So it sends its output to only one place - it becomes the upcoming input\n","# for the lowest Decoder level_3\n","#----------------------------------------------------\n","class UnetResnet(nn.Module):\n","  def __init__(self, resnet):\n","    super().__init__()\n","\n","    # Resnet Layers to be used for the Encoder levels, which have cross \n","    # connections to the Decoder\n","    resnet_layers = list(resnet.children())\n","    self.enc0 = nn.Sequential(*resnet_layers[slice(0, 3)])\n","    self.enc1 = nn.Sequential(*resnet_layers[slice(3, 5)])\n","    self.enc2 = nn.Sequential(*resnet_layers[slice(5, 6)])\n","    self.enc3 = nn.Sequential(*resnet_layers[slice(6, 7)])\n","    self.enc4 = nn.Sequential(*resnet_layers[slice(7, 8)])\n","\n","    # Channels output from each of the encoder's cross levels become\n","    # input channels for the corresponding decoder level\n","    n0 = 64\n","    cross_chs = [n0, n0, n0 * 2, n0 * 4]\n","\n","    # Channels output upward from each of the decoder's levels\n","    # Bottom encoder level_4 output becomes the upward input for the bottom\n","    # decoder level_3\n","    bottom_ch_4 = n0 * 8\n","    up_chs = [256, 256, 256, 256, bottom_ch_4]\n","    \n","    # Build Decoder levels\n","    # Bottom decoder level_3 gets a cross input from the encoder level_3 and\n","    # an upward input from the bottom encoder level_4\n","    self.dec3 = UnetUpConv(cross_chs[3], up_chs[4], up_chs[3])\n","    self.dec2 = UnetUpConv(cross_chs[2], up_chs[3], up_chs[2])\n","    self.dec1 = UnetUpConv(cross_chs[1], up_chs[3], up_chs[1])\n","    self.dec0 = UnetUpConv(cross_chs[0], up_chs[1], up_chs[0])\n","\n","    # Output layer after the Decoder\n","    self.out = nn.ConvTranspose2d(up_chs[0], 1, 2, stride=2)\n","\n","  def forward(self, inp):\n","    # Process the input sequentially downward through all the encoder levels, saving away\n","    # the intermediate outputs of each level\n","\n","    e0 = self.enc0 (inp)\n","    e1 = self.enc1 (e0)\n","    e2 = self.enc2 (e1)\n","    e3 = self.enc3 (e2)\n","\n","    # Bottom encoder layer\n","    e4 = self.enc4 (e3)\n","    e4 = F.relu(e4)\n","\n","    # Process the encoder's outputs sequentially upward through all the decoder levels\n","    # The bottom encoder layer's output 'e4' becomes the upward input to the bottom decoder\n","    # level. The previously saved encoder intermediate outputs become the cross inputs\n","    # to the matching decoder level. \n","    d3 = self.dec3(e3, e4)\n","    d2 = self.dec2(e2, d3)\n","    d1 = self.dec1(e1, d2)\n","    d0 = self.dec0(e0, d1)\n","\n","    # The decoder's output is passed through the final output layer to generate\n","    # the Unet model's output. We also flatten the output from [1, 128, 128] to\n","    # [128, 128]\n","    out = self.out(d0)\n","    out = out[:, 0]\n","    return out\n","\n","#----------------------------------------------------\n","# Create the Unet architecture\n","#----------------------------------------------------\n","class ArchImageSegmentationUnet(ArchBase):\n","  # ----------------------------\n","  # Create a Unet model. It is slightly different because it uses a Resnet\n","  # encoder backbone instead of building it from scratch\n","  # ----------------------------\n","  def create_model(self):\n","    # Encoder is all Resnet layers except the last two (Adaptive Pool \n","    # and FC layers)\n","    resnet = models.resnet34(pretrained=True)\n","\n","    # Build the Unet\n","    self.model = UnetResnet(resnet)\n","\n","  # ----------------------------\n","  # Show summary of the model with output sizes of each layer, for a given\n","  # input size\n","  # ----------------------------\n","  def summary(self, sz=128):\n","    super().summary(input_sz=(3, sz, sz))\n","\n","  # ----------------------------\n","  # Freeze the resnet layers of the model\n","  # ----------------------------\n","  def freeze(self, on=False):\n","    freeze_module = nn.ModuleList([model.enc0, model.enc1, model.enc2, model.enc3, model.enc4])\n","    super().freeze(module=freeze_module, on=on)\n","\n","  # ----------------------------\n","  # Define the module-layer groups to split the model for discriminative LRs. \n","  # The parameters from each module group will be put into separate parameter \n","  # groups.\n","  # ----------------------------\n","  def module_groups(self):\n","    model = self.model\n","    lr_groups = [nn.ModuleList([model.enc0, model.enc1, model.enc2]), \n","                 nn.ModuleList([model.enc3, model.enc4]),\n","                 nn.ModuleList([model.dec3, model.dec2, model.dec1, model.dec0, model.out])]\n","    return lr_groups\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsjygTKEfwvV","colab_type":"text"},"source":["### Define Image Segmentation Data Bundle"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rQubDndlsefc","colab":{}},"source":["def mask_name_fn(row):\n","  col = 'img'\n","  mn = f'{row[col][:-4]}_mask.gif'\n","  return mn\n","\n","#----------------------------------------------------\n","# Image Segmentation preparation pipeline\n","#----------------------------------------------------\n","class ImageSegmentationDataBundle(DataBundle):\n","  def __init__(self, csv_path, img_folder_path, mask_folder_path, resize_sz=128, bs=6):\n","    print ('--------- Image Segmentation DataBundle init', csv_path, img_folder_path, mask_folder_path)\n","\n","    load_params = {'source': CSVItemContainer, 'target_cls': DfItemList, 'csv_path': csv_path}\n","    split_params = {'split_procedure': 'split_sequential', 'train_ratio': 0.8, 'valid_ratio': 0.2}\n","    extract_x_params = {'extract_procedure': 'extract_col', 'target_cls': ImageFileItemList, 'col': 'img', 'folder_path': img_folder_path}\n","    extract_y_params = {'extract_procedure': 'extract_custom', 'target_cls': ImageFileItemList, 'folder_path': mask_folder_path, 'custom_fn': mask_name_fn}\n","    convert_x_params = [\n","        {'target_cls': ImageItemList, 'convert_procedure': 'FileToImage', 'pair_type': 'mask'}, \n","    ]\n","    convert_y_params = [\n","        {'target_cls': ImageItemList, 'convert_procedure': 'FileToImage'}, \n","    ]\n","    xform_x_params = [\n","        {'xform_procedure': 'make_rgb'}, \n","        {'xform_procedure': 'resize', 'size': resize_sz},\n","        {'xform_procedure': 'flip_rotate'},\n","        #{'xform_procedure': 'perspective_warp', 'crop_size': 100},\n","        #{'xform_procedure': 'aug', 'aug_name': 'Horizontal Flip'},\n","        #{'xform_procedure': 'aug', 'aug_name': 'Rotate'},\n","        {'xform_procedure': 'aug', 'aug_name': 'Random Brightness Contrast'},\n","        {'xform_procedure': 'to_byte_tensor'},\n","        {'xform_procedure': 'to_float_tensor'},\n","        {'xform_procedure': 'normalise'},\n","    ]\n","    ds_params = {'target_ds': ILPairedDataset}\n","    dl_params = (\n","        {'bs': bs, 'sampler_fn': RandomSampler},     # for training\n","        {'bs': bs, 'sampler_fn': SequentialSampler}  # for valid/test\n","    )\n","    self.post_proc_params = [\n","        # Normalise with mean/std as specified by pre-trained Resnet\n","        {'proc_procedure': 'set_mean_std', 'mean': [0.485, 0.456, 0.406], 'std':[0.229, 0.224, 0.225]}, \n","    ]\n","    self.display_params = {\n","        'layout_procedure': 'display_images', 'figsize': (20, 5),\n","        'xyz_procedures': ('image', 'mask', 'label')\n","    }\n","    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=xform_x_params, ds_params=ds_params, dl_params=dl_params)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0s6fY5mK4hf","colab_type":"text"},"source":["### Define Image Segmentation application class "]},{"cell_type":"code","metadata":{"id":"t5hWAJmEvUEf","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Image Segmentation Application\n","#----------------------------------------------------\n","class AppImageSegmentation():\n","\n","  def __init__(self):\n","    self._arch = None\n","    self.db = None\n","\n","  # ----------------------------\n","  # Prepare a subset of the images for rapid testing during development\n","  # ----------------------------\n","  def _subset_data(self, masks_file_path, imgs_path, masks_path, subset_imgs_path, subset_masks_path, num_subset=80):\n","    masks_df = pd.read_csv(masks_file_path)\n","    subset_df = masks_df.iloc[:num_subset]\n","    subset_df.to_csv(subset_imgs_path / masks_file_path.name, index=False)\n","\n","    for i in range(num_subset):\n","      img_file_name = masks_df.iloc[i]['img']\n","      mask_file_name = f'{img_file_name[:-4]}_mask.gif'\n","      img_file = imgs_path / img_file_name\n","      mask_file = masks_path / mask_file_name\n","      \n","      shutil.copy(img_file, subset_imgs_path)\n","      shutil.copy(mask_file, subset_masks_path)\n","\n","    return subset_df\n","\n","  # ----------------------------\n","  # Open an image file, resize it and save it\n","  # ----------------------------\n","  @staticmethod\n","  def _resize_file(fn, sz, dest_path):\n","    Image.open(fn).resize((sz, sz)).save(dest_path/fn.name)\n","\n","  # ----------------------------\n","  # Reduce the image size to a smaller size\n","  # ----------------------------\n","  def _resize_data(self, orig_imgs_path, orig_masks_path, out_imgs_path, out_masks_path):\n","    sz=128\n","\n","    orig_mask_files = list(orig_masks_path.iterdir())\n","    resize_masks = partial(self._resize_file, sz=sz, dest_path=out_masks_path)\n","    with ThreadPoolExecutor(8) as e: e.map(resize_masks, orig_mask_files)\n","\n","    orig_img_files = list(orig_imgs_path.iterdir())\n","    resize_imgs = partial(self._resize_file, sz=sz, dest_path=out_imgs_path)\n","    with ThreadPoolExecutor(8) as e: e.map(resize_imgs, orig_img_files)\n","\n","  # ----------------------------\n","  # Pre-process data by resizing images to a manageable size and creating a subset\n","  # of the data set\n","  # ----------------------------\n","  def pre_process_data(self, full_masks_file_path, orig_imgs_path, orig_masks_path, out_imgs_path, out_masks_path, subset_imgs_path, subset_masks_path):\n","    num_out_imgs = len(list(out_imgs_path.iterdir()))\n","    if (orig_masks_path.is_dir() and (num_out_imgs < 10)):\n","      self._resize_data(orig_imgs_path, orig_masks_path, out_imgs_path, out_masks_path)\n","    \n","    num_subset_imgs = len(list(subset_imgs_path.iterdir()))\n","    if (num_subset_imgs < 10):\n","      self._subset_data(full_masks_file_path, out_imgs_path, out_masks_path, subset_imgs_path, subset_masks_path)\n","\n","  # ----------------------------\n","  # Load the data using the Image Segmentation Data Bundle\n","  # ----------------------------\n","  def load_data(self, csv_path, img_folder_path, mask_folder_path, **kwargs):\n","    self.db = ImageSegmentationDataBundle(csv_path, img_folder_path, mask_folder_path, **kwargs)\n","    self.db.do()\n","\n","  # ----------------------------\n","  # Create the architecture\n","  # ----------------------------\n","  def create_arch(self):\n","    self._arch = ArchImageSegmentationUnet()\n","    self._arch.create_model()\n","    return self._arch\n","\n","  # ----------------------------\n","  # Create a basic architecture with a simple upsampling decoder\n","  # ----------------------------\n","  def create_basic_arch(self):\n","    self._arch = ArchImageSegmentationBasic()\n","    self._arch.create_model()\n","    return self._arch\n","\n","  # ----------------------------\n","  # Train the model\n","  # ----------------------------\n","  def run_train(self, freeze_on=False, split_lr=None, split=False, one_cycle=False, num_epochs=1):\n","    assert(isinstance(freeze_on, bool))\n","    assert(isinstance(split, bool))\n","    assert(isinstance(one_cycle, bool))\n","\n","    train_dl = self.db.train_dl\n","    valid_dl = self.db.valid_dl\n","\n","    # Loss function\n","    loss_func = nn.BCEWithLogitsLoss()\n","\n","    # Model\n","    arch = self._arch\n","    model = arch.model\n","    lr_groups = arch.module_groups()\n","    arch.freeze(freeze_on)\n","\n","    split_lr = split_lr if (split_lr is not None) else [4e-2]\n","    opt, hyper_cbs = HyperParams.set(model, lr_groups, split_lr, split, one_cycle, opt_func=adam_opt_func)\n","\n","    # Debug Tracker\n","    dtr = DebugTracker(max_count=20, disp=(True, True))\n","    debug_cbs = [dtr, DebugYhatLossCB(fwd=False)]\n","\n","    # Compute accuracy\n","    metrics_dict = {\"acc\": accuracy_thresh, \"dice\": dice}\n","    callbs=[CudaCB(device = torch.device('cuda',0)), Recorder(), ProgressCallback(), MetricsCB(metrics_dict)]\n"," \n","    callbs += hyper_cbs + debug_cbs\n","\n","    loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs, dtr=dtr)\n","    loop.fit(num_epochs=num_epochs)\n","    return loop\n","\n","  # ----------------------------\n","  # Make some prediction inferences with the trained model\n","  # ----------------------------\n","  def run_predict(self):\n","    valid_dl = self.db.valid_dl\n","    self._arch.model.eval()\n","    device = list(self._arch.model.parameters())[0].device\n","\n","    inps, outs, targs = [], [], []\n","    with torch.no_grad():\n","      for _, (xb, yb) in enumerate(valid_dl):\n","        xb = xb.to(device)\n","        yhat = self._arch.model(xb)\n","\n","        for x, y, p in zip (xb, yb, yhat):\n","          inps.append(x.cpu())\n","          outs.append(p.cpu())\n","          targs.append(y.cpu())\n","    return inps, outs, targs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8_wpoU7DpT2b","colab_type":"text"},"source":["### Pre-process Kaggle data - Resize smaller images and Create subset of data"]},{"cell_type":"code","metadata":{"id":"oFz7dTYqTwyS","colab_type":"code","colab":{}},"source":["ispp_app = AppImageSegmentation()\n","ispp_app.pre_process_data(full_masks_file_path, orig_imgs_path, orig_masks_path, full_imgs_path, full_masks_path, subset_imgs_path, subset_masks_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"liz3yvXzbXUJ","colab_type":"code","colab":{}},"source":["!ls {full_imgs_path}/154ee2b6d27a*.jpg\n","!ls {orig_masks_path}/154ee2b6d27a*"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIb6Zn9ostK3","colab_type":"code","colab":{}},"source":["subset_df = pd.read_csv(subset_imgs_path / full_masks_file_path.name)\n","subset_df\n","!ls '{subset_imgs_path}' '{subset_masks_path}'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUMZfq3WymJN","colab_type":"text"},"source":["### Load data"]},{"cell_type":"code","metadata":{"id":"R4XDhqhqzYm4","colab_type":"code","colab":{}},"source":["load_full = True\n","if (load_full):\n","  load_masks_file_path = full_masks_file_path\n","  load_masks_path = full_masks_path\n","  load_imgs_path = full_imgs_path\n","  load_bs = 64\n","else:\n","  load_masks_file_path = subset_imgs_path/'train_masks.csv'\n","  load_masks_path = subset_masks_path\n","  load_imgs_path = subset_imgs_path\n","  load_bs = 6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlI4Bb3wc0_6","colab_type":"code","colab":{}},"source":["is_app = AppImageSegmentation()\n","is_app.load_data(load_masks_file_path, load_imgs_path, load_masks_path, bs=load_bs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Wqrq0-CEW6m","colab_type":"code","colab":{}},"source":["is_app.db.display_batch()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcCsDyCn5WqX","colab_type":"text"},"source":["### Basic Model - Test run"]},{"cell_type":"code","metadata":{"id":"hpbyToE6y9OE","colab_type":"code","colab":{}},"source":["is_app.create_basic_arch()\n","loop = is_app.run_train(num_epochs=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mq28ktNNBvih","colab_type":"code","colab":{}},"source":["recorder=loop.cbs[1]\n","recorder.plot_lr(), recorder.plot_loss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xx2kd7QT9VmT","colab_type":"code","colab":{}},"source":["is_run_df, is_batch_df, is_layer_df, _ = loop.dtr.pd_results()\n","is_run_df\n","is_batch_df\n","is_layer_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pqaS_mrODFj9","colab_type":"code","colab":{}},"source":["inps, outs, targs = is_app.run_predict()\n","is_app.db.display_results(inps, targs, outs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fVftt3fdW8yT","colab_type":"text"},"source":["### Basic Model - Freeze Encoder, Train, Unfreeze and Retrain"]},{"cell_type":"code","metadata":{"id":"KOf2Ynj9U6a7","colab_type":"code","colab":{}},"source":["is_app.create_basic_arch()\n","loop = is_app.run_train(freeze_on=True, split_lr=[4e-2], one_cycle=True, num_epochs=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rp4nE2IYU3cT","colab_type":"code","colab":{}},"source":["lr=1e-2\n","loop = is_app.run_train(freeze_on=False, split_lr=[lr/100,lr/10,lr], split=True, one_cycle=True, num_epochs=20)\n","is_app._arch.save_weights(basic_model_128_path)\n","recorder=loop.cbs[1]\n","recorder.plot_lr()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZIRKqKZeTz4m","colab_type":"code","colab":{}},"source":["inps, outs, targs = is_app.run_predict()\n","is_app.db.display_results(inps, targs, outs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bmztacepXdwf","colab_type":"text"},"source":["### Basic Model - Resize to larger images - (512, 512) and (1024, 1024)"]},{"cell_type":"code","metadata":{"id":"g_g-cOYyW78Y","colab_type":"code","colab":{}},"source":["is_app = AppImageSegmentation()\n","is_app.load_data(load_masks_file_path, load_imgs_path, load_masks_path, resize_sz=512, bs=16)\n","\n","is_app.create_basic_arch()\n","is_app._arch.load_weights(basic_model_128_path)\n","\n","loop = is_app.run_train(freeze_on=True, split_lr=[4e-2], one_cycle=True, num_epochs=5)\n","lr=1e-2\n","loop = is_app.run_train(freeze_on=False, split_lr=[lr/100,lr/10,lr], split=True, one_cycle=True, num_epochs=8)\n","\n","is_app._arch.save_weights(basic_model_512_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQ8TbvURXvZ2","colab_type":"code","colab":{}},"source":["is_app = AppImageSegmentation()\n","is_app.load_data(load_masks_file_path, load_imgs_path, load_masks_path, resize_sz=1024, bs=4)\n","\n","is_app.create_basic_arch()\n","is_app._arch.load_weights(basic_model_512_path)\n","\n","loop = is_app.run_train(freeze_on=True, split_lr=[4e-2], one_cycle=True, num_epochs=2)\n","lr=5e-3\n","loop = is_app.run_train(freeze_on=False, split_lr=[lr/100,lr/10,lr], split=True, one_cycle=True, num_epochs=4)\n","\n","is_app._arch.save_weights(basic_model_1024_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IzHDkDBUTEh2","colab_type":"text"},"source":["### Unet Model - Test run"]},{"cell_type":"code","metadata":{"id":"Glog1wBhTG7S","colab_type":"code","colab":{}},"source":["is_app.create_arch()\n","loop = is_app.run_train(num_epochs=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DPjP1Nr-Ymhn","colab_type":"code","colab":{}},"source":["is_app._arch.summary(128)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yiXR3FC-cryo","colab":{}},"source":["recorder=loop.cbs[1]\n","recorder.plot_lr(), recorder.plot_loss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XM67R2Mpcry3","colab":{}},"source":["is_run_df, is_batch_df, is_layer_df, _ = loop.dtr.pd_results()\n","is_run_df\n","is_batch_df\n","is_layer_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3w_OITd8ek9e","colab":{}},"source":["inps, outs, targs = is_app.run_predict()\n","is_app.db.display_results(inps, targs, outs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1iINbctmegml"},"source":["### Unet Model - Freeze Encoder, Train, Unfreeze and Retrain"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9cuxdWoVegmt","colab":{}},"source":["is_app.create_arch()\n","loop = is_app.run_train(freeze_on=True, split_lr=[4e-2], one_cycle=True, num_epochs=8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OEZMMJILegm0","colab":{}},"source":["lr=1e-2\n","loop = is_app.run_train(freeze_on=False, split_lr=[lr/100,lr/10,lr], split=True, one_cycle=True, num_epochs=20)\n","is_app._arch.save_weights(unet_model_128_path)\n","recorder=loop.cbs[1]\n","recorder.plot_lr()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8YxcZgT2egm3","colab":{}},"source":["inps, outs, targs = is_app.run_predict()\n","is_app.db.display_results(inps, targs, outs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8bRliR7Wmqxb"},"source":["### Unet Model - Resize to larger images - (512, 512) and (1024, 1024)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7rkm_4fpmqxt","colab":{}},"source":["is_app = AppImageSegmentation()\n","is_app.load_data(load_masks_file_path, load_imgs_path, load_masks_path, resize_sz=512, bs=16)\n","\n","is_app.create_arch()\n","is_app._arch.load_weights(unet_model_128_path)\n","\n","loop = is_app.run_train(freeze_on=True, split_lr=[4e-2], one_cycle=True, num_epochs=5)\n","lr=1e-2\n","loop = is_app.run_train(freeze_on=False, split_lr=[lr/100,lr/10,lr], split=True, one_cycle=True, num_epochs=8)\n","\n","is_app._arch.save_weights(unet_model_512_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ei3sH4vvmqx4","colab":{}},"source":["is_app = AppImageSegmentation()\n","is_app.load_data(load_masks_file_path, load_imgs_path, load_masks_path, resize_sz=1024, bs=4)\n","\n","is_app.create_arch()\n","is_app._arch.load_weights(unet_model_512_path)\n","\n","loop = is_app.run_train(freeze_on=True, split_lr=[4e-2], one_cycle=True, num_epochs=2)\n","lr=4e-3\n","loop = is_app.run_train(freeze_on=False, split_lr=[lr/200,lr/30,lr], split=True, one_cycle=True, num_epochs=4)\n","\n","is_app._arch.save_weights(basic_model_1024_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4m3XpDco-iqS","colab_type":"text"},"source":["### Setup Tensorboard"]},{"cell_type":"code","metadata":{"id":"gMjOnrRTFQtk","colab_type":"code","colab":{}},"source":["!rm -r tbtry/Run-0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KxvL08OFE8zX","colab_type":"code","colab":{}},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","%tensorboard --logdir tbtry"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7nw-l32zTIRo","colab_type":"text"},"source":["### Obsolete"]},{"cell_type":"code","metadata":{"id":"akJ8fvsfGVdn","colab_type":"code","colab":{}},"source":["# Run this cell and select the kaggle.json file downloaded\n","# from the Kaggle account settings page.\n","from google.colab import files\n","files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDLXrqcNGlU7","colab_type":"code","colab":{}},"source":["# Let's make sure the kaggle.json file is present.\n","!ls -lha kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QITWh1N5GoqG","colab_type":"code","colab":{}},"source":["# Next, install the Kaggle API client after forcing an upgrade\n","!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6\n","!kaggle -v\n","\n","# Reason for doing a force-upgrade. The underlying problem: Colab installs both py2 and py3 \n","# packages, and (for historical reasons) the py2 packages are installed second. kaggle is a \n","# wrapper installed by the kaggle python package; since we do py2 second, the py2 wrapper \n","# is in /usr/local/bin, and happens to be an older version."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KixiXyagGy7Y","colab_type":"code","colab":{}},"source":["# The Kaggle API client expects this file to be in ~/.kaggle,\n","# so move it there.\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","\n","# This permissions change avoids a warning on Kaggle tool startup.\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPZaJ7oOGuyh","colab_type":"code","colab":{}},"source":["# List available datasets.\n","!kaggle competitions list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hyJ4Rb5HMwH","colab_type":"code","colab":{}},"source":["# First, you have to login to Kaggle, go to that competition's page, navigate to \n","# the Rules tab and accept the terms and conditions. Unless you do that, you will get\n","# a 403-Forbidden error when you run the command below\n","\n","# Copy the carvana data set locally.\n","!kaggle competitions download -c carvana-image-masking-challenge"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8FgC4GlvHZQ1","colab_type":"code","colab":{}},"source":["# It is a gigantic 24GB zip file, so delete some of the largest unnecessary files\n","# to reduce it to 8.7GB and save some disk space\n","!zip -d carvana-image-masking-challenge.zip test_hq.zip\n","!zip -d carvana-image-masking-challenge.zip train_hq.zip\n","!zipinfo carvana-image-masking-challenge.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"omjhu7fKhv8r","colab_type":"code","colab":{}},"source":["!unzip -j carvana-image-masking-challenge.zip metadata.csv.zip train_masks.csv.zip train.zip train_masks.zip -d {data_path}\n","!unzip \"carvana/*.zip\" -d {data_path} >> /dev/null\n","!rm {data_path}/*.zip\n","!ls -l {data_path}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KMeE2kmFue9M","colab_type":"code","colab":{}},"source":["def resize_files(fn, dest_path):\n","  Image.open(fn).resize((128,128)).save(dest_path/fn.name)\n","\n","orig_mask_files = list(orig_masks_path.iterdir())\n","resize_masks = partial(resize_files, dest_path=full_masks_path)\n","with ThreadPoolExecutor(8) as e: e.map(resize_masks, orig_mask_files)\n","\n","orig_img_files = list(orig_imgs_path.iterdir())\n","resize_imgs = partial(resize_files, dest_path=full_imgs_path)\n","with ThreadPoolExecutor(8) as e: e.map(resize_imgs, orig_img_files)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ACnAry61ohUo","colab_type":"code","colab":{}},"source":["def subset_images(masks_file_path, imgs_path, masks_path, subset_imgs_path, subset_masks_path, num_subset=80):\n","  masks_df = pd.read_csv(masks_file_path)\n","  subset_df = masks_df.iloc[:num_subset]\n","  subset_df.to_csv(subset_imgs_path / masks_file_path.name, index=False)\n","\n","  for i in range(num_subset):\n","    img_file_name = masks_df.iloc[i]['img']\n","    mask_file_name = f'{img_file_name[:-4]}_mask.gif'\n","    img_file = imgs_path / img_file_name\n","    mask_file = masks_path / mask_file_name\n","    \n","    shutil.copy(img_file, subset_imgs_path)\n","    shutil.copy(mask_file, subset_masks_path)\n","\n","  return subset_df\n","\n","subset_df = subset_images(full_masks_file_path, full_imgs_path, full_masks_path, temp_subset_imgs_path, temp_subset_masks_path, num_subset=5)\n","#subset_df = subset_images(full_masks_file_path, full_imgs_path, full_masks_path, subset_imgs_path, subset_masks_path, num_subset=80)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UGDGGS_enaMI","colab_type":"code","colab":{}},"source":["class ShowData():\n","\n","  # Can display both image tensors and PIL image objects\n","  def show_image(self, img, ax):\n","    ax.axis('off')\n","    if (isinstance(img, torch.Tensor)):\n","      img = img.permute(1,2,0)\n","    ax.imshow(img)\n","\n","  def show_label(self, label, ax):\n","    ax.set_title(f'{label}')\n","\n","  def show_mask(self, mask, ax):\n","    mask = mask.convert('RGBA')\n","    ax.imshow(mask, alpha=0.7, cmap=\"Reds\")\n","\n","  # Can display both image tensors and PIL image objects\n","  def show_images(self, x_imgs, y_labels, img_type='label', num_cols=10, figsize=None, **kwargs):\n","    assert(len(x_imgs) == len(y_labels))\n","\n","    num_imgs = len(x_imgs)\n","    num_rows = int (math.ceil (num_imgs / num_cols))\n","    if (figsize is None):\n","      figsize=(num_cols * 3, num_rows * 3)\n","    fig,axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n","    for img, label, ax in zip (x_imgs, y_labels, axes.flat):\n","      self.show_image(img, ax)\n","      if (img_type == 'label'):\n","        self.show_label(label, ax)\n","      elif (img_type == 'mask'):\n","        self.show_mask(label, ax)\n","\n","sd=ShowData()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5s95O1BhkMn3","colab_type":"code","colab":{}},"source":["\n","#----------------------------------------------------\n","# Image Segmentation preparation pipeline\n","#----------------------------------------------------\n","class OldImageSegmentationDataBundle(DataBundle):\n","  def __init__(self, csv_path, img_folder_path, mask_folder_path):\n","    print ('--------- Image Segmentation DataBundle init', csv_path, img_folder_path, mask_folder_path)\n","\n","    # Load all rows from the given CSV file\n","    # Split sequentially. based on a percentage ratio for training and validation. \n","    #   We do this sequentially rather than randomly because each car is in a set of 16 \n","    #   images taken from different angles. And for a particular car we don't want some\n","    #   of those images to be in the training set and some in the validation set. Otherwise\n","    #   it will validate on the same car on which it got trained and will give artificially\n","    #   good validation results. To address this we make sure that the entire set of images\n","    #   for a particular car is entirely in training or entirely in validation. Since the\n","    #   items in our source CSV file are sorted and grouped by car, the set of images for a particular\n","    #   car appear sequentially in that file. So when we split sequentially we are ensuring that the\n","    #   first 'n' car sets are all in training and the next 'm' car sets are all in validation.\n","    # 'x' items are taken from 'img' column as image file names and\n","    # 'y' labels are taken from 'img' column and then transformed into mask file names\n","    # Convert the 'x' items from Image Files to Images\n","    # Convert the 'y' items from Image Files to Images\n","    # At runtime, dynamically read an Image and apply some image processing steps. Finally\n","    # convert to tensors of floats\n","\n","    load_params = {'source': CSVItemContainer, 'target_cls': DfItemList, 'csv_path': csv_path}\n","    split_params = {'split_procedure': 'split_sequential', 'train_ratio': 0.8, 'valid_ratio': 0.2}\n","    extract_x_params = {'extract_procedure': 'extract_col', 'target_cls': ImageFileItemList, 'col': 'img', 'folder_path': img_folder_path}\n","    extract_y_params = {'extract_procedure': 'extract_custom', 'target_cls': ImageFileItemList, 'folder_path': mask_folder_path, 'custom_fn': mask_name_fn}\n","    convert_x_params = [\n","        {'target_cls': ImageItemList, 'convert_procedure': 'FileToImage'}, \n","    ]\n","    convert_y_params = [\n","        {'target_cls': ImageItemList, 'convert_procedure': 'FileToImage'}, \n","    ]\n","    xform_x_params = [\n","        {'xform_procedure': 'make_rgb'}, \n","        {'xform_procedure': 'resize', 'size': 128},\n","        {'xform_procedure': 'to_byte_tensor'},        \n","        {'xform_procedure': 'to_float_tensor'}\n","        # Example only {'xform_procedure': 'Custom', 'custom_fn': func}        \n","    ]\n","    # !!!! Need to xform the masks similar to the image\n","    dl_params = (\n","        {'bs': 6, 'sampler_fn': RandomSampler, 'collate_fn': collate},     # for training\n","        {'bs': 6, 'sampler_fn': SequentialSampler, 'collate_fn': collate}  # for valid/test\n","    )\n","    self.show_params = {\n","        'layout_procedure': 'show_images', 'figsize': (20, 5),\n","        'xyz_procedures': ('image', 'mask', 'label')\n","    }\n","    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, xform_x_params=xform_x_params, dl_params=dl_params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dgFEVfrXC7nf","colab_type":"code","colab":{}},"source":["# All pre-trained models expect input images normalized in the same way, i.e. mini-batches of \n","# 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The \n","# images have to be loaded in to a range of [0, 1] and \n","# then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]. \n","\n","class MyFuncLayer(nn.Module):\n","    def __init__(self, func):\n","        super().__init__()\n","        self.func = func\n","\n","    def forward(self, x): \n","      ret = self.func(x)\n","      return ret"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkK0yQPBUaux","colab_type":"code","colab":{}},"source":["is_app._arch.freeze()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2Knc_PNDwsa","colab_type":"code","colab":{}},"source":["ShowImg.show_grid(inps[:5], outs[:5], y_method=ShowImg.show_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eVD0VuB4NHyS","colab_type":"code","colab":{}},"source":["outs[0][80, :60], targs[0][80, :60]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VjOGjYZmLpHW","colab_type":"code","colab":{}},"source":["def smask(img):\n","    _, ax = plt.subplots(1, 1)\n","    ax.axis('off')\n","    #if (isinstance(img, torch.Tensor)):\n","    #  img = img.permute(1,2,0)\n","    ax.imshow(img)\n","\n","smask(outs[0] > 0)\n","smask(targs[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUadoCz-5Iz6","colab_type":"code","colab":{}},"source":["def tryit():\n","  loop = is_app.run_train(num_epochs=5)\n","\n","tryit()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7aWLj3VKv91R","colab_type":"code","colab":{}},"source":["torch.manual_seed(0)\n","tt_app = AppImageSegmentation()\n","tt_app.load_data(data_file_path, bs=64)\n","tt_app.load_emb(emb_x_wgts_path, emb_y_wgts_path)\n","tt_app.create_arch()\n","loop = tt_app.run_train(num_epochs=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gfUKwGdIR3MX","colab_type":"text"},"source":["**Inspect Model**"]},{"cell_type":"code","metadata":{"id":"qmvKReI7j4yG","colab_type":"code","colab":{}},"source":["summary(is_app._arch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YNShfxRM25-x","colab_type":"code","colab":{}},"source":["from torchsummary import summary\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","my_res = models.resnet34(pretrained=True).to(device)\n","summary(my_res, input_size=(3, 128, 128))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdsTngmB4vAf","colab_type":"code","colab":{}},"source":["my_model = is_app._arch.model.to(device)\n","summary(my_model, input_size=(3, 224, 224))"],"execution_count":null,"outputs":[]}]}