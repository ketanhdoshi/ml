{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Geo Location.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["ySwr9rRWGvdz","D5JWSsSoHKP3","uthKJeEh8-f8","zwjpd3gs_fHW","V2ULS-h6AgJ-","S7OgU4Z9L25L","WyE07GvNNUoc","i5F6IKz6xnLg","yEsyiocsyyWc","96fZZfsCl3VU","mlKPllcFlqPG","Uc7mWgJoLHYr","-59J81t95yLl"],"authorship_tag":"ABX9TyNnr8hvZ3LgpHAXgVSxLs/y"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oN6cRUObVRh0"},"source":["## Download NYC Taxi data"]},{"cell_type":"markdown","metadata":{"id":"bKVkzLxQ9P_8","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"kI5e1uoLdVMQ","colab_type":"code","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vufwd39Jj3bJ","colab_type":"code","colab":{}},"source":["!pip install geo-py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mrhetkgLiIL4","colab_type":"code","colab":{}},"source":["import IPython.core.debugger as db\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import plotly.express as px\n","import time \n","\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import glob\n","from PIL import Image\n","\n","from sklearn.cluster import KMeans, MiniBatchKMeans\n","from sklearn.metrics import mean_squared_log_error, mean_squared_error\n","\n","from sklearn.ensemble import RandomForestRegressor\n","from lightgbm import LGBMRegressor\n","\n","from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, GridSearchCV\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn import metrics\n","from sklearn import mixture\n","\n","import geo.sphere \n","\n","%matplotlib inline\n","\n","plt.style.use('fivethirtyeight')\n","plt.rcParams['figure.figsize'] = (12,8)\n","pd.options.display.max_rows = 100\n","pd.options.display.max_columns = 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RzqxrSZZ6SC","colab_type":"text"},"source":["### Data Paths"]},{"cell_type":"code","metadata":{"id":"G2MgyvFsZ433","colab_type":"code","colab":{}},"source":["data_path = Path.cwd()/'nyc'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMC2GmeSVURq","colab_type":"text"},"source":["### Kaggle Data Download"]},{"cell_type":"code","metadata":{"id":"Qh_3u9DXOEJu","colab_type":"code","colab":{}},"source":["def kaggle_data(data_type, data_name):\n","  # Run this cell and select the kaggle.json file downloaded\n","  # from the Kaggle account settings page.\n","  from google.colab import files\n","  files.upload()\n","\n","  # Let's make sure the kaggle.json file is present.\n","  !ls -lha kaggle.json\n","\n","  # Next, install the Kaggle API client after forcing an upgrade\n","  !pip uninstall -y kaggle\n","  !pip install --upgrade pip\n","  !pip install kaggle==1.5.6\n","  !kaggle -v\n","\n","  # Reason for doing a force-upgrade. The underlying problem: Colab installs both py2 and py3 \n","  # packages, and (for historical reasons) the py2 packages are installed second. kaggle is a \n","  # wrapper installed by the kaggle python package; since we do py2 second, the py2 wrapper \n","  # is in /usr/local/bin, and happens to be an older version.\n","\n","  # The Kaggle API client expects this file to be in ~/.kaggle,\n","  # so move it there.\n","  !mkdir -p ~/.kaggle\n","  !cp kaggle.json ~/.kaggle/\n","\n","  # This permissions change avoids a warning on Kaggle tool startup.\n","  !chmod 600 ~/.kaggle/kaggle.json\n","\n","  # List available datasets.\n","  !kaggle {data_type} list\n","\n","  # First, you have to login to Kaggle, go to that competition's page, navigate to \n","  # the Rules tab and accept the terms and conditions. Unless you do that, you will get\n","  # a 403-Forbidden error when you run the command below\n","\n","  # Copy the carvana data set locally.\n","  !kaggle {data_type} download {data_name}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sq5D3KfPQ939","colab_type":"code","colab":{}},"source":["kaggle_data_type = 'competitions'\n","kaggle_data_name = 'nyc-taxi-trip-duration'\n","kaggle_data(kaggle_data_type, kaggle_data_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6ckZFYZTERc","colab_type":"code","colab":{}},"source":["def nyc_extract(zip_file, out_path):\n","  !zipinfo {zip_file}\n","\n","  !unzip {zip_file} -d {out_path} >> /dev/null\n","  !unzip {out_path}/'train.zip' -d {out_path} >> /dev/null\n","  !unzip {out_path}/'test.zip' -d {out_path} >> /dev/null\n","  !ls -l {out_path}\n","\n","nyc_extract(f'{kaggle_data_name}.zip', data_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C79fYM7fiAIB","colab_type":"text"},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"JY_4MnOCi_ck","colab_type":"code","colab":{}},"source":["train_file = data_path/'train.csv'\n","df = pd.read_csv(train_file)\n","df.shape\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enlLsgJRetLD","colab_type":"text"},"source":["## Geo Location NYC Taxi data with Light GBM - [article](https://medium.com/analytics-vidhya/feature-engineering-with-geospatial-data-predicting-nyc-cab-trip-duration-a121ec16021b), [notebook](https://github.com/claudian37/DS_Portfolio/blob/master/NYC_cab_dataset/02_Modeling_NYC_Cab_final.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"W9QQ5AuWFh3G","colab_type":"text"},"source":["### Explore Target "]},{"cell_type":"code","metadata":{"id":"PzRDHAXXFlgt","colab_type":"code","colab":{}},"source":["target='trip_duration'\n","df[target].mean()\n","sns.kdeplot(df[target])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0FTLbZkXF-vh","colab_type":"code","colab":{}},"source":["# Take Log Transformation of Target LabelT. Trip durations are highly skewed (left histogram), \n","# with values concentrated near zero and a few outliers of long trips\n","fig, ax = plt.subplots(1,2)\n","fig.set_size_inches(12,6)\n","foo = ax[0].hist(df[target], bins=30, label='original', color='red')\n","foo = ax[1].hist(np.log1p(df[target]), bins=30, label='log')\n","foo = fig.suptitle(target)\n","foo = fig.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0e1pojRGGgUz","colab_type":"code","colab":{}},"source":["df['trip_duration_log'] = np.log1p(df['trip_duration'])\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ySwr9rRWGvdz","colab_type":"text"},"source":["### Nulls"]},{"cell_type":"code","metadata":{"id":"fSRCU9TeGus7","colab_type":"code","colab":{}},"source":["# There are no NULL data values\n","null_summary = pd.concat((df.isnull().sum(), df.isnull().sum()/df.shape[0]), axis=1)\n","null_summary.columns = ['actual', 'pct']\n","null_summary['dtype'] = df.dtypes\n","null_summary"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5JWSsSoHKP3","colab_type":"text"},"source":["### Model - Baseline"]},{"cell_type":"code","metadata":{"id":"UHXd-2LKHMi-","colab_type":"code","colab":{}},"source":["# RMSLE for predicting mean of trip duration\n","np.sqrt(mean_squared_log_error(df['trip_duration'], np.repeat(df['trip_duration'].mean(), df.shape[0])))\n","\n","# RMSLE for predicting mean of log of trip duration\n","np.sqrt(mean_squared_log_error(np.exp(df['trip_duration_log']), np.exp(np.repeat(df['trip_duration_log'].mean(), df.shape[0]))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y71ksqrvHgzP","colab_type":"code","colab":{}},"source":["target = 'trip_duration_log'\n","\n","numerical_cols = df.dtypes[df.dtypes != 'object'].index\n","# Remove the target columns\n","numerical_cols = numerical_cols.drop(['trip_duration_log', 'trip_duration'])\n","\n","# Naive model - No feature engineering; just modeling with cross validation. Use a small sample of 10k observations to see how random forest does\n","rf_cv_mean = np.sqrt(-cross_val_score(estimator=RandomForestRegressor(),\n","                X=df[numerical_cols].sample(10000, random_state=1), \n","                y=df[target].sample(10000, random_state=1),\n","                scoring='neg_mean_squared_error').mean())\n","print(f'Random Forest CV Mean RMSLE: {rf_cv_mean}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uthKJeEh8-f8","colab_type":"text"},"source":["### Feature Engineering"]},{"cell_type":"code","metadata":{"id":"BquLdfIlG632","colab_type":"code","colab":{}},"source":["# Time-based Features - transform pickup_datetime column into features by month, day, day_of_week, hour_of_day, minute_of_hour.\n","# Use holidays package to create binary feature to flag dates as holidays in NY state.\n","\n","import holidays\n","class FeaturizeTime(object):\n","    def fit(self, X, y):\n","        return self\n","        \n","    def transform(self, X):\n","        X = X.copy()\n","        # Engineer temporal features\n","        X['pickup_datetime'] = pd.to_datetime(X['pickup_datetime'])\n","        X['month'] = X['pickup_datetime'].dt.month\n","        X['day'] = X['pickup_datetime'].dt.day\n","        X['day_of_week'] = X['pickup_datetime'].dt.dayofweek\n","        X['hour_of_day'] = X['pickup_datetime'].dt.hour\n","        X['minute_of_hour'] = X['pickup_datetime'].dt.minute\n","        \n","        # Get holidays in NY state in 2016\n","        us_holidays = holidays.US(state='NY', years=2016)\n","        X['is_holiday'] = X['pickup_datetime'].dt.date.apply(lambda x: 1 if x in us_holidays.keys() else 0)\n","        \n","        # Drop time column in final df\n","        X = X.drop(['pickup_datetime'], axis=1)\n","        print('Time-based features transformed')\n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XkuvIJ4VLqMW","colab_type":"code","colab":{}},"source":["# Calculate Distance Between Coordinates using geo-py package:\n","# Great Circle Distance between pickup and dropoff locations (great_circle_distance): this is the shortest distance between two points on a sphere.\n","# Manhattan Distance between pickup and dropoff locations (manhattan_distance)\n","# Bearing between pickup and dropoff locations (bearing): this is the direction from the pickup location to dropoff location.\n","\n","class CalculateDistances(object):\n","    def fit(self, X, y):\n","        return self\n","    \n","    def transform(self, X):\n","        X = X.copy()\n","        \n","        ## 1) Find min distance between pickup and dropoff coordinates \n","        X['great_circle_distance'] = X.apply(lambda x: self._calculate_great_circle_distance(x['pickup_latitude'], x['pickup_longitude'], \n","                                                                       x['dropoff_latitude'], x['dropoff_longitude']),\n","                                                                       axis=1)\n","        \n","        ## 2) Calculate manhattan distance between pickup and dropoff coordinates\n","        X['manhattan_distance'] = X.apply(lambda x: self._calculate_manhattan_distance(x['pickup_latitude'], x['pickup_longitude'], \n","                                                                       x['dropoff_latitude'], x['dropoff_longitude']),\n","                                                                       axis=1)\n","        \n","        ## 3) Calculate bearing between pickup and dropoff latitude\n","        X['bearing'] = X.apply(lambda x: self._calculate_bearing(x['pickup_latitude'], x['pickup_longitude'], \n","                                                           x['dropoff_latitude'], x['dropoff_longitude']),\n","                                                           axis=1)\n","        print('Distance features calculated')\n","        return X\n","    \n","    def _calculate_great_circle_distance(self, pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n","        pickup = [pickup_lat, pickup_long]\n","        dropoff = [dropoff_lat, dropoff_long]\n","        distance = geo.sphere.distance(pickup, dropoff)\n","        return distance\n","    \n","    def _calculate_manhattan_distance(self, pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n","        pickup = [pickup_lat, pickup_long]\n","        dropoff_a = [pickup_lat, dropoff_long]\n","        dropoff_b = [dropoff_lat, pickup_long]\n","        distance_a = geo.sphere.distance(pickup, dropoff_a)\n","        distance_b = geo.sphere.distance(pickup, dropoff_b)\n","        return distance_a + distance_b\n","        \n","    def _calculate_bearing(self, pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n","        pickup = [pickup_lat, pickup_long]\n","        dropoff = [dropoff_lat, dropoff_long]\n","        bearing = geo.sphere.bearing(pickup, dropoff)\n","        return bearing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LTTDd89R_yhv","colab_type":"code","colab":{}},"source":["# Cluster Density - estimate traffic density at the different pickup and dropoff locations.\n","# Use mini-batch K-Means to group our pickup and dropoff locations into 20 distinct clusters \n","# Get the number of pickups/dropoffs at each cluster per time interval (per hour)\n","# Compute the percentage of rides in each pickup/ dropoff cluster per day.\n","\n","class GetClusterDensity():\n","    def __init__(self, n_clusters):\n","        self.n_clusters = n_clusters\n","        self.pickup_time_clusters = pd.DataFrame()\n","        self.dropoff_time_clusters = pd.DataFrame()\n","    \n","    def fit(self, X, y):\n","        # I. Fit pickup cluster\n","        df_pickup = X[['pickup_latitude', 'pickup_longitude']].copy()\n","        \n","        ## Ia. Initialize K-means \n","        self.clf_pickup = MiniBatchKMeans(n_clusters=self.n_clusters, batch_size=10000, max_iter=300, random_state=1)\n","        self.clf_pickup.fit_predict(df_pickup)\n","        df_pickup['pickup_cluster'] = self.clf_pickup.labels_\n","        df_pickup['pickup_cluster'] = df_pickup['pickup_cluster'].astype(str)\n","        X = pd.merge(X, df_pickup[['pickup_cluster']], left_index=True, right_index=True, how='left')\n","        self.clf_pickup.fit(df_pickup.drop(['pickup_cluster'], axis=1))\n","        \n","        ## Ib. Calculate number of rides grouped by cluster and time as proxy for \"density\" of traffic by location and time\n","        pickup_time_clusters = pd.DataFrame(X.groupby(['month', 'day', 'hour_of_day', 'pickup_cluster'])['pickup_latitude'].count()).reset_index()\n","        pickup_time_clusters = pickup_time_clusters.rename(columns={'pickup_latitude': 'num_rides_by_pickup_group'})\n","        pickup_time_clusters_agg = pd.DataFrame(pickup_time_clusters.reset_index().groupby(['month', 'day'])['num_rides_by_pickup_group'].sum().round(4))\n","        pickup_time_clusters_agg = pickup_time_clusters_agg.rename(columns={'num_rides_by_pickup_group': 'agg_rides_per_day'})\n","        pickup_time_clusters = pd.merge(pickup_time_clusters.set_index(['month', 'day', 'hour_of_day']), pickup_time_clusters_agg, left_index=True, right_index=True)\n","        pickup_time_clusters['perc_rides_per_day_by_pickup_group'] = pickup_time_clusters['num_rides_by_pickup_group']/pickup_time_clusters['agg_rides_per_day']\n","        pickup_time_clusters = pickup_time_clusters.reset_index()\n","        pickup_time_clusters['pickup_group'] = pickup_time_clusters['pickup_cluster'].map(str) + str(',') + \\\n","                            pickup_time_clusters['month'].map(str) + str(',') + pickup_time_clusters['day'].map(str) + \\\n","                            str(',') + pickup_time_clusters['hour_of_day'].map(str)\n","        self.pickup_time_clusters = pickup_time_clusters\n","        \n","        \n","        # II. Fit dropoff cluster\n","        df_dropoff = X[['dropoff_latitude', 'dropoff_longitude']].copy()       \n","        \n","        ## IIa. Initialize K-means \n","        self.clf_dropoff = MiniBatchKMeans(n_clusters=20, batch_size=10000, max_iter=300, random_state=1)\n","        self.clf_dropoff.fit_predict(df_dropoff)\n","        df_dropoff['dropoff_cluster'] = self.clf_dropoff.labels_\n","        df_dropoff['dropoff_cluster'] = df_dropoff['dropoff_cluster'].astype(str)\n","        X = pd.merge(X, df_dropoff[['dropoff_cluster']], left_index=True, right_index=True, how='left')\n","        self.clf_dropoff.fit(df_dropoff.drop(['dropoff_cluster'], axis=1))\n","        \n","        ## IIb. Calculate number of rides grouped by cluster and time as proxy for \"density\" of traffic by location and time\n","        dropoff_time_clusters = pd.DataFrame(X.groupby(['month', 'day', 'hour_of_day', 'dropoff_cluster'])['dropoff_latitude'].count()).reset_index()\n","        dropoff_time_clusters = dropoff_time_clusters.rename(columns={'dropoff_latitude': 'num_rides_by_dropoff_group'})\n","        dropoff_time_clusters_agg = pd.DataFrame(dropoff_time_clusters.reset_index().groupby(['month', 'day'])['num_rides_by_dropoff_group'].sum().round(4))\n","        dropoff_time_clusters_agg = dropoff_time_clusters_agg.rename(columns={'num_rides_by_dropoff_group': 'agg_rides_per_day'})\n","        dropoff_time_clusters = pd.merge(dropoff_time_clusters.set_index(['month', 'day', 'hour_of_day']), dropoff_time_clusters_agg, left_index=True, right_index=True)\n","        dropoff_time_clusters['perc_rides_per_day_by_dropoff_group'] = dropoff_time_clusters['num_rides_by_dropoff_group']/dropoff_time_clusters['agg_rides_per_day']\n","        dropoff_time_clusters = dropoff_time_clusters.reset_index()\n","        dropoff_time_clusters['dropoff_group'] = dropoff_time_clusters['dropoff_cluster'].map(str) + str(',') + \\\n","                            dropoff_time_clusters['month'].map(str) + str(',') + dropoff_time_clusters['day'].map(str) + \\\n","                            str(',') + dropoff_time_clusters['hour_of_day'].map(str)\n","        self.dropoff_time_clusters = dropoff_time_clusters\n","        \n","        return self\n","    \n","    def transform(self, X):\n","        # III. Predict pickup cluster\n","        df_pickup = X[['pickup_latitude', 'pickup_longitude']].copy()\n","        \n","        ## IIIa. Add cluster label\n","        df_pickup['pickup_cluster'] = self.clf_pickup.predict(df_pickup)\n","        df_pickup['pickup_cluster'] = df_pickup['pickup_cluster'].astype(str)\n","        \n","        ## IIIb. Merge cluster label back to original dataframe\n","        X = pd.merge(X, df_pickup[['pickup_cluster']], left_index=True, right_index=True, how='left')\n","        \n","        ## IIIc. Merge to create \"num_rides_by_pickup_group\" and \"perc_rides_by_pickup_group\" features\n","        X['pickup_group'] = X['pickup_cluster'].map(str) + str(',') + X['month'].map(str) + str(',') + \\\n","                            X['day'].map(str) + str(',') + X['hour_of_day'].map(str)\n","        X = pd.merge(X, self.pickup_time_clusters[['pickup_group','num_rides_by_pickup_group', 'perc_rides_per_day_by_pickup_group']], on='pickup_group', how='left')\n","        X = X.drop(['pickup_group', 'pickup_cluster'], axis=1)\n","        print('Pickup Clusters Found')\n","        \n","        \n","        # IV. Predict dropoff cluster\n","        df_dropoff = X[['dropoff_latitude', 'dropoff_longitude']].copy() \n","        \n","        ## IVa. Add cluster label\n","        df_dropoff['dropoff_cluster'] = self.clf_dropoff.predict(df_dropoff)\n","        df_dropoff['dropoff_cluster'] = df_dropoff['dropoff_cluster'].astype(str)\n","        \n","        ## IVb. Merge cluster label back to original dataframe\n","        X = pd.merge(X, df_dropoff[['dropoff_cluster']], left_index=True, right_index=True, how='left')\n","        \n","        ## IVc. Merge to create \"num_rides_by_pickup_group\" and \"perc_rides_by_pickup_group\" features\n","        X['dropoff_group'] = X['dropoff_cluster'].map(str) + str(',') + X['month'].map(str) + str(',') + \\\n","                             X['day'].map(str) + str(',') + X['hour_of_day'].map(str)\n","        X = pd.merge(X, self.dropoff_time_clusters[['dropoff_group','num_rides_by_dropoff_group', 'perc_rides_per_day_by_dropoff_group']], on='dropoff_group', how='left')\n","        X = X.drop(['dropoff_group', 'dropoff_cluster'], axis=1)\n","        print('Dropoff Clusters Found')\n","        \n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwYilauwARFB","colab_type":"code","colab":{}},"source":["# K-Means clusters didnâ€™t add much on top of lat/long coordinates\n","# Instead, use a probabilistic Gaussian Mixture Model (GMM) model - GMM learns the \n","# probabilities of a data point belonging to each pickup/dropoff cluster. groups \n","# of pickup and dropoff locations. So a point is not uniquely assigned to a single\n","# cluster, but can reside in multiple cluster with some probability.\n","\n","class GetClusterProbability():\n","    def __init__(self, n_components):\n","        self.n_components = n_components\n","    \n","    # First scale the pickup and dropoff locations using StandardScaler(), then fit \n","    # a GMM model of 20 components on the train set \n","    def fit(self, X, y):\n","        # Pickup locations\n","        self.pickup_cols = ['pickup_latitude', 'pickup_longitude']\n","        self.scaler_pickup = StandardScaler()\n","        pickup_scaled = self.scaler_pickup.fit_transform(X[self.pickup_cols])\n","        self.gmm_pickup = mixture.GaussianMixture(n_components=self.n_components).fit(pickup_scaled)\n","        \n","        # Dropoff locations\n","        self.dropoff_cols = ['dropoff_latitude', 'dropoff_longitude']\n","        self.scaler_dropoff = StandardScaler()\n","        dropoff_scaled = self.scaler_dropoff.fit_transform(X[self.dropoff_cols])\n","        self.gmm_dropoff = mixture.GaussianMixture(n_components=self.n_components).fit(dropoff_scaled)\n","        \n","        return self\n","    \n","    def transform(self, X):\n","        # Pickup locations\n","        pickup_scaled = self.scaler_pickup.transform(X[self.pickup_cols])\n","        preds_pickup = pd.DataFrame(self.gmm_pickup.predict_proba(pickup_scaled))\n","        preds_pickup = preds_pickup.add_prefix('gmm_pickup_')\n","        X = pd.merge(X, preds_pickup, left_index=True, right_index=True)\n","        print('GMM for pickup done')\n","        \n","        # Dropoff locations\n","        dropoff_scaled = self.scaler_dropoff.transform(X[self.dropoff_cols])\n","        preds_dropoff = pd.DataFrame(self.gmm_dropoff.predict_proba(dropoff_scaled))\n","        preds_dropoff = preds_dropoff.add_prefix('gmm_dropoff_')\n","        X = pd.merge(X, preds_dropoff, left_index=True, right_index=True)\n","        print('GMM for dropoff done')\n","        \n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q4TPnlofBA9R","colab_type":"code","colab":{}},"source":["# One-hot Encode categorical features - the vendor_id and store_and_fwd_flag columns, \n","# both of which only have two unique values\n","\n","class DummifyCategoricals():\n","    def fit(self, X, y):\n","        self.cols_to_encode=['vendor_id', 'store_and_fwd_flag']\n","        self.encoder = OneHotEncoder(categories='auto', handle_unknown='ignore')\n","        \n","        transformed_array = self.encoder.fit_transform(X[self.cols_to_encode]).toarray()\n","        self.transformed_colnames = [f'{prefix}_{value}' \n","                                     for prefix, values in zip(self.cols_to_encode, self.encoder.categories_) \n","                                     for value in values]\n","            \n","        return self\n","        \n","    def transform(self, X):\n","        transformed_array = self.encoder.transform(X[self.cols_to_encode]).toarray()\n","        transformed_df = pd.DataFrame(transformed_array, columns=self.transformed_colnames)\n","        \n","        X = pd.concat((X.drop(self.cols_to_encode, axis=1).reset_index(drop=True), \n","                       transformed_df.reset_index(drop=True)), axis=1)\n","        print('Categorical variables dummified')\n","        return X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xLs0MYoK9A_G","colab_type":"code","colab":{}},"source":["# Split into train, valid and test\n","train, df_test = train_test_split(df)\n","df_train, df_valid = train_test_split(train)\n","df_train.shape, df_valid.shape, df_test.shape\n","\n","# Remove original target from df_train and df_test\n","df_train = df_train.drop(['trip_duration'], axis=1)\n","df_valid = df_valid.drop(['trip_duration'], axis=1)\n","df_test = df_test.drop(['trip_duration'], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ro6gtaM-foL","colab_type":"code","colab":{}},"source":["# Use sklearn make_pipeline() to construct a half pipeline to transform features.\n","\n","initial_feats = ['vendor_id',\n","                 'pickup_datetime',\n","                 'passenger_count',\n","                 'pickup_longitude',\n","                 'pickup_latitude',\n","                 'dropoff_longitude',\n","                 'dropoff_latitude',\n","                 'store_and_fwd_flag']\n","\n","half_pipeline = make_pipeline(\n","    FeaturizeTime(),\n","    CalculateDistances(),\n","    GetClusterDensity(n_clusters=20),\n","    GetClusterProbability(n_components=20),\n","    DummifyCategoricals()\n","    )\n","\n","df_train_feat = half_pipeline.fit_transform(df_train[initial_feats])\n","df_valid_feat = half_pipeline.transform(df_valid[initial_feats])\n","df_test_feat = half_pipeline.transform(df_test[initial_feats])\n","\n","df_train_feat.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwjpd3gs_fHW","colab_type":"text"},"source":["### Model - training using Light GBM"]},{"cell_type":"code","metadata":{"id":"lODFCcqd_vTH","colab_type":"code","colab":{}},"source":["# Evaluation metric - use rmse rather than rmsle, as we are taking log() of the target values\n","def calc_rmsle(y_true_log, y_preds_log):\n","    return np.sqrt(mean_squared_log_error(np.exp(y_true_log), np.exp(y_preds_log)))\n","\n","# Light GBM model\n","lgb = LGBMRegressor()\n","lgb.fit(df_train_feat, df_train[target], eval_set=[(df_valid_feat, df_valid[target])], \n","        eval_metric='rmse', early_stopping_rounds=10, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V2ULS-h6AgJ-","colab_type":"text"},"source":["### Model - Prediction"]},{"cell_type":"code","metadata":{"id":"Bk5tDhf7AblE","colab_type":"code","colab":{}},"source":["y_preds_lgbm = lgb.predict(df_test_feat)\n","calc_rmsle(df_test[target], y_preds_lgbm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S7OgU4Z9L25L","colab_type":"text"},"source":["### Feature Selection"]},{"cell_type":"code","metadata":{"id":"DRhCaBYtL4wl","colab_type":"code","colab":{}},"source":["# Use top 50 features ranked by feature importance\n","\n","best_feats = df_train_feat.columns[np.argsort(lgb.feature_importances_)[::-1]]\n","best_feats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E0HdoW8fMPHM","colab_type":"code","colab":{}},"source":["features_intersection = [col for col in df_train_feat[df_train_feat.columns.intersection(df_test_feat.columns)].columns]\n","features_intersection\n","len(df_train_feat.columns), len(df_test_feat.columns), len(features_intersection)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dTP9OvrlYbhU","colab_type":"code","colab":{}},"source":["best_feats_intersection = [feat for feat in best_feats if feat in features_intersection]\n","print(len(best_feats_intersection))\n","best_feats_intersection"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VeoWi12Yf5i","colab_type":"code","colab":{}},"source":["train_scores = []\n","test_scores = []\n","\n","for i in range(1, len(best_feats_intersection)):\n","    lgb.fit(df_train_feat.loc[:, best_feats_intersection[:i]], df_train[target])\n","    train_preds = lgb.predict(df_train_feat.loc[:, best_feats_intersection[:i]])\n","    train_scores.append(calc_rmsle(df_train[target],train_preds))\n","    test_preds = lgb.predict(df_test_feat.loc[:, best_feats_intersection[:i]])\n","    test_scores.append(calc_rmsle(df_test[target], test_preds))\n","    print(f'RMSLE recorded for round {i}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"03FfNwacYjAj","colab_type":"code","colab":{}},"source":["plt.plot(range(1, len(best_feats_intersection)), train_scores, label='train RMSLE')\n","plt.plot(range(1, len(best_feats_intersection)), test_scores, label='test RMSLE')\n","\n","plt.xticks(np.arange(1,len(best_feats_intersection),2))\n","plt.xlabel('num features used in LGBM Regressor Model')\n","plt.ylabel('RMSLE Score')\n","plt.legend()\n","plt.title('Forward Feature Selection')\n","#plt.ylim(0.39,0.45)\n","#plt.savefig('./plots/step_forward_feat_sel.png', bbox_inches='tight')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ih2mZWFuM8w9","colab_type":"code","colab":{}},"source":["min(test_scores)\n","best_n_feats = test_scores.index(min(test_scores)) + 1\n","print ('Num features', best_n_feats)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjRKjqQBNBjR","colab_type":"code","colab":{}},"source":["X_train = df_train_feat.loc[:, best_feats_intersection[:best_n_feats]]\n","X_valid = df_valid_feat.loc[:, best_feats_intersection[:best_n_feats]]\n","X_test = df_test_feat.loc[:, best_feats_intersection[:best_n_feats]]\n","\n","y_train = df_train[target]\n","y_valid = df_valid[target]\n","y_test = df_test[target]\n","X_train.shape, X_valid.shape, X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyE07GvNNUoc","colab_type":"text"},"source":["### Hyperparam Tuning"]},{"cell_type":"code","metadata":{"id":"kjjUfvmwNeYb","colab_type":"code","colab":{}},"source":["# Use GridSearch to find the optimal hyperparameters for LightGBM, like max_depth and num_leaves\n","\n","params = {\n","    'num_leaves': [256,512,1024], \n","    'max_depth': [8,10,12], \n","    'n_estimators': [500],\n","    'subsample': [0.8],\n","    'feature_fraction': [0.9],\n","    'lambda_l1': [0.2],\n","    'learning_rate': [0.1]\n","}\n","\n","def calc_rmsle(y_true_log, y_preds_log):\n","    return np.sqrt(mean_squared_log_error(np.exp(y_true_log), np.exp(y_preds_log)))\n","\n","rmsle_scorer = metrics.make_scorer(calc_rmsle, greater_is_better=False)\n","\n","lgb = LGBMRegressor(eval_metric='rmse')\n","reg = GridSearchCV(lgb, params, scoring=rmsle_scorer, verbose=True)\n","reg.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJYIKJZ2nxfD","colab_type":"code","colab":{}},"source":["reg.best_score_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5SCWO9mn15X","colab_type":"code","colab":{}},"source":["reg.best_params_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3tJEN2Mn7BP","colab_type":"code","colab":{}},"source":["gb_cv_results = pd.DataFrame.from_dict(reg.cv_results_).sort_values(by='mean_test_score', ascending=False)\n","gb_cv_results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MJI2M8KxGF5F","colab_type":"text"},"source":["## Geo Clusters with Geopandas and Shapely - [article 2](https://towardsdatascience.com/finding-and-visualizing-clusters-of-geospatial-data-698943c18fed), [notebook 2](https://github.com/claudian37/DS_Portfolio/blob/master/NYC_cab_dataset/03_visualizing_kmeans_clusters_geospatial_data.ipynb)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UydzxQa0F3LM","colab":{}},"source":["!pip install geopandas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HZSzE-1lF3LS","colab":{}},"source":["import geopandas as gpd\n","from geopandas import GeoSeries\n","from shapely.geometry import Point\n","\n","%matplotlib inline\n","\n","plt.style.use('fivethirtyeight')\n","plt.rcParams['figure.figsize'] = (12,8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qAC4WjImnMq","colab_type":"code","colab":{}},"source":["# Download the shape file for NY boroughs\n","ny_shape_zip='nydata.zip'\n","ny_shape_dir=Path.cwd()/'nyshape'\n","\n","# Put double quotes around the URL so that the query params get passed correctly\n","!wget -O {ny_shape_zip} \"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\"\n","!zipinfo {ny_shape_zip}\n","!unzip {ny_shape_zip} -d {ny_shape_dir} >> /dev/null\n","\n","# Downloaded filenames are randomly generated, so rename to a known name\n","!mv nyshape/geo*.shp nyshape/ny.shp\n","!mv nyshape/geo*.shx nyshape/ny.shx\n","!mv nyshape/geo*.prj nyshape/ny.prj\n","!mv nyshape/geo*.dbf nyshape/ny.dbf\n","\n","!ls nyshape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i5F6IKz6xnLg","colab_type":"text"},"source":["### Pre-process data"]},{"cell_type":"code","metadata":{"id":"_NAgPZnzxmlG","colab_type":"code","colab":{}},"source":["df_pickup = df[['pickup_latitude', 'pickup_longitude']].copy()\n","df_pickup = df_pickup.rename(columns={'pickup_latitude': 'latitude', 'pickup_longitude': 'longitude'})\n","\n","# Drop lat/longs of extreme outliers below 0.01 percentile or above 99.99 percentile\n","df_pickup_filtered = df_pickup[(df_pickup.quantile(0.0001) < df_pickup) & (df_pickup < df_pickup.quantile(0.9999))]\n","df_pickup_filtered = df_pickup_filtered.dropna(how='any')\n","\n","print(f'{df_pickup.shape[0] - df_pickup_filtered.shape[0]} extreme outliers removed')\n","print(f'Shape of filtered df_pickup: {df_pickup_filtered.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yEsyiocsyyWc","colab_type":"text"},"source":["### Find clusters and plot"]},{"cell_type":"code","metadata":{"id":"0dcNIUUR9r1R","colab_type":"code","colab":{}},"source":["import IPython.core.debugger as db\n","def get_cluster(df, n_clusters):\n","    km = MiniBatchKMeans(n_clusters=n_clusters)\n","    km.fit_predict(df)\n","\n","    # Calculate sum of squared distances\n","    ssd = km.inertia_\n","    \n","    # Label cluster centers\n","    centers = km.cluster_centers_\n","    \n","    # Get cluster center\n","    clusters = km.labels_\n","\n","    return (ssd, centers, clusters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byxKVktsAfyd","colab_type":"code","colab":{}},"source":["def plot_cluster(base_map, gdf, centers, clusters):\n","  fig, ax = plt.subplots(figsize=(10,10))\n","  ax.set_aspect('equal')\n","  base_map.plot(ax=ax, alpha=0.4, edgecolor='darkgrey', color='lightgrey', label=base_map['boro_name'], zorder=1)\n","  gdf.plot(ax=ax, column=clusters, alpha=0.5, cmap='viridis', linewidth=0.8, zorder=2)\n","  centers_gseries = GeoSeries([Point(centers[i, 1], centers[i, 0]) for i in range(centers.shape[0])])\n","  centers_gseries.plot(ax=ax, alpha=1, marker='X', color='red', markersize=100, zorder=3)\n","\n","  n_clusters = len(clusters)\n","  plt.title(f'K-Means: Pickup Locations grouped into {n_clusters} clusters')\n","  plt.xlabel('longitude')\n","  plt.ylabel('latitude')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2TKyNqBCRkAJ","colab_type":"code","colab":{}},"source":["def best_clusters(df, max_clusters):\n","  ssds = []\n","\n","  for i in range(2, max_clusters):\n","    ssd, centers, clusters = get_cluster(df=df, n_clusters=i)\n","    # Save sum of squared distances\n","    ssds.append(ssd)\n","\n","  plt.plot(range(2,max_clusters), ssds)\n","  plt.xlabel('k')\n","  plt.ylabel('Sum of Squared Distances')\n","  plt.title('Elbow Method to Find Optimal Value of k')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92THllAr1ApO","colab_type":"code","colab":{}},"source":["# Create a Geopandas dataframe and plot it\n","ny_crs = {'init': 'epsg:4326'}\n","ny_geom = [Point(xy) for xy in zip(df_pickup_filtered['longitude'], df_pickup_filtered['latitude'])]\n","ny_gdf = gpd.GeoDataFrame(df_pickup_filtered, crs=ny_crs, geometry=ny_geom)\n","ny_gdf.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M56r_o6b35gi","colab_type":"code","colab":{}},"source":["# Load the Shape map of the NYC as the base map\n","nyc_full = gpd.read_file(ny_shape_dir/'ny.shp')\n","nyc_full.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6HwgjsRbc-hI","colab":{}},"source":["# Plot lat/long and clusters on map for some cluster values\n","_, centers, clusters = get_cluster(df=df_pickup_filtered, n_clusters=3)\n","plot_cluster(nyc_full, ny_gdf, centers, clusters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VXZ1nwgMP4Df","colab_type":"code","colab":{}},"source":["_, centers, clusters = get_cluster(df=df_pickup_filtered, n_clusters=6)\n","plot_cluster(nyc_full, ny_gdf, centers, clusters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmHzza5vc6UL","colab_type":"code","colab":{}},"source":["_, centers, clusters = get_cluster(df=df_pickup_filtered, n_clusters=12)\n","plot_cluster(nyc_full, ny_gdf, centers, clusters)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7AZJSNcwR616","colab_type":"code","colab":{}},"source":["best_clusters(df=df_pickup_filtered, max_clusters=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"38_9jneMGUnS"},"source":["## Geo Heatmaps with Folium"]},{"cell_type":"code","metadata":{"id":"vipUUgJcfMfP","colab_type":"code","colab":{}},"source":["# Install newer version of Folium so that HeatMapWithTime() works\n","# Colab has 0.8.3 installed by default, whereas we need 0.11.0 or greater\n","!pip install folium --force-reinstall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A_OfHvxCJn06","colab_type":"code","colab":{}},"source":["# !!!!!! Delete this cell if the above cell works\n","!pip uninstall folium\n","!pip install folium --force-reinstall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyujT-uAoKct","colab_type":"code","colab":{}},"source":["import folium\n","print ('Folium version:', folium.__version__)\n","from folium import plugins\n","from folium.plugins import HeatMap\n","from IPython.display import IFrame\n","\n","df_heat = df[['pickup_datetime', 'pickup_longitude', 'pickup_latitude']].copy()\n","df_heat['day_of_week'] = pd.to_datetime(df_heat['pickup_datetime']).dt.dayofweek\n","df_heat['hour_of_day'] = pd.to_datetime(df_heat['pickup_datetime']).dt.hour\n","df_heat.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jq4gJw5tlIsB","colab_type":"code","colab":{}},"source":["# Instantiate map object \n","map_5 = folium.Map(location=[40.75, -73.96], tiles='openstreetmap', zoom_start=12)\n","\n","hm_data=[]\n","ti=[]\n","for i in range(df_heat.day_of_week.min(), df_heat.day_of_week.max()+1):\n","    for j in range(df_heat.hour_of_day.min(), df_heat.hour_of_day.max()+1):    \n","        \n","        # Filter to include only data for each day of week and hour of day\n","        df_geo = df_heat.loc[(df_heat.day_of_week==i) & (df_heat.hour_of_day==j)][['pickup_latitude', 'pickup_longitude']].copy()\n","        hm_data.append(df_geo.to_numpy().tolist())\n","        ti.append(f'{i}-{j}')\n","\n","# Plot heatmap - data size is too big, so plot a subset of the heatmaps\n","# Then click on the animation in the bottom left corner to see the heatmaps over time slots\n","hm = plugins.HeatMapWithTime(hm_data[:50], index=ti[:50])\n","hm.add_to(map_5)\n","map_5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-59J81t95yLl","colab_type":"text"},"source":["## Interactive Map Visualisations with Folium"]},{"cell_type":"code","metadata":{"id":"vQbuInEl5xsH","colab_type":"code","colab":{}},"source":["# Uses lat then lon. The bigger the zoom number, the closer in you get\n","map_hooray = folium.Map(location=[51.5074, 0.1278], zoom_start = 11) \n","map_hooray # Calls the map to display"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kae4x8T6a4N","colab_type":"code","colab":{}},"source":["# Different visual styles\n","t_list = [\"Stamen Terrain\", \"Stamen Toner\", \"Mapbox Bright\"]\n","map_hooray = folium.Map(location=[51.5074, 0.1278], tiles = \"Stamen Terrain\",  zoom_start = 12)\n","map_hooray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgROTqYb6kln","colab_type":"code","colab":{}},"source":["# Different visual styles\n","map_hooray = folium.Map(location=[51.5074, 0.1278], tiles = \"Stamen Toner\", zoom_start = 12)\n","map_hooray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeV3Qsoj6wuc","colab_type":"code","colab":{}},"source":["# Markers - once the base map is defined, everything else gets added on top of it with map.add_to()\n","# Note the 'popup attribute. This text appears on clicking the map.\n","# 'width=int' and 'height=int' can be in pixels or percentages\n","map_hooray = folium.Map(location=[51.5074, 0.1278], width=600, height=500, zoom_start = 12)\n","folium.Marker([51.5079, 0.0877], popup='London Bridge').add_to(map_hooray)\n","map_hooray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PaIw5hvF8dx-","colab_type":"code","colab":{}},"source":["# More complex markers\n","# The London Bridge marker is a pin icon marker, with an extra line of green.\n","# The second marker is a coloured overlay marker. CircleMarker radius is set in pixels so if you change the zoom you need to change the pixels. It can also take a fill_color that's semi-transparent.\n","# The third marker is interactive - so the user can click anywhere multiple times to add their own markers.\n","\n","# Set the map up\n","map_hooray = folium.Map(location=[51.5074, 0.1278], width=600, height=500, zoom_start = 9)\n","# Simple marker\n","folium.Marker([51.5079, 0.0877], popup='London Bridge', icon=folium.Icon(color='green')).add_to(map_hooray)\n","# Circle marker\n","folium.CircleMarker([51.4183, 0.2206], radius=30, popup='East London', color='red').add_to(map_hooray)\n","# Interactive marker\n","map_hooray.add_child(folium.ClickForMarker(popup=\"Folium is awesome\"))\n","\n","map_hooray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEvHRa-N94cA","colab_type":"code","colab":{}},"source":["# Adds tool to the top right\n","from folium import plugins\n","from folium.plugins import MeasureControl\n","from folium.plugins import FloatImage\n","\n","# Interactivity\n","map_hooray = folium.Map(location=[51.5074, 0.1278], width=600, height=500, zoom_start = 11) \n","# Click on the map if you don't see the measure control icon in the top right\n","map_hooray.add_child(MeasureControl())\n","url = ('https://media.licdn.com/mpr/mpr/shrinknp_100_100/AAEAAQAAAAAAAAlgAAAAJGE3OTA4YTdlLTkzZjUtNDFjYy1iZThlLWQ5OTNkYzlhNzM4OQ.jpg')\n","FloatImage(url, bottom=5, left=85).add_to(map_hooray)\n","\n","map_hooray"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNfm5_xzFDOV","colab_type":"code","colab":{}},"source":["# Add icons from fontawesome.io, by referencing the \"prefix='fa'\"\"\n","\n","map_hooray = folium.Map(location=[51.5074, 0.1278], width=600, height=500, zoom_start = 9)\n","\n","folium.Marker([51.5079, 0.0877], popup='London Bridge', icon=folium.Icon(color='green')).add_to(map_hooray)\n","folium.Marker([51.5183, 0.5206], popup='East London', icon=folium.Icon(color='red',icon='university', prefix='fa')).add_to(map_hooray)\n","folium.Marker([51.5183, 0.3206], popup='East London', icon=folium.Icon(color='blue',icon='bar-chart', prefix='fa')).add_to(map_hooray)\n","             # icon=folium.Icon(color='red',icon='bicycle', prefix='fa')\n","map_hooray.add_child(folium.ClickForMarker(popup=\"Folium is awesome\"))\n","\n","map_hooray"],"execution_count":null,"outputs":[]}]}