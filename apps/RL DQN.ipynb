{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL DQN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNFSVEOIJ8O3/A+wCgbnCub"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9iPVdEgBoo-9"},"source":["### Deep Q Learning"]},{"cell_type":"markdown","metadata":{"id":"N8V8vS_PosKv"},"source":["### Import Libraries"]},{"cell_type":"code","metadata":{"id":"kI5e1uoLdVMQ","executionInfo":{"status":"ok","timestamp":1604998572592,"user_tz":-330,"elapsed":3004,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YXajwfsp8Bj"},"source":["!sudo apt-get update\n","!sudo apt-get install cmake libboost-all-dev libsdl2-dev libfreetype6-dev libgl1-mesa-dev libglu1-mesa-dev libpng-dev libjpeg-dev libbz2-dev libfluidsynth-dev libgme-dev libopenal-dev zlib1g-dev timidity tar nasm wget\n","!pip install vizdoom"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qUnUHuVhoeBc","executionInfo":{"status":"ok","timestamp":1604998919803,"user_tz":-330,"elapsed":2056,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"source":["import tensorflow as tf      # Deep Learning library\n","import numpy as np           # Handle matrices\n","from vizdoom import *        # Doom Environment\n","\n","import random                # Handling random number generation\n","import time                  # Handling time calculation\n","from skimage import transform# Help us to preprocess the frames\n","\n","from collections import deque# Ordered collection with ends\n","import matplotlib.pyplot as plt # Display graphs\n","import IPython.core.debugger as db\n","\n","import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\n","warnings.filterwarnings('ignore') "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"756rf2_esUy2","executionInfo":{"status":"ok","timestamp":1604998931748,"user_tz":-330,"elapsed":11958,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"23b22be2-dddd-4835-902b-25adc8ae4a73","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git clone https://github.com/simoninithomas/Deep_reinforcement_learning_Course.git\n","\n","!cp \"Deep_reinforcement_learning_Course/Deep Q Learning/Doom/basic.cfg\" .\n","!cp \"Deep_reinforcement_learning_Course/Deep Q Learning/Doom/basic.wad\" .\n","!cp \"Deep_reinforcement_learning_Course/Deep Q Learning/Doom/_vizdoom.ini\" ."],"execution_count":5,"outputs":[{"output_type":"stream","text":["Cloning into 'Deep_reinforcement_learning_Course'...\n","remote: Enumerating objects: 19, done.\u001b[K\n","remote: Counting objects: 100% (19/19), done.\u001b[K\n","remote: Compressing objects: 100% (19/19), done.\u001b[K\n","remote: Total 891 (delta 9), reused 0 (delta 0), pack-reused 872\u001b[K\n","Receiving objects: 100% (891/891), 230.11 MiB | 38.91 MiB/s, done.\n","Resolving deltas: 100% (394/394), done.\n","Checking out files: 100% (216/216), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pgeEu8DosywX","executionInfo":{"status":"ok","timestamp":1605009093270,"user_tz":-330,"elapsed":17358,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"23fba73a-da97-4c0e-d96b-9a0d86ce31ad","colab":{"base_uri":"https://localhost:8080/"}},"source":["def test_environment():\n","    game = DoomGame()\n","    game.load_config(\"basic.cfg\")\n","    game.set_doom_scenario_path(\"basic.wad\")\n","    game.set_window_visible(False)\n","    game.init()\n","    shoot = [0, 0, 1]\n","    left = [1, 0, 0]\n","    right = [0, 1, 0]\n","    actions = [shoot, left, right]\n","\n","    episodes = 2\n","    for i in range(episodes):\n","        game.new_episode()\n","        step_i = 0\n","        while not game.is_episode_finished():\n","            state = game.get_state()\n","            img = state.screen_buffer\n","            misc = state.game_variables\n","            action = random.choice(actions)\n","            #print(action)\n","            reward = game.make_action(action)\n","            #print (\"\\treward:\", reward)\n","            time.sleep(0.02)\n","            step_i += 1\n","        print (\"Result:\", game.get_total_reward(), step_i)\n","        time.sleep(2)\n","    game.close()\n","\n","test_environment()"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Result: -250.0 281\n","Result: -385.0 300\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ikX1uKYbsOUI"},"source":["import matplotlib.pyplot as plt\n","\n","def show_environment():\n","    game = DoomGame()\n","    game.load_config(\"basic.cfg\")\n","    game.set_doom_scenario_path(\"basic.wad\")\n","    game.set_window_visible(False)\n","    game.init()\n","    shoot = [0, 0, 1]\n","    left = [1, 0, 0]\n","    right = [0, 1, 0]\n","    actions = [shoot, left, right]\n","\n","    episodes = 1\n","    for i in range(episodes):\n","        game.new_episode()\n","        for j in range(10):\n","            state = game.get_state()\n","            img = state.screen_buffer\n","            print(img.shape)\n","            plt.imshow(img)\n","            plt.show()\n","\n","            pimg = preprocess_frame(img)\n","            print(pimg.shape)\n","            plt.imshow(pimg)\n","            plt.show()\n","\n","            misc = state.game_variables\n","            action = random.choice(actions)\n","            #print(action)\n","            reward = game.make_action(action)\n","            #print (\"\\treward:\", reward)\n","            time.sleep(0.02)\n","        time.sleep(2)\n","    game.close()\n","\n","show_environment()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"inuJggnT4hID"},"source":["###Stacked Frame"]},{"cell_type":"code","metadata":{"id":"U96HWuZr4UCb","executionInfo":{"status":"ok","timestamp":1605000397007,"user_tz":-330,"elapsed":1895,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"source":["from skimage.color import rgb2gray\n","\n","class StackFrame():\n","  def __init__(self, stack_sz, frame_w, frame_h):\n","    self.stack_sz = stack_sz\n","    self.frame_w, self.frame_h = frame_w, frame_h\n","\n","    self.init_stack()\n","\n","  def init_stack(self):\n","    stack_sz = self.stack_sz\n","    frame_w, frame_h = self.frame_w, self.frame_h\n","\n","    # Initialize deque with zero-images, one array for each image\n","    self.stacked_frames = deque([np.zeros((frame_w, frame_h), dtype=np.int) for _ in range(stack_sz)], maxlen=stack_sz)\n","\n","  def preprocess(self, frame):\n","    # Greyscale frame already done in our vizdoom config\n","    # x = np.mean(frame,-1)\n","      \n","    # Convert to grey scale. Not strictly necessary in this particular example because\n","    # the vizdoom config file already set to generate grey scale.\n","    grey_frame = rgb2gray(frame)\n","      \n","    # Crop the screen (remove the roof because it contains no information)\n","    cropped_frame = grey_frame[30:-10,30:-30]\n","      \n","    # Normalize Pixel Values\n","    normalized_frame = cropped_frame/255.0\n","      \n","    # Resize\n","    preprocessed_frame = transform.resize(normalized_frame, [self.frame_w, self.frame_h])\n","      \n","    return preprocessed_frame\n","\n","  #-----------------------------------------------------------------\n","  # We feed the last four frames to the Q Network, by stacking them with shape\n","  # (frame_w, frame_h, 4). The previous frames allows the network to reason about\n","  # motion of objects.\n","  #\n","  # Due to the way that Atari renders screens, every other frame may not actually be rendered.\n","  # This negatively affects our performance, so we take Deepmind's approach of \n","  # taking the elementwise-maxima of the last two frames\n","  #-----------------------------------------------------------------\n","  def combine_prev(self, preprocessed_frame, is_new_episode):    \n","    if is_new_episode:\n","      # Clear our stacked_frames\n","      self.init_stack()\n","          \n","      # Because we're in a new episode, copy the same frame 4x, apply element-wise maxima\n","      maxframe = np.maximum(preprocessed_frame, preprocessed_frame)\n","      self.stacked_frames.append(maxframe)\n","      self.stacked_frames.append(maxframe)\n","      self.stacked_frames.append(maxframe)\n","      self.stacked_frames.append(maxframe)\n","          \n","    else:\n","      #Since deque append to right, we can fetch the previous frame\n","      maxframe=np.maximum(self.stacked_frames[-1], preprocessed_frame)\n","          \n","      # Append frame to deque, automatically removes the oldest frame\n","      self.stacked_frames.append(maxframe)\n","            \n","    # Stack the frames\n","    stacked_state = np.stack(self.stacked_frames, axis=2)\n","    \n","    return stacked_state\n","\n","  def empty_stack(self):\n","    return np.zeros((self.frame_w, self.frame_h, 4))"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"-tS6VUWysrFA","executionInfo":{"status":"ok","timestamp":1605008453200,"user_tz":-330,"elapsed":1124,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"source":["class Environment():\n","  def __init__(self):\n","    self.game, self.actions = self._create()\n","\n","  def _create(self):\n","      game = DoomGame()\n","      \n","      # Load the correct configuration\n","      game.load_config(\"basic.cfg\")\n","      \n","      # Load the correct scenario (in our case basic scenario)\n","      game.set_doom_scenario_path(\"basic.wad\")\n","\n","      game.set_window_visible(False)\n","      \n","      game.init()\n","      \n","      # Here our possible actions\n","      left = [1, 0, 0]\n","      right = [0, 1, 0]\n","      shoot = [0, 0, 1]\n","      possible_actions = [left, right, shoot]\n","      \n","      return game, possible_actions\n","\n","  def OLD_bootstrap(self, sf, exp_replay):\n","    game = self.game\n","    max_elements = 3\n","    num_elements = 0\n","    while (num_elements < max_elements):\n","      game.new_episode()\n","\n","      state = game.get_state()\n","      frame = state.screen_buffer\n","      preprocessed_frame = sf.preprocess(frame)\n","      stacked_state = sf.combine_prev(preprocessed_frame, is_new_episode=True)\n","\n","      done = False\n","      while ((num_elements < max_elements) and not done):\n","        action = random.choice(self.actions)\n","        reward = game.make_action(action)\n","        done = game.is_episode_finished()\n","\n","        if (not done):\n","          next_state = game.get_state()\n","          next_frame = next_state.screen_buffer\n","          next_preprocessed_frame = sf.preprocess(next_frame)\n","          next_stacked_state = sf.combine_prev(next_preprocessed_frame, is_new_episode=False)\n","        else:\n","          next_stacked_state = sf.empty_stack()\n","\n","        # Package a sample of (state, action, reward, next_state) and add to Experience Replay\n","        exp_replay.add(stacked_state, action, reward, next_stacked_state, done)\n","        num_elements += 1\n","\n","        state = next_state\n","        stacked_state = next_stacked_state\n","\n","        time.sleep(0.02)\n","      time.sleep(2)"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BgByNNG54lTW"},"source":["### Experience Replay Memory"]},{"cell_type":"code","metadata":{"id":"DAJ7TOP3bKFG","executionInfo":{"status":"ok","timestamp":1605001706985,"user_tz":-330,"elapsed":1214,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"source":["class Replay():\n","  def __init__(self, mem_sz, batch_sz):\n","    self.mem_sz = mem_sz\n","    self.batch_sz = batch_sz\n","    self.mem = self._create()\n","\n","  def _create(self):\n","    mem = deque(maxlen=self.mem_sz)\n","    return mem\n","\n","  def add(self, stacked_state, action, reward, next_stacked_state, done):\n","    self.mem.append((stacked_state, action, reward, next_stacked_state, done))\n","\n","  def sample(self):\n","    batch = random.sample(self.mem, self.batch_sz)\n","    return np.array(batch)"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2WwgeWRLVbtP"},"source":["### Deep Q Network"]},{"cell_type":"code","metadata":{"id":"JDcZDKVtVd0b","executionInfo":{"status":"ok","timestamp":1605016614267,"user_tz":-330,"elapsed":910,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}}},"source":["import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Conv2D, BatchNormalization, Flatten, Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","\n","class E_Greedy():\n","  def __init__(self, epsilon_start, epsilon_end, actions, qn):\n","    self.epsilon = self.epsilon_start = epsilon_start\n","    self.epsilon_end = epsilon_end\n","    self.actions = actions\n","    self.num_actions = len(actions)\n","    self.qn = qn\n","\n","  def decay(self, i_episode, num_episodes):\n","    epsilon_decay = (num_episodes - i_episode) / num_episodes\n","    self.epsilon = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * epsilon_decay\n","\n","  def pick_action(self, stacked_state):\n","    if (random.uniform(0,1) < self.epsilon):\n","      action_idx = random.randint(0, self.num_actions - 1)\n","    else:\n","      qvalues = self.qn.predict(stacked_state[None, ...])\n","      action_idx = np.argmax(qvalues)\n","    return (action_idx, self.actions[action_idx])\n","\n","class DQN():\n","  def __init__(self, lr, gamma):\n","    self.lr = lr\n","    self.gamma = gamma\n","\n","    self.q_network = self.create_qn()\n","    self.target_network = self.create_qn()\n","    self.copy_weights()\n","\n","  def create_qn(self):\n","    model = Sequential([\n","      # Input is 84x84x4\n","      Input(shape=(84, 84, 4)),\n","      Conv2D(filters=32, kernel_size=(8, 8), strides=(4,4), activation='relu', padding='valid', kernel_initializer='glorot_normal'),\n","      BatchNormalization(),\n","      Conv2D(filters=64, kernel_size=(4, 4), strides=(2,2), activation='relu', padding='valid', kernel_initializer='glorot_normal'),\n","      BatchNormalization(),\n","      Conv2D(filters=128, kernel_size=(4, 4), strides=(2,2), activation='relu', padding='valid', kernel_initializer='glorot_normal'),\n","      BatchNormalization(),\n","      Flatten(),\n","      Dense(512, activation='relu', kernel_initializer='he_uniform'),\n","      Dense(3, activation='relu', kernel_initializer='he_uniform')\n","    ])\n","    model.compile(loss='mse', optimizer=Adam(lr=self.lr))\n","    return model\n","\n","  def train(self, batch):\n","    batch_sz = batch.shape[0]\n","    cur_stacked_states = np.stack(batch[:, 0].ravel())\n","    next_stacked_states = np.stack(batch[:, 3].ravel())\n","\n","    cur_qvalues = self.q_network.predict(cur_stacked_states)\n","    next_qvalues = self.target_network.predict(next_stacked_states)\n","\n","    for i in range(batch_sz):\n","      _, action_idx, reward, _, done = batch[i]\n","      if (done):\n","        cur_qvalues[i, action_idx] += reward\n","      else:\n","        cur_qvalues[i, action_idx] += reward + self.gamma * np.max(next_qvalues[i])\n","\n","    self.q_network.fit(cur_stacked_states, cur_qvalues, batch_size=batch_sz, epochs=1, verbose=0)\n","\n","  def copy_weights(self):\n","    self.target_network.set_weights(self.q_network.get_weights())"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tqgJARVr4p18"},"source":["### Main Loop"]},{"cell_type":"code","metadata":{"id":"lPgfUIOS4t7C","executionInfo":{"status":"ok","timestamp":1605016895034,"user_tz":-330,"elapsed":118857,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"dc226f63-be29-49a9-f272-9e2d45c177ea","colab":{"base_uri":"https://localhost:8080/"}},"source":["def main():\n","  exp_replay_sz = 200\n","  batch_sz=3\n","  frame_w, frame_h = 84, 84\n","  stack_size = 4 # We stack 4 composite frames in total\n","  total_steps = 0\n","  num_copy_steps = 100\n","  num_pre_populate = 6\n","  lr, gamma = 0.002, 0.99\n","  epsilon_start, epsilon_end = 1.0, 0.05\n","  num_episodes = 2\n","\n","  sf = StackFrame(stack_size, frame_w, frame_h)\n","  env = Environment()\n","  game, actions = env.game, env.actions\n","  exp_replay = Replay(mem_sz=exp_replay_sz, batch_sz=batch_sz)\n","  dq_network = DQN(lr, gamma)\n","  egp = E_Greedy(epsilon_start, epsilon_end, actions, dq_network.q_network)\n","\n","  for i_episode in range(num_episodes):\n","    game.new_episode()\n","    egp.decay(i_episode, num_episodes)\n","\n","    state = game.get_state()\n","    frame = state.screen_buffer\n","    misc = state.game_variables\n","    preprocessed_frame = sf.preprocess(frame)\n","    stacked_state = sf.combine_prev(preprocessed_frame, is_new_episode=True)\n","\n","    done = False\n","    while not done:\n","      action_idx, action = egp.pick_action(stacked_state)\n","      reward = game.make_action(action)\n","      done = game.is_episode_finished()\n","\n","      if (not done):\n","        next_state = game.get_state()\n","        next_frame = next_state.screen_buffer\n","        next_preprocessed_frame = sf.preprocess(next_frame)\n","        next_stacked_state = sf.combine_prev(next_preprocessed_frame, is_new_episode=False)\n","      else:\n","        next_stacked_state = sf.empty_stack()\n","\n","      # Package a sample of (state, action, reward, next_state) and add to Experience Replay\n","      exp_replay.add(stacked_state, action_idx, reward, next_stacked_state, done)\n","\n","      if (total_steps >= num_pre_populate):\n","        # Then pick a batch from Experience Replay and use it for training\n","        batch = exp_replay.sample()\n","        dq_network.train(batch)\n","\n","      state = next_state\n","      stacked_state = next_stacked_state\n","\n","      total_steps += 1\n","      if (total_steps % num_copy_steps == 0):\n","        dq_network.copy_weights()\n","\n","      time.sleep(0.02)\n","    print ('Episode complete ', i_episode, total_steps)\n","    time.sleep(2)\n","  game.close()\n","\n","main()"],"execution_count":81,"outputs":[{"output_type":"stream","text":["Episode complete  0 252\n","Episode complete  1 552\n"],"name":"stdout"}]}]}