{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhj9PN_-Vc4j",
        "colab_type": "text"
      },
      "source": [
        "### Character-level Text Generation - [Tensorflow tutorial](https://www.tensorflow.org/tutorials/sequences/text_generation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IFit7QOVjmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lUA2OCYVzhH",
        "colab_type": "code",
        "outputId": "598bd6dc-14c3-4fee-8456-0841189f5bf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# Download the Shakespeare dataset\n",
        "data_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Read all the text in the file into a single array\n",
        "text = open(data_file, 'r').read()\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters\\n'.format(len(text)))\n",
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])\n",
        "\n",
        "# The unique characters in the file. \n",
        "# Vocab contains all the characters like 'a' - 'Z', '!', '\\n', '?', '&', ' ', '.' and so on\n",
        "vocab = sorted(set(text))\n",
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHUjGSKVc7k7",
        "colab_type": "code",
        "outputId": "e9de6267-c98a-4f22-ff25-d88ffbd1d4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "# Vectorize the text - map strings to a numerical representation. \n",
        "# Create two lookup tables: one mapping characters to numbers, and another for numbers to characters.\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# map the character as indexes from 0 to len(unique)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print ('=========== Char -> Index\\n', char2idx, '\\n========== Index -> Char\\n', idx2char)\n",
        "print ('=========== Sample of first 100 charaters\\n', repr(text[0:100]), '\\n===========\\n', text_as_int[0:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== Char -> Index\n",
            " {'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64} \n",
            "========== Index -> Char\n",
            " ['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
            " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n",
            "=========== Sample of first 100 charaters\n",
            " 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou' \n",
            "===========\n",
            " [18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPfX6t6anPpH",
        "colab_type": "code",
        "outputId": "eae6c99b-c4a0-4182-be3d-4d6d58e7d687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Create training examples and targets. Divide the text into example sequences. Each input sequence will contain seq_length characters from the text.\n",
        "# For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "\n",
        "seq_length = 100 # number of time-steps\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# Create a TF Dataset object from the text indices\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "# take(5) returns a tensor of the first 5 elements (ie. numeric indices) from the Dataset\n",
        "# So 'i' is also a tensor with one element of the Dataset. i.numpy() converts that to a plain number\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGT8bwvBq2A0",
        "colab_type": "code",
        "outputId": "fb4c591a-cbae-407c-fbd8-91bc1dab9a78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Use batch() to convert the individual characters into sequences of the desired size\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  # Use join() to join the individual characters into a single string\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjuITWJLucvk",
        "colab_type": "code",
        "outputId": "95333c35-b901-452d-9e55-005d81695834",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# For each sequence, duplicate and shift it to form the input and target text by using map() to apply a simple function to each batch\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# After the map, each element of the dataset is a tuple with two tensors in it.\n",
        "# The first tensor is an input text sequence, and the second tensor is a target text sequence\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "\n",
        "print ('======================')\n",
        "  \n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "======================\n",
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEqRyD6h2h97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training batches - shuffle the data and pack it into batches\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "# Now, each element of the dataset will be a tuple with two tensors.\n",
        "# The first tensor is an input batch and the second tensor is an target batch\n",
        "# The input batch tensor has 'BATCH_SIZE' items in it, each item being an input sequence of length 'seq_length'\n",
        "# The target batch tensor has 'BATCH_SIZE' items in it, each item being a target sequence of length 'seq_length'\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrqiFwVDOjmU",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://tensorflow.org/tutorials/sequences/images/text_generation_training.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9vDNFzc--qa",
        "colab_type": "code",
        "outputId": "cdf4906c-27aa-44a6-e80f-351c859b737a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024\n",
        "\n",
        "# Build The Model with three layers:\n",
        "#     tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
        "#     tf.keras.layers.LSTM: A type of RNN with size units=rnn_units\n",
        "#     tf.keras.layers.Dense: The output layer, with vocab_size outputs.\n",
        "\n",
        "if tf.test.is_gpu_available():\n",
        "  rnn = tf.keras.layers.CuDNNLSTM\n",
        "else:\n",
        "  import functools\n",
        "  rnn = functools.partial(\n",
        "    tf.keras.layers.LSTM, recurrent_activation='sigmoid')\n",
        "\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    rnn(rnn_units,\n",
        "        return_sequences=True,\n",
        "        recurrent_initializer='glorot_uniform',\n",
        "        stateful=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_8 (CuDNNLSTM)     (64, None, 1024)          5251072   \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,334,337\n",
            "Trainable params: 5,334,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSoZAtWOCFb4",
        "colab_type": "code",
        "outputId": "7113a386-f788-4a57-9201-3c9ebe4a5670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# We feed it sequences of length 'seq_length' but the model can be run on inputs of any length\n",
        "\n",
        "# Try the untrained model for one example, and check the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n",
        "  \n",
        "# To get actual predictions from the model we need to sample from the output distribution, \n",
        "# to get actual character indices. This distribution is defined by the logits over the \n",
        "# character vocabulary. We must sample, not use an argmax of the output\n",
        "\n",
        "# Get a prediction for the first example from the batch, by sampling from its outputs.\n",
        "# example_batch_predictions[0] gives us the first example from the batch. It has\n",
        "# shape [seq_length, vocab_size]\n",
        "#\n",
        "# We ask it to return to us one sample from the distribution ie. 'num_samples = 1'\n",
        "# The returned 'sampled_indices' will have shape [seq_length, num_samples] ie [seq_length, 1]\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "\n",
        "# Squeeze() removes the second dimension since it has size=1\n",
        "# 'sampled_indices' now has shape [seq_length, ] ie. there is one predicted output\n",
        "# value for each character in the input sequence\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "print ('Sample indices shape ', sampled_indices.shape)\n",
        "sampled_indices\n",
        "\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "\n",
        "print(\"\\nNext Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n",
            "Sample indices shape  (100,)\n",
            "Input: \n",
            " 'd and father, madam,\\nI will not marry yet; and, when I do, I swear,\\nIt shall be Romeo, whom you know'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'pmrcO3wj$tpoZheM. b?zw?oDJbblWRkASjHxdHjzeSPekW$G.m3G33T3cYWshk \\nNlPVdK;I3NtikstVu.v\\nQ.i.WoWVylFXNl?'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFr9Q3DXLJuS",
        "colab_type": "code",
        "outputId": "b971b6fb-f6cd-408d-f5e4-8fe64806db70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "# Train the model - as a standard classification problem. Given the previous RNN state, and the \n",
        "# input for this time step, predict the class of the next character.\n",
        "\n",
        "# Attach an optimizer, and a loss function\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "model.compile(\n",
        "    optimizer = tf.train.AdamOptimizer(),\n",
        "    loss = loss)\n",
        "\n",
        "# Configure checkpoints - with tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training\n",
        "\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "# Execute the training\n",
        "EPOCHS=3\n",
        "\n",
        "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1742887\n",
            "Epoch 1/3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0705 09:56:43.711948 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer\n",
            "W0705 09:56:43.713227 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer\n",
            "W0705 09:56:43.714274 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.global_step\n",
            "W0705 09:56:43.717313 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer.beta1_power\n",
            "W0705 09:56:43.720095 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer.beta2_power\n",
            "W0705 09:56:43.722656 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "W0705 09:56:43.723775 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
            "W0705 09:56:43.724933 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.recurrent_kernel\n",
            "W0705 09:56:43.727011 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
            "W0705 09:56:43.729013 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "W0705 09:56:43.730373 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "W0705 09:56:43.731716 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "W0705 09:56:43.733467 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
            "W0705 09:56:43.734351 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.recurrent_kernel\n",
            "W0705 09:56:43.735995 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
            "W0705 09:56:43.737388 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "W0705 09:56:43.738714 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "W0705 09:56:43.739923 140673951852416 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n",
            "W0705 09:56:43.742053 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer\n",
            "W0705 09:56:43.743527 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer\n",
            "W0705 09:56:43.744856 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.global_step\n",
            "W0705 09:56:43.746143 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer.beta1_power\n",
            "W0705 09:56:43.747903 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer.beta2_power\n",
            "W0705 09:56:43.749210 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "W0705 09:56:43.750087 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.kernel\n",
            "W0705 09:56:43.751451 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.recurrent_kernel\n",
            "W0705 09:56:43.753218 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-1.bias\n",
            "W0705 09:56:43.754508 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "W0705 09:56:43.755825 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "W0705 09:56:43.757169 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "W0705 09:56:43.758504 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.kernel\n",
            "W0705 09:56:43.759271 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.recurrent_kernel\n",
            "W0705 09:56:43.759978 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-1.bias\n",
            "W0705 09:56:43.760724 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "W0705 09:56:43.761475 140673951852416 util.py:244] Unresolved object in checkpoint: (root).optimizer.optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "W0705 09:56:43.762342 140673951852416 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "174/174 [==============================] - 15s 88ms/step - loss: 2.5780\n",
            "Epoch 2/3\n",
            "174/174 [==============================] - 13s 76ms/step - loss: 1.8907\n",
            "Epoch 3/3\n",
            "174/174 [==============================] - 13s 77ms/step - loss: 1.6461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNsbO1I4cc4g",
        "colab_type": "code",
        "outputId": "03b5520e-86f3-4cd6-ad79-3c5e18cf6e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# Restore the latest checkpoint. To keep this prediction step simple, use a batch size of 1.\n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_9 (CuDNNLSTM)     (1, None, 1024)           5251072   \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 5,334,337\n",
            "Trainable params: 5,334,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY7qYrIEdA3j",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://tensorflow.org/tutorials/sequences/images/text_generation_sampling.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ_E3kjkc8ip",
        "colab_type": "code",
        "outputId": "be6aae1e-6bb8-4221-b570-53e912e1e36b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "# Generate text \n",
        "#\n",
        "# Start by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "# Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "# Sample from the distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "# \n",
        "# The RNN state returned by the model is fed back into the model so that it now has more context, instead \n",
        "# of only one word. After predicting the next word, the modified RNN states are again fed back into the \n",
        "# model, which is how it learns as it gets more context from the previously predicted words.\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  # Add an extra dimension of size 1 for the batch. The shape is [1, len(start_string)]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      # 'input_eval' is a single character, except for the first time through\n",
        "      # the loop, when it is the same as the start_string\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a multinomial distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      \n",
        "      # The predicted value from the distribution is of shape [len(input_eval), 1]\n",
        "      # We take [-1, 0] ie. the single character which is the predicted value for the \n",
        "      # last character in input_eval\n",
        "      # So predicted_id is a single scalar number\n",
        "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      # 'input_eval' now is a single character and has shape [1, 1] \n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      # Save away the single predicted character\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: VIjRjE&VEJPHEqQD3YEC$NVYZNYTHIOADAUKxCZDBOFAHFMHEHBUNWNVGOCHlZAdZVMONGjOD&.ZYOZEEMEEL$ZKUCGAUABZNUUD$QYZVIQFP3VQGCKQE$ZKDZOKBRA3DP&DYUK;3RHZY:NWEARTHSGEMWONVQY\n",
            "HAKENVONMENGERD LORETCOF Eame the othel men\n",
            "Asheer'd when he Lith my here:\n",
            "My have one-brother out ofmeniby for the gaven,\n",
            "And this tould their curest complessige.\n",
            "\n",
            "Battban:\n",
            "We can thou, changles, pare, dy I cannoty:\n",
            "Who, that your lordser: to more hand be more?\n",
            "\n",
            "Second Sen:\n",
            "Whure is be a king, let him betite.\n",
            "\n",
            "LORD EDWARD:\n",
            "But thind unong steke palies\n",
            "To the gelour brew himself, singe\n",
            "Before I am I was bud uned I ame least I know more in not\n",
            "les dead? I subj, for 'tway, if:\n",
            "Ifford get wish house.\n",
            "\n",
            "Second Co:\n",
            "Mur douns\n",
            "And ancolth, hor how may have how he canist those fair groly'd\n",
            "F, inse, betreate Hercince but bein\n",
            "When deed can terus by the lire, to geet int\n",
            "To sen to will bed: by soup in the clown;\n",
            "That back her own pather, Hord you word.\n",
            "A good knews sir, and thou lovess you,\n",
            "he\n",
            "Tould to faight,\n",
            "Brows for their facce whe sou\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUiak9hfiH1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r training_checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}