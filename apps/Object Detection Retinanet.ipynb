{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Object Detection Retinanet.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["dhhGqGK5vhfK","vCkiXxG3vF5C","olBXhaqNvNW3","PZqVyxIyDsI8","6_g_XkjxVaZa","XGtoECVhNcy4"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"08_TsyDoqlwo","colab_type":"text"},"source":["## Object Detection\n","Fastai DL Course Lesson 8 and 9 - [video](https://www.youtube.com/watch?v=0frKXR-2PBY&feature=youtu.be), [notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb)"]},{"cell_type":"code","metadata":{"id":"-SM1fKBRb6f5","colab_type":"code","colab":{}},"source":["!curl -s https://course.fast.ai/setup/colab | bash"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhhGqGK5vhfK","colab_type":"text"},"source":["### Download and prepare data for learning"]},{"cell_type":"code","metadata":{"id":"XzXugyz6t7jg","colab_type":"code","colab":{}},"source":["!mkdir pascal\n","\n","# Download the Pascal 2007 data set containing images and annotations\n","!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n","!tar xf VOCtrainval_06-Nov-2007.tar\n","!mv VOCdevkit pascal\n","\n","# Download the annotations in JSON format (as the above dataset has them in XML)\n","# The annotations contain multiple object types along with respective bounding boxes for each image\n","!wget https://storage.googleapis.com/coco-dataset/external/PASCAL_VOC.zip\n","!unzip PASCAL_VOC.zip\n","\n","!mv PASCAL_VOC/*.json pascal\n","!rmdir PASCAL_VOC"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UBkm6myQhukD","colab_type":"code","colab":{}},"source":["from pathlib import Path\n","import collections\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import json\n","\n","from fastai.vision import *\n","\n","#-----------------------------------------------------\n","# List the contents of the two important directories of the\n","# dataset - one containing the info JSON file and the other\n","# containing the JPEG images themselves.\n","#-----------------------------------------------------\n","def list_data_dirs ():\n","  # Info JSON\n","  data_dir = Path.cwd()/'pascal'\n","  print (list(data_dir.iterdir()))\n","  info_json = data_dir / 'pascal_train2007.json'\n","\n","  # JPEG Image files\n","  voc = data_dir/'VOCdevkit'/'VOC2007'\n","  print (list(voc.iterdir()))\n","  jpeg_dir = voc/'JPEGImages'\n","  print (list(jpeg_dir.iterdir())[:3])\n","\n","  # Return the path objects for both\n","  return data_dir, info_json, jpeg_dir\n","\n","#-----------------------------------------------------\n","# Bounding boxes in the dataset are specified as [top-left-col, top-left-row, height, width]\n","# We convert them to [top-left-row, top-left-col, bottom-right-row, bottom-right-col] so\n","# that it is consistent with numpy\n","#-----------------------------------------------------\n","def hw_bb(bb): return np.array([bb[1], bb[0], bb[3]+bb[1]-1, bb[2]+bb[0]-1])\n","\n","#-----------------------------------------------------\n","# Create an inverse of the above function to reverse the conversion\n","#-----------------------------------------------------\n","def bb_hw(a): return np.array([a[1],a[0],a[3]-a[1]+1,a[2]-a[0]+1])\n","\n","#-----------------------------------------------------\n","# List the contents of the two important directories of the\n","# dataset - one containing the info JSON file and the other\n","# containing the JPEG images themselves.\n","#-----------------------------------------------------\n","def process_json (info_json):\n","  \n","  # Load the JSON file, which contains 4 keys:\n","  #    'images': maps to a list of image_ids to image_filenames\n","  #    'annotations': maps to a list of bounding boxes for each image, and object category IDs\n","  #    'categories': maps object category IDs to category names eg. car, horse etc.\n","  with info_json.open() as fp:\n","    info_dict = json.load(fp)\n","    print (info_dict.keys())\n","\n","  IMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', 'categories']\n","  print (info_dict[IMAGES][:2])\n","  print (info_dict[ANNOTATIONS][:2]) \n","  print (info_dict[CATEGORIES][:2])\n","  \n","  # Create dictionary mapping image id to image file names\n","  image_dict = {image['id']:image['file_name'] for image in info_dict[IMAGES] }\n","  \n","  # Create dictionary mapping category id to category name\n","  category_dict = {category['id']:category['name'] for category in info_dict[CATEGORIES] }\n","\n","  # Create dictionary  mapping image id to a list of bounding boxes and category names\n","  annot_dict = collections.defaultdict(lambda:[])\n","  for annot in info_dict[ANNOTATIONS]:\n","    if not annot['ignore']:\n","      # Tuple of (category name, [bounding box coordinates])\n","      new_tuple = (category_dict[annot['category_id']], hw_bb(annot['bbox']))\n","    \n","      # Append the new tuple to the existing list of tuples for this image id\n","      # Because we used defaultdict() above, it will initialise a default value of\n","      # an empty list if the key for the image id doesn't exist\n","      annot_dict[annot['image_id']].append (new_tuple)\n","    \n","  # Return the three dictionaries we created\n","  return (image_dict, annot_dict, category_dict)\n","\n","data_dir, info_json, jpeg_dir = list_data_dirs ()\n","image_dict, annot_dict, category_dict = process_json (info_json)\n","\n","print(image_dict[17])\n","print (category_dict[2])\n","print(list(annot_dict.items())[3])\n","open_image (jpeg_dir/image_dict[17])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6e-JefqCwFfT","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Some Matplotlib utility functions\n","#-----------------------------------------------------\n","\n","#-----------------------------------------------------\n","# Show an image\n","#-----------------------------------------------------\n","def show_img(im, figsize=None, ax=None):\n","    if not ax: fig,ax = plt.subplots(figsize=figsize)\n","    ax.imshow(im)\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","    return ax\n","\n","#-----------------------------------------------------\n","# Draw bounding box\n","# A trick to making text visible regardless of background is to use white text with black outline, or visa versa\n","#-----------------------------------------------------\n","def draw_outline(o, lw):\n","    o.set_path_effects([patheffects.Stroke(\n","        linewidth=lw, foreground='black'), patheffects.Normal()])\n","\n","#-----------------------------------------------------\n","# Draw a rectangle\n","# Note that * in argument lists is the splat operator. In this case it's a little shortcut compared to writing out b[-2],b[-1].\n","#-----------------------------------------------------\n","def draw_rect(ax, b):\n","    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor='white', lw=2))\n","    draw_outline(patch, 4)\n","\n","#-----------------------------------------------------\n","# Draw some text\n","#-----------------------------------------------------\n","def draw_text(ax, xy, txt, sz=14):\n","    text = ax.text(*xy, txt,\n","        verticalalignment='top', color='white', fontsize=sz, weight='bold')\n","    draw_outline(text, 1)\n","\n","#-----------------------------------------------------\n","# Given the image data, show it and draw the bounding boxes and\n","# annotate the category name\n","#-----------------------------------------------------\n","def draw_image(im_data, annot):\n","  ax = show_img(im_data, figsize=(12,6))\n","  for category_name, bbox in annot:\n","    bb = bb_hw(bbox)\n","    draw_rect(ax, bb)\n","    draw_text(ax, bb[:2], category_name)\n","  \n","#-----------------------------------------------------\n","# Given the image_id, draw the image along with the bounding boxes and category\n","# names of each object in the image\n","#-----------------------------------------------------\n","def draw_idx(image_id, image_dict, annot_dict, jpeg_dir):\n","  image_filename = image_dict[image_id]\n","  \n","  # Fastai open_image() returns the image data with the channel as the first\n","  # dimension, so transpose it to third dimension\n","  im = open_image (jpeg_dir/image_filename)\n","  im_data = np.transpose (im.data, (1, 2, 0))\n","  \n","  # Draw the image with the annotations\n","  draw_image(im_data, annot_dict[image_id])\n","    \n","draw_idx(12, image_dict, annot_dict, jpeg_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LPUys95pGk60","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Create a dictionary mapping each image_id to its largest bounding box\n","#-----------------------------------------------------\n","\n","\n","#-----------------------------------------------------\n","# Helper function to find the largest bounding box for an image\n","# Sort all the bounding boxes in descending order based on the area, and then \n","# return the first one\n","#-----------------------------------------------------\n","def largest_bbox (annot):\n","  # The lambda function computes the area of the bounding box based on the top-left and\n","  # bottom-right corners\n","  sorted_annot = sorted (annot, key=lambda x: (x[1][2] - x[1][0]) * (x[1][3] - x[1][1]), reverse=True)\n","  return sorted_annot[0]\n","\n","# We map each image to a list with only a single boounding box ie. {image_id: [largest_bbox(annot)]} rather\n","# than without a list ie. {image_id: largest_bbox(annot)} so that we can continue to use draw_idx() \n","# which expects a list.\n","largest_annot_dict = {image_id: [largest_bbox(annot)] for image_id, annot in annot_dict.items()}\n","\n","draw_idx(23, image_dict, largest_annot_dict, jpeg_dir)\n","annot_dict[23], largest_annot_dict[23]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E8IRcgxmOu3r","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Convert the bounding box from a numpy array to a string of coordinates\n","# separated by spaces\n","#\n","# Supporting function for creating the CSV file\n","#-----------------------------------------------------\n","def bbox2str (bbox):\n","  bbox_str = \" \".join (str(float(i)) for i in bbox)\n","  return (bbox_str)\n","\n","#-----------------------------------------------------\n","# Write the data out to a CSV file using Pandas, so that we can create a Fastai Dataset\n","# easily by import from the CSV\n","#-----------------------------------------------------\n","def save_csv (csv_file):\n","  # We create several variations of columns for the bounding box data, after converting to floats\n","  # 'box' - a single string with all the coordinates separated by spaces\n","  # 'tl' and 'br' - coordinates of the top-left point and bottom-right point as an array [row, col]\n","  # 'tlr' and 'tlc '- coordinate of the top-left point as row and column. Similarly for the bottom-right\n","  # We actually use only 'tl' and 'br'\n","  df = pd.DataFrame (\n","      {'file': [image_dict[image_id] for image_id in largest_annot_dict.keys()], \n","      'category': [annot[0][0] for annot in largest_annot_dict.values()],\n","      'box': [bbox2str(annot[0][1]) for annot in largest_annot_dict.values()],\n","      'tl': [[float(annot[0][1][0]), float(annot[0][1][1])] for annot in largest_annot_dict.values()],\n","      'br': [[float(annot[0][1][2]), float(annot[0][1][3])] for annot in largest_annot_dict.values()],\n","      'tlr': [float(annot[0][1][0]) for annot in largest_annot_dict.values()],\n","      'tlc': [float(annot[0][1][1]) for annot in largest_annot_dict.values()],\n","      'brr': [float(annot[0][1][2]) for annot in largest_annot_dict.values()],\n","      'brc': [float(annot[0][1][3]) for annot in largest_annot_dict.values()]},\n","      columns=['file', 'category', 'box', 'tl', 'br', 'tlr', 'tlc', 'brr', 'brc'])\n","  df.to_csv(csv_file, index=False)\n","  return (df)\n","  \n","csv_file = data_dir/'data.csv'\n","df = save_csv (csv_file)\n","df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vCkiXxG3vF5C","colab_type":"text"},"source":["### Single Object Classification of Largest Object (without bounding box)"]},{"cell_type":"code","metadata":{"id":"TwRQ7k1HaTea","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# The first step is to do single object classification of the largest object for each\n","# image, without considering the bounding boxes\n","#\n","# Build the data bunch as usual from the CSV file. Since it resizes all images to size 224x224,\n","# we use the SQUISH method (shrink both sides without preserving aspect ratio) rather than a \n","# CROP method (preserve aspect ratio, make the smaller side=224 and then crop from centre)\n","#-----------------------------------------------------\n","tfms = get_transforms()\n","data_oc = ImageDataBunch.from_csv(data_dir, 'VOCdevkit/VOC2007/JPEGImages', csv_labels=csv_file, ds_tfms=tfms, \n","                               size=224, resize_method=ResizeMethod.SQUISH, bs=64).normalize(imagenet_stats)\n","data_oc.show_batch(rows=3, figsize=(7,6))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U5-Pm-8Bpj6K","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Create a CNN learner object based on resnet transfer learning, and use the Learning Rate Finder\n","#-----------------------------------------------------\n","learn_oc = cnn_learner(data_oc, models.resnet34, metrics=[accuracy])\n","learn_oc.lr_find()\n","learn_oc.recorder.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHBFIW3KUvSm","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Train using the Fit-One-Cycle\n","#-----------------------------------------------------\n","lr = 2e-2\n","learn_oc.fit_one_cycle(cyc_len=2, max_lr=slice(lr))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTWWiWiiiapC","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Unfreeze the earlier layers and use the LR Finder again\n","#-----------------------------------------------------\n","learn_oc.unfreeze()\n","learn_oc.lr_find()\n","learn_oc.recorder.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mhpntzJi7rv","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Train again using the Fit-One-Cycle\n","#-----------------------------------------------------\n","learn_oc.fit_one_cycle(cyc_len=2, max_lr=slice(1e-6,1e-4))\n","learn_oc.freeze()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mdGBrS9zkOrA","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Look at the results ie. incorrect predictions, using top losses\n","#-----------------------------------------------------\n","interp_oc = ClassificationInterpretation.from_learner(learn_oc)\n","interp_oc.plot_top_losses(9, figsize=(15,11), heatmap=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"evKAdw2RlYMw","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Look at the results ie. confusion matrix\n","#-----------------------------------------------------\n","interp_oc.plot_confusion_matrix(figsize=(8,6))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g375ytuMlWjK","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Look at the results ie. classes incorrectly predicted\n","#-----------------------------------------------------\n","interp_oc.most_confused(min_val=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"olBXhaqNvNW3","colab_type":"text"},"source":["### Regression of Largest bounding box (without classification)"]},{"cell_type":"code","metadata":{"id":"du3eC4IMxfLn","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# We want to learn how to predict just the bounding box. The bounding box\n","# consists of 4 floating point numbers corresponding to the coordinates of the top-left\n","# and bottom-right corners. So we can model this as a multi-value regression problem.\n","#\n","# We create the data bunch such that:\n","#  X: images\n","#  y: two image points for the two bounding box corners, using the PointsItemList\n","#-----------------------------------------------------\n","\n","#-----------------------------------------------------\n","# Supporting function to compute the 'y' label for the bounding box coordinates, given\n","# the image file name. It looks these up from the CSV dataframe\n","#\n","# Returns a tensor of two points [[tlr, tlc][brr, brc]], as required by PointsItemList\n","#-----------------------------------------------------\n","def bbox_label_func(filepath):\n","    image_name = Path(filepath).name\n","    image_row = df[df['file']==image_name].iloc[0]\n","    tl, br = image_row['tl'], image_row['br']\n","    return tensor([tl, br])\n","\n","#-----------------------------------------------------\n","# The PointsItemList is suitable since our 'y' labels are two bounding box points on an \n","# image. Those labels are computed using a function, by looking up the bounding-box \n","# coordinates from our CSV dataframe\n","#\n","# (I think) The PointsItemList has built-in intelligence to transform the points when\n","# the corresponding image is transformed during data augmentation.\n","#-----------------------------------------------------\n","data_bb = (PointsItemList.from_df(df, path=data_dir, folder='VOCdevkit/VOC2007/JPEGImages')\n","      .split_by_rand_pct()\n","      .label_from_func(bbox_label_func)\n","      .transform(get_transforms(), resize_method=ResizeMethod.SQUISH, size=224)\n","      .databunch(bs=64)) \n","data_bb.normalize(imagenet_stats)\n","\n","# Since this is a PointsItemList, it shows the images along small red dots for the\n","# two bounding box points\n","data_bb.show_batch(rows=3, figsize=(7,6))\n","\n","# No need for tfm_y = True, TfmType.COORD?"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWitCg7tGRKS","colab_type":"code","colab":{}},"source":["data_bb.train_ds.x.show_xys(data_bb.train_ds.x[0:8], data_bb.train_ds.y[0:8])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3aoGJ64aPFoV","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# (I think) The PointsItemList has built-in intelligence to create a learner with the\n","# appropriate final layers suitable for multi-value regression, and the appropriate\n","# loss function.\n","#\n","# There is no need to use custom head to add the custom layers and to manually\n","# add a custom loss function\n","#-----------------------------------------------------\n","learn_bb = cnn_learner(data_bb, models.resnet34)\n","learn_bb.lr_find()\n","learn_bb.recorder.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6DoJIe8VeuY","colab_type":"code","colab":{}},"source":["lr = 2e-2\n","learn_bb.fit_one_cycle(cyc_len=2, max_lr=slice(lr))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I36JtpUVsldH","colab_type":"code","colab":{}},"source":["# https://docs.fast.ai/basic_train.html#See-results\n","learn_bb.data.train_ds[0]\n","learn_bb.predict()\n","learn_bb.get_preds()\n","learn_bb.validate()\n","learn_bb.pred_batch()\n","learn.data.one_item(item)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOKJgTAC3-zS","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# NB: Fastai doesn't yet have a class for Regression Interpretation similar to\n","# ClassificationInterpretation\n","#-----------------------------------------------------\n","learn_bb.show_results()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Ct_XeUPgXSU","colab_type":"code","colab":{}},"source":["# Figure out how to look at model dataloader input data\n","# Figure out how to look at model result outputs\n","# Figure out whether transforms are doing the bounding box as well or not"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"35mt8pYOqao4","colab_type":"text"},"source":["### Object Detection with Retinanet from scratch. Based on [this](https://github.com/fastai/course-v3/blob/master/nbs/dl2/pascal.ipynb) notebook"]},{"cell_type":"markdown","metadata":{"id":"64AKMqf1T1Fq","colab_type":"text"},"source":["**================= Prepare Data ===============**"]},{"cell_type":"code","metadata":{"id":"glSaGinhqlJl","colab_type":"code","colab":{}},"source":["# These lines didn't work. In Jupyter, any variables in the last line of a cell are displayed as\n","# the output of that cell. These two lines were supposed to turn on a feature in Jupyter where variables\n","# on any line, not just the last one, get displayed. \n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","from fastai.vision import *\n","import json\n","\n","# If you ever get the bizarre \"CUDA device-side assert\" error, then uncomment these lines to get a meaningful stack trace so you can debug\n","#import os\n","#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","#print(\"CUDA \", os.environ['CUDA_LAUNCH_BLOCKING']) \n","\n","# Load the Pascal VOC 2007 dataset (which is smaller than the 2012 dataset)\n","path = untar_data(URLs.PASCAL_2007)\n","annots = json.load(open(path/'train.json'))\n","\n","# We ignore the parsed Json. Instead we use fastai built-in get_annotations to prepare the train \n","# and validation data. For each image file, there are one or more bounding boxes and class labels\n","train_images, train_lbl_bbox = get_annotations(path/'train.json')\n","val_images, val_lbl_bbox = get_annotations(path/'valid.json')\n","#tst_images, tst_lbl_bbox = get_annotations(path/'test.json')\n","train_images[1], train_lbl_bbox[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6Ewbr0QznWX","colab_type":"code","colab":{}},"source":["# Open a sample image\n","img = open_image(path/'train'/train_images[1])\n","# ImageBBox is Fastai's class to represent bounding boxes on an image. We create an ImageBBox object from the list of bounding boxes. This will allow us \n","# to apply data augmentation to our bounding box. We need to give it the height and the width of the \n","# original picture, the list of bounding boxes, the list of category ids and the classes list (to map an id to a class)\n","bbox = ImageBBox.create(*img.size, train_lbl_bbox[1][0], [0, 1], classes=['person', 'horse'])\n","img.show(figsize=(6,4), y=bbox)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kFyP9l_4z42M","colab_type":"code","colab":{}},"source":["# If we apply a transform to our image and the ImageBBox object, they stay aligned\n","img = img.rotate(-10)\n","bbox = bbox.rotate(-10)\n","img.show(figsize=(6,4), y=bbox)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMdCENdT1qOh","colab_type":"code","colab":{}},"source":["#------------------------------------------------------\n","# Use the Datablock API to load a Databunch\n","#------------------------------------------------------\n","images, lbl_bbox = train_images+val_images,train_lbl_bbox+val_lbl_bbox\n","img2bbox = dict(zip(images, lbl_bbox))\n","get_y_func = lambda o:img2bbox[o.name]\n","\n","def get_data(bs, size):\n","    src = ObjectItemList.from_folder(path/'train')\n","    src = src.split_by_files(val_images)\n","    src = src.label_from_func(get_y_func)\n","    src = src.transform(get_transforms(), size=size, tfm_y=True)\n","    # our images may have multiple bounding boxes, so the collate function pads them \n","    # to the largest number of bounding boxes\n","    return src.databunch(path=path, bs=bs, collate_fn=bb_pad_collate)\n","  \n","data = get_data(64,128)\n","data.show_batch(rows=3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3u2fqblRhXST","colab_type":"text"},"source":["**========================= Plotting Functions for Visualisation only ======================**"]},{"cell_type":"code","metadata":{"id":"MD4w1KlmU79r","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Utility functions to visualise images, with grids and bounding boxes\n","#-----------------------------------------------------\n","\n","import pdb\n","import IPython.core.debugger as db\n","\n","import matplotlib.cm as cmx\n","import matplotlib.colors as mcolors\n","from cycler import cycler\n","\n","def get_cmap(N):\n","    color_norm  = mcolors.Normalize(vmin=0, vmax=N-1)\n","    return cmx.ScalarMappable(norm=color_norm, cmap='Set3').to_rgba\n","\n","num_color = 12\n","cmap = get_cmap(num_color)\n","color_list = [cmap(float(x)) for x in range(num_color)]\n","\n","def draw_outline(o, lw):\n","    o.set_path_effects([patheffects.Stroke(\n","        linewidth=lw, foreground='black'), patheffects.Normal()])\n","\n","def draw_rect(ax, b, color='white'):\n","    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n","    draw_outline(patch, 4)\n","\n","def draw_text(ax, xy, txt, sz=14, color='white'):\n","    text = ax.text(*xy, txt,\n","        verticalalignment='top', color=color, fontsize=sz, weight='bold')\n","    draw_outline(text, 1)\n","\n","def show_boxes(boxes):\n","    \"Show the `boxes` (size by 4)\"\n","    _, ax = plt.subplots(1,1, figsize=(5,5))\n","    ax.set_xlim(-1,1)\n","    ax.set_ylim(1,-1)\n","    for i, bbox in enumerate(boxes):\n","        bb = bbox.numpy()\n","        rect = [bb[1]-bb[3]/2, bb[0]-bb[2]/2, bb[3], bb[2]]\n","        draw_rect(ax, rect, color=color_list[i%num_color])\n","        draw_text(ax, [bb[1]-bb[3]/2,bb[0]-bb[2]/2], str(i), color=color_list[i%num_color])\n","\n","def show_anchors(ancs, size):\n","    _,ax = plt.subplots(1,1, figsize=(5,5))\n","    ax.set_xticks(np.linspace(-1,1, size[1]+1))\n","    ax.set_yticks(np.linspace(-1,1, size[0]+1))\n","    ax.grid()\n","    ax.scatter(ancs[:,1], ancs[:,0]) #y is first\n","    ax.set_yticklabels([])\n","    ax.set_xticklabels([])\n","    ax.set_xlim(-1,1)\n","    ax.set_ylim(1,-1) #-1 is top, 1 is bottom\n","    for i, (x, y) in enumerate(zip(ancs[:, 1], ancs[:, 0])): ax.annotate(i, xy = (x,y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P3N03Ss1hhY2","colab_type":"text"},"source":["**===================== Supporting functions needed for Loss Function =====================**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EMWi9A4jZoJN","colab":{}},"source":["#-----------------------------------------------------\n","# Define a grid of cells on the image, given size as a tuple (number of rows, number of columns)\n","# Compute the parameters of the grid eg. height and width of the grid, and height and width of each cell.\n","# \n","# Our convention is that y is first (like in numpy or PyTorch), and that all coordinates are \n","# scaled from -1 to 1 (-1 being top/right, 1 being bottom/left).\n","# So, to avoid confusion, for our variable names which represent coordinates, rather than use 'x' and y', we use 'r'and 'c' which \n","# stands for 'row' (ie. vertical coordinates) and 'column' (ie. horizontal coordinates)\n","#-----------------------------------------------------\n","def grid_info(size):\n","  # Number of rows, number of columns in the grid\n","  size_r, size_c = size\n","  # Coordinates of Top Row, Bottom Row, Left Column and Right Column\n","  top_r, bottom_r, left_c, right_c = -1, 1, 1, -1\n","  # Height and Width of the Grid\n","  grid_h, grid_w = bottom_r - top_r, left_c - right_c\n","  # Height and Width of Each cell\n","  cell_h, cell_w = grid_h/size_r, grid_w/size_c\n","\n","  return (size_r, size_c, top_r, bottom_r, left_c, right_c, cell_h, cell_w)\n","\n","#-----------------------------------------------------\n","# Return an array with the coordinates of all the grid cell centres. It has shape (total number of centres, 2) \n","# where the total number of centres = number of grid rows * number of grid columns\n","#\n","# Note that we return coordinates of each centre using the coordinate system from -1 to +1 as above\n","# [[ctr1_r, ctr1_c], [ctr2_r, ctr2_c]....]\n","#-----------------------------------------------------\n","def create_grid_centres (size):\n","  size_r, size_c, top_r, bottom_r, left_c, right_c, cell_h, cell_w = grid_info(size)\n","\n","  # Sequence of values of the centre point in vertical direction, from top to bottom\n","  val_r = np.linspace (top_r + cell_h/2, bottom_r - cell_h/2, size_r)\n","  # Sequence of values of the centre point in horizontal direction, from right to left ie. in the 'reverse' direction\n","  # We do this so that the show_anchors() puts a index number on each centre in the 'correct' direction\n","  val_c = np.linspace (right_c + cell_w/2, left_c - cell_w/2, size_c)\n","  \n","  # Tile the horizontal values as [a b c] -> [a b c a b c ...]\n","  # This represents the 'horizontal half' of the coordinate of each centre\n","  pts_c = np.tile(val_c, size_r)\n","  # Repeat the vertical values as [u v w] -> [u u u v v v w w w]\n","  # This represents the 'vertical half' of the coordinate of each centre\n","  pts_r = np.repeat(val_r, size_c)\n","\n","  # Now finally we get actual coordinates for the centres by concatenating the two halves of \n","  # the sequence of values as [column of vertical values, column of horizontal values] to give us\n","  # [[u, a], [u, b], [u, c], [v, a], ....]\n","  ctrs = np.stack((pts_r, pts_c), axis=1)\n","  return (ctrs)\n","\n","gsize=(2, 5)\n","bar = create_grid_centres (gsize)\n","print (bar)\n","show_anchors (bar, gsize)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qRX6pZupwmS","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# We use different grid sizes (ie. gsizes). For each grid size, we define several anchor boxes\n","# per grid cell. The anchor boxes are defined only by their height and width, and are assumed to\n","# be centred on the centre of the grid cell. The height and width of the anchor boxes is calculated\n","# using a formula based on a list of 'ratios' and 'scales'\n","#\n","# Typically we have 5 grid sizes ranging from (2,2) to (32, 32). Typically, for each grid size, for each grid cell, we have\n","# 9 anchors each. eg. a (2, 2) grid has 4 (ie. 2 * 2) grid cells. That means a total of 36 (ie. 4 * 9) anchors for this grid.\n","# Similarly we have anchors for all the other grid sizes.\n","#-----------------------------------------------------\n","ratios = [1/2,1,2]\n","scales = [1,2**(-1/3), 2**(-2/3)] \n","#Paper used [1,2**(1/3), 2**(2/3)] but a bigger size (600) too, so the largest feature map gave anchors that cover less of the image.\n","gsizes = [(2**i,2**i) for i in range(5)]\n","gsizes.reverse() #Predictions come in the order of the smallest feature map to the biggest\n","print(ratios, scales, gsizes)\n","\n","#-----------------------------------------------------\n","# Return an array of all anchors for all grid sizes, and all grid cells for each grid size\n","# The shape is (total number of anchors, 4) ie. [[anchor_1], [anchor_2], .....] where \n","# the 4 coordinates are [centre_r, centre_c, height, width]\n","#\n","# We are given a list of grid sizes, a list of ratios and a list of scales\n","# If there are 3 ratios and 3 scales, we will have 9 (ie. 3 * 3) anchors per grid cell\n","#-----------------------------------------------------\n","def create_anchor_boxes (sizes, ratios, scales):\n","  # We have to make combinations of every ratio with every scale\n","  # This statement returns [[ratio_1, scale_1], [ratio_1, scale_2], ....[ratio_m, scale_n]]\n","  combos = np.array(np.meshgrid(ratios, scales)).T.reshape(-1,2)\n","  \n","  # First column is ratio values and second column is scale values\n","  ratio_v, scale_v = combos[:, 0], combos[:, 1]\n","  \n","  # Now use the formula to compute the aspect height and width proportions of the anchor boxes\n","  # eg. if there are 9 anchor boxes per grid cell, there are 9 aspect ratios. These aspects are\n","  # the same across different grid sizes. As grid sizes change, and therefore grid cell sizes,\n","  # the actual anchor height and width also change, even thought the aspect ratio doesn't change.\n","  aspect_h = scale_v * np.sqrt (ratio_v)\n","  aspect_w = scale_v * np.sqrt (1/ratio_v)\n","  \n","  # Loop through every grid size and populate the anchors array\n","  anchors = np.empty(shape=(0, 4))\n","  for h, w in sizes:\n","    # Calculate actual anchor height and width based on grid cell height and width\n","    # and the aspect height and width\n","    _, _, _, _, _, _, cell_h, cell_w = grid_info((h, w))\n","    anchor_h = 4 * aspect_h * cell_h\n","    anchor_w = 4 * aspect_w * cell_w\n","    \n","    # Concatenate the height and width columns. \n","    # eg. if there are 9 anchors, this has shape (9, 2)\n","    anchor_hw = np.stack([anchor_h, anchor_w], axis=1)\n","    \n","    # Get the grid cell centres for this grid size\n","    ctrs = create_grid_centres ((h,w))\n","    \n","    # Cartesian product of every grid cell centre with every (height, width) pair\n","    # eg. if there are 64 grid cells (for grid size (8, 8)) and 9 anchors, it has\n","    # shape (576, 4). The 4 coordinates are (centre_r, centre_c, height, width) \n","    ancs = cartesian(ctrs, anchor_hw)\n","    \n","    # Concatenate the anchors for this grid size to the cumulative array of anchors\n","    anchors = np.vstack((anchors, ancs))\n","  \n","  # Convert to Pytorch tensor\n","  anchors_t = torch.from_numpy(anchors.astype('float32'))\n","  return (anchors_t)\n","\n","#-----------------------------------------------------\n","# Cartesian Product of two 2D arrays\n","# Given the first array of shape (m, 2) and the second array of shape (n, 2) it\n","# returns an array of shape (m * n, 4)\n","#-----------------------------------------------------\n","def cartesian (f2d, s2d):\n","  # Repeat [[a1, a2], [b1, b2],  [c1, c2]] -> [[a1, a2], [a1, a2], [b1, b2], [b1, b2] [c1, c2], [c1, c2]]. \n","  # Repeated 2 times since second array length (ie. 'n') is 2\n","  # Resulting shape is (m * n, 2)\n","  f = np.repeat(f2d, s2d.shape[0], axis=0)\n","\n","  # Tile [[u1, u2], [v1, v2]] -> [[u1, u2], [v1, v2], [u1, u2], [v1, v2], [u1, u2], [v1, v2]]. \n","  # Tile 3 times since first array length (ie. 'm') is 3\n","  # Resulting shape is (m * n, 2)\n","  s = np.tile(s2d, (f2d.shape[0], 1))\n","  \n","  # Concatenate the two arrays column-wise to get [[a1, a2, u1, u2], [a1, a2, v1, v2], ...]\n","  # Resulting shape is (m * n, 4)\n","  c = np.hstack((f, s))\n","\n","  return (c)\n","\n","#foo = create_anchors(gsizes, ratios, scales)\n","#anchors[70:80], foo[70:80], anchors.shape, foo.shape\n","\n","anchors = create_anchor_boxes (gsizes, ratios, scales)\n","show_boxes(anchors[900:909])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qAdhvl9s47Wy","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Convert the output activations predicted by the model's regressor to bounding box coordinates\n","#\n","# For each anchor, there are 4 floats predicted: p_y,p_x,p_h,p_w \n","# If the corresponding anchor has a center (anc_y, anc_x) and height-width as (anc_h, anc_w), then \n","# the predicted bounding box coordinates are computed as:\n","#     center = [p_y * anc_h + anc_y, p_x * anc_w + anc_x]\n","#     height = anc_h * exp(p_h)\n","#     width  = anc_w * exp(p_w)\n","#-----------------------------------------------------\n","def prediction_to_bbox (activations, anchors):\n","  # This part is optional, but we dampen the activations by scaling them as it helps regularization\n","  scale_val = np.array([0.1, 0.1, 0.2, 0.2], dtype=np.float32)\n","  activations_mul = torch.as_tensor(np.multiply (activations, scale_val), device=activations.device)\n","  \n","  # First two columns are (p_y, p_x)\n","  activations_xy = activations_mul[:, :2]\n","  # Last two columns are (p_h, p_w)\n","  activations_hw = activations_mul[:, 2:]\n","  \n","  # First two columns are (anc_y, anc_x)\n","  anchors_xy = anchors[:, :2]\n","  # Last two columns are (anc_h, anc_w)\n","  anchors_hw = anchors[:, 2:]\n","  \n","  # Use the above formula to compute the bounding box centre and height and width\n","  bboxes_ctrs = anchors_hw * activations_xy + anchors_xy\n","  bboxes_hw = anchors_hw * torch.exp(activations_hw)\n","  \n","  # We have just computed two columns for the bounding box centre coordinate and two columns\n","  # for the bounding box height and width. Concatenate them to give us four columns\n","  bboxes = np.hstack((bboxes_ctrs, bboxes_hw))\n","  bboxes_t = torch.as_tensor(bboxes.astype('float32'), device=activations.device)\n","  return (bboxes_t)\n","\n","#-----------------------------------------------------\n","# Inverse of above function. Convert from predicted bounding boxes to output activations\n","#-----------------------------------------------------\n","def bbox_to_prediction (bboxes, anchors):\n","  # First two columns are bounding box centres and last two columns are height and width\n","  bboxes_ctrs = bboxes[:, :2]\n","  bboxes_hw = bboxes[:, 2:]\n","  \n","  # First two columns are (anc_y, anc_x) and last two columns are (anc_h, anc_w)\n","  anchors_xy = anchors[:, :2]\n","  anchors_hw = anchors[:, 2:]\n","  \n","  # Use the above formula to compute the activations\n","  activations_xy = (bboxes_ctrs - anchors_xy) / anchors_hw\n","  activations_hw = np.log(bboxes_hw / anchors_hw)\n","  \n","  # Concatenate the two arrays column-wise\n","  activations_mul = np.hstack((activations_xy, activations_hw))\n","  \n","  # Reverse the scaling that we applied in the above function\n","  scale_val = np.array([0.1, 0.1, 0.2, 0.2], dtype=np.float32)\n","  activations = np.divide (activations_mul, scale_val)\n","  \n","  # Convert to Pytorch tensor on the same device\n","  activations_t = torch.as_tensor(activations.astype('float32'), device=bboxes.device)\n","  \n","  return (activations_t)\n","\n","size=(3,4)\n","anchors_np = create_grid_centres(size)\n","anchors = torch.from_numpy(anchors_np).float()\n","anchors = torch.cat([anchors, torch.tensor([2/size[0],2/size[1]]).expand_as(anchors)], 1)\n","activations = torch.randn(size[0]*size[1], 4) * 0.1\n","\n","bboxes = prediction_to_bbox (activations, anchors)\n","\n","activations_reverse = bbox_to_prediction (bboxes, anchors)\n","print(activations, activations_reverse)\n","\n","show_boxes(bboxes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PiNPRFH5FrNu","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# Convert coordinates for an array of boxes from (ctr, height, width) to \n","# (top left, bottom right)\n","#-----------------------------------------------------\n","def ctr_hw_to_tl_br(boxes):\n","  # First two columns are centre coordinates\n","  centre = boxes[:, :2]\n","  # Last two columns are height and width\n","  hw = boxes[:, 2:]\n","  \n","  # Top Left is (ctr_y - height / 2, ctr_x - width / 2)\n","  top_left = centre - hw/2\n","  # Bottom right is (ctr_y + height / 2, ctr_x + width / 2)\n","  bottom_right = centre + hw/2\n","  \n","  convert_boxes = np.hstack((top_left, bottom_right))\n","  convert_boxes_t = torch.as_tensor(convert_boxes.astype('float32'), device=boxes.device)\n","  return (convert_boxes_t)\n","\n","#-----------------------------------------------------\n","# Inverse of the above function. Converts coordinates for an array of boxes \n","# from (top left, bottom right) to (ctr, height, width)\n","#-----------------------------------------------------\n","def tl_br_to_ctr_hw(boxes):\n","  # First two columns are top-left, last two columns are bottom-right\n","  tl, br = boxes[:, :2], boxes[:, 2:]\n","  \n","  # Height and width are (bottom-right-y - top-left-y, bottom-right-x - top-left-x)\n","  hw = br - tl\n","  # Centre is (top-left-y + height / 2, top-left-x + width / 2)\n","  centre = tl + hw/2\n","  \n","  convert_boxes = np.hstack((centre, hw))\n","  convert_boxes_t = torch.as_tensor(convert_boxes.astype('float32'), device=boxes.device)\n","  return (convert_boxes_t)\n","\n","#-----------------------------------------------------\n","# Calculate the area of intersection between an array of anchor boxes\n","# and an array of ground truth target boxes. Compute intersection\n","# for every combination of anchor and target\n","#-----------------------------------------------------\n","def intersection(anchors_ctrhw, targets_ctrhw):\n","  # First convert the coordinates from (ctr, hw) to (top_left, bottom_right)\n","  anchors_tlbr, targets_tlbr = ctr_hw_to_tl_br(anchors_ctrhw), ctr_hw_to_tl_br(targets_ctrhw)\n","  \n","  # First two columns are Top Left. Last two columns are Bottom Right \n","  anchors_tl, targets_tl = anchors_tlbr[:, :2], targets_tlbr[:, :2]\n","  anchors_br, targets_br = anchors_tlbr[:, 2:], targets_tlbr[:, 2:]\n","  \n","  # We need to find the top-left and bottom-right coordinates of the intersection box\n","  # between every anchor and every target.\n","  #\n","  # To do this, we use broadcasting to replicate each element in anchor as well \n","  # as each element in target, so that there is a pair for every combination. It\n","  # is similar to doing a cartesian product.\n","  #\n","  # anchor_tl is shaped (m, 2) and target_tl is shaped (n, 2). By introducing a None extra\n","  # dimension, they get reshaped to (m, 1, 2) and (1, n, 2) respectively\n","  #\n","  # When used in np.maximum, they both get broadcast to (m, n, 2) each. This gives\n","  # us m x n pairs which represents every combination of anchor and target\n","  # The 3rd dimension corresponds to the (x, y) coordinate\n","  #\n","  # Note that np.maximum() does an element-wise maximum between the anchor and\n","  # target arrays and returns a (m, n, 2) shape\n","  intersection_tl = np.maximum (anchors_tl[:, None, :], targets_tl[None, :, :])\n","  intersection_br = np.minimum (anchors_br[:, None, :], targets_br[None, :, :])\n","  \n","  # Height/Width is Bottom-Right - Top-Left. Set Height/Width to 0 if that number is\n","  # negative which means there is no overlap\n","  # Returns a (m, n, 2) shape with the height/width in the third dimension\n","  intersection_hw = np.maximum(intersection_br - intersection_tl, 0)\n","  \n","  # Multiply the height and width to get the area of the intersection\n","  # Returns (m, n) shape\n","  intersection_area = intersection_hw[..., 0] * intersection_hw[..., 1]\n","  intersection_area = intersection_area.to(device=targets_ctrhw.device)\n","  return (intersection_area)\n","\n","#-----------------------------------------------------\n","# Calculate the IoU between an array of anchor boxes and an array of \n","# ground truth target boxes using \n","# IoU = area of intersection / area of union\n","#-----------------------------------------------------\n","def iou(anchors_ctrhw, targets_ctrhw):\n","  intersection_area = intersection(anchors_ctrhw, targets_ctrhw)\n","  anchors_area = anchors_ctrhw[:, 2] * anchors_ctrhw[:, 3]\n","  targets_area = targets_ctrhw[:, 2] * targets_ctrhw[:, 3]\n","  \n","  # We need to add up the area for every combination of anchor and target. We\n","  # use broadcasting to replicate each element in anchor as well \n","  # as each element in target, so that there is a pair for every combination.\n","  #\n","  # anchors_area shape is (m,) and targets_area shape is (n,). By introducing a None extra\n","  # dimension, they get reshaped to (m, 1) and (1, n) respectively. During broadcasting\n","  # they get reshaped to (m, n) each. intersection_area is already (m, n)\n","  union_area = anchors_area[:, None] + targets_area[None, :] - intersection_area\n","  return (intersection_area / union_area)\n","\n","#-----------------------------------------------------\n","# Match each anchor to targets with the following rules:\n","#   for each anchor we take the maximum overlap possible with any of the targets.\n","#   if that maximum overlap is less than 'bkg_thr', we match the anchor box to background, the classifier's target will be that class\n","#   if the maximum overlap is greater than 'match_thr', we match the anchor box to that ground truth object. The classifier's target will be the category of that target\n","#   if the maximum overlap is between 0.4 and 0.5, we ignore that anchor in our loss computation\n","# \n","# We return the matches, with one value per anchor which is the index of the matching\n","# ground truth target, -1 if it matches 'background', and -2 is 'ignore'\n","#-----------------------------------------------------\n","def match_anchors(anchors_ctrhw, targets_ctrhw, match_thr=0.5, bkg_thr=0.4):\n","  # Initialise the 1d array of matches to a value of 'ignore'\n","  matches = torch.from_numpy(np.full([anchors_ctrhw.shape[0]], -2))\n","  # If target is empty, return\n","  if targets_ctrhw.numel() == 0: return matches\n","\n","  # Get the IoU values\n","  iou_val = iou(anchors_ctrhw, targets_ctrhw)\n","  # Get maximum overlap per anchor (with any target)\n","  max_iou_val = np.maximum.reduce(iou_val, axis=1)\n","  # Get the index value of the target which has the maximum overlap\n","  max_iou_idx = np.argmax(iou_val, axis=1)\n","  \n","  # Match is 'background' if the max overlap is less than the background threshold\n","  matches[max_iou_val < bkg_thr] = -1\n","  \n","  # Match is the index value of the target if the max overlap is greater than the match threshold\n","  matches[max_iou_val > match_thr] = max_iou_idx[max_iou_val > match_thr]\n","\n","  return (matches)\n","\n","boxes_ctrhw = tensor([[5., 5., 2., 2.], [12., 12., 3., 3.]])\n","boxes_tlbr = ctr_hw_to_tl_br(boxes_ctrhw)\n","boxes_reverse = tl_br_to_ctr_hw(boxes_tlbr)\n","print(boxes_tlbr, boxes_reverse)\n","\n","show_boxes(anchors)\n","targets = torch.tensor([[0.,0.,2.,2.], [-0.5,-0.5,1.,1.], [1/3,0.5,0.5,0.5]])\n","show_boxes(targets)\n","iou(anchors, targets)\n","match_anchors(anchors, targets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxXCzWgrp9v8","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# One-hot encode the targets with the convention that the class of index 0 is \n","# the background, which is the absence of any other classes. That is coded by a row of zeros\n","#-----------------------------------------------------\n","def onehot_encode(values, n_classes):\n","  #db.set_trace()\n","  # Initialise array of encoded values to 0s\n","  n_values = values.shape[0]\n","  encoded = torch.as_tensor(np.zeros((n_values, n_classes), dtype=np.float32), device=values.device)\n","  \n","  # Define mask to filter out background values (ie value = 0)\n","  mask = values != 0\n","\n","  # Row index for each element in the values array ie. [0, 1, 2, .... n_values]\n","  row_idx = torch.as_tensor(np.arange(n_values), device=values.device)\n","\n","  # Column index to encode the values\n","  #\n","  # Subtract 1 because one-hot encoded values have to be 1-indexed not 0-indexed\n","  # In other words, a value of 3 is encoded as [0 0 1 ...] not [0 0 0 1 ...]\n","  col_idx = values - 1\n","  \n","  # Encode each value by setting the corresponding column to 1. We filter out the\n","  # background rows so those values will not be set to 1\n","  encoded[row_idx[mask], col_idx[mask]] = 1\n","  return (encoded)\n","\n","onehot_encode(LongTensor([1,2,0,1,3]),3)\n","onehot_encode(tensor([3, 4, 0]), 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nSn3X9EaDxaC","colab_type":"text"},"source":["###  ===================== Retinanet model =====================\n","![alt text](https://github.com/fastai/course-v3/raw/e08c4b712f459f77247df2f6fb04465579cd6653/nbs/dl2/images/retinanet.png)"]},{"cell_type":"code","metadata":{"id":"Grq8-QJdTxVp","colab_type":"code","colab":{}},"source":["from fastai.vision import *\n","from fastai.vision.models.unet import _get_sfs_idxs, model_sizes, hook_outputs\n","import pdb\n","import IPython.core.debugger as db\n","\n","#-----------------------------------------------------\n","# Subclass to handle the Upsample, the Lateral Connection and the Merging of the two\n","#-----------------------------------------------------\n","class LateralUpsampleMerge(nn.Module):\n","    def __init__(self, enc_lateral_channels, n_channels, hook):\n","        super().__init__()\n","\n","        # (I think) A hook is a Fastai callback which allows you to tap into the internals\n","        # of the encoder and store the output of a layer for use later\n","        self.hook = hook\n","\n","        # Lateral Conv layer, with 1x1 kernel and stride=1\n","        self.lateral = conv2d(enc_lateral_channels, n_channels, ks=1, bias=True)\n","    \n","    def forward(self, x):\n","        # Use the hook to access the output of the Cx layer and apply the lateral connection to it\n","        lateral = self.lateral(self.hook.stored)\n","        \n","        # Use nearest neighbour upsampling for the top-down pathway\n","        upsample = F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')\n","\n","        # Merge the upsample and the lateral connection with a simple addition\n","        return (lateral + upsample)\n","\n","#-----------------------------------------------------\n","# Create the RetinaNet model\n","#\n","# It is based on a pre-trained 'encoder' CNN which acts as the backbone. It adds\n","# several Conv layers on top of the existing Conv layers of the backbone.\n","# \n","# The backbone's built-in layers C1 through C5 is the 'bottom-up pathway'.\n","# We create layers P3 through P7 as the 'top-down pathway'\n","#\n","# There is a lateral connection from some Cx layers to the corresponding Px layers. This lateral\n","# connection is 'merged' with the top-down connections.\n","# \n","# Some Px layers then have a 'smoothing' Conv layer. Finally, after that, there is a 'detector head'\n","# which consists of two similar sub-networks, one for classifier and one for the bounding box regressor\n","#\n","# An nn.Module has two required methods - init() and forward()\n","# In the init() method, we only define all the layers. The layers stay standalone and are not actually 'wired' to\n","# one another here. That happens in the forward() method, when we actually pass in input data and do the \n","# computation as data flows from one layer to another, which implicitly wires them up as per the computation.\n","#-----------------------------------------------------\n","class RetinaNet(nn.Module):\n","  def __init__(self, encoder:nn.Module, n_classes, n_anchors=9, final_bias=0.):\n","    super().__init__()\n","\n","    # All layers that we create will have the same depth of 256\n","    self.n_channels = 256\n","\n","    self.n_classes = n_classes\n","    self.encoder = encoder\n","\n","    # BOTTOM-UP PATHWAY\n","    # The encoder has many more layers internally, but we designate five of them\n","    # as C1 through C5. These are the layers which downsample from the previous layer by a factor of 2\n","    # resulting in a size that is half the previous. For example, if the input image size is (256x256), the layers are:\n","    # C1 - 128 x 128 x 64\n","    # C2 - 64 x 64 x 256\n","    # C3 - 32 x 32 x 512\n","    # C4 - 16 x 16 x 1024\n","    # C5 - 8 x 8 x 2048\n","    #\n","    # Note that if the image size is (128x128) the sizes of C1 through C5 will be half of the numbers shown. \n","    # eg. C1 will be 64 x 64 x 64 and so on. The depth will not change.\n","\n","    # Get the depth of layers C3, C4 and C5 from the encoder\n","    c3_channels, c4_channels, c5_channels  = self._encoder_layers()\n","    \n","    # The depth of all other layers is the same\n","    p6_channels = self.n_channels\n","    p7_channels = self.n_channels\n","    \n","    # TOP DOWN PATHWAY\n","    # The layers are:\n","    # P7 - 2 x 2 x 256   - ReLU activation then 3x3 with stride=2 on C6.\n","    # P6 - 4 x 4 x 256   - 3x3 with stride=2 on C5.\n","    # P5 - 8 x 8 x 256   - 1x1 with stride=1 on C5.\n","    # P4 - 16 x 16 x 256 - up-sample from P5 merged with lateral connection from C4\n","    # P3 - 32 x 32 x 256 - up-sample from P4 merged with lateral connection from C3\n"," \n","    self.p5 = conv2d(c5_channels, self.n_channels, ks=1, bias=True)\n","    self.p6 = conv2d(c5_channels, p6_channels, stride=2, bias=True)\n","    self.p7 = nn.Sequential(nn.ReLU(), conv2d(p6_channels, p7_channels, stride=2, bias=True))\n","    self.p4 = LateralUpsampleMerge(c4_channels, self.n_channels, self.hooks[3])\n","    self.p3 = LateralUpsampleMerge(c3_channels, self.n_channels, self.hooks[2])\n","\n","    # SMOOTHERS - 3x3 with stride=1 on P5, P4 and P3. So sizes remain the same. All depths are 256.\n","    self.smoothers = nn.ModuleList([conv2d(self.n_channels, self.n_channels, bias=True) for _ in range(3)])\n","\n","    # DETECTOR HEAD - Consists of two similar sub-networks viz. Classifier and Bounding Box Regressor\n","    # Each sub-net consists of:\n","    #    4 Conv layers (with 3x3, stride=1, depth=256) followed by \n","    #    Predictor Conv layer (with 3x3, stride=1, depth=KA for Classifier and 4A for Regressor\n","    # where K = number of classes and A = number of anchors\n","    #\n","    # The Detector Head is connected in parallel, with each of P3 through P7 ie.\n","    # P7 -> Detector\n","    # P6 -> Detector\n","    # P5 -> Smoother -> Detector\n","    # P4 -> Smoother -> Detector\n","    # P3 -> Smoother -> Detector\n","    #\n","    # The final step of the Detector is a Sigmoid activation. However we don't apply that\n","    # here in the model, but inside the loss function when we take a Sigmoid of the model's output\n","    \n","    self.classifier = self._head_detector(self.n_classes * n_anchors, final_bias=final_bias)\n","    self.bbox = self._head_detector(4 * n_anchors)\n","\n","  #-----------------------------------------------------\n","  # Perform the model's calculations layer by layer, given the input data. Here is where we actually wire up the output\n","  # of one layer to the input of the next etc.\n","  #-----------------------------------------------------\n","  def forward(self, x):\n","    # Get the output after applying all the encoder layers to the input image, ending up with the output of the final C5 layer.\n","    c5 = self.encoder(x)\n","    \n","    # Outputs of each of the P3-P7 layers. We have only 5 layers, but we allocate a list of size 8, just so that we could index\n","    # the list using indices p[3] - p[7] for readability instead of allocating a list of size 5 and using indices p[0] - p[4]\n","    p = [None] * 8\n","    p[5] = self.p5(c5.clone())    # Compute P5 from C5\n","    p[6] = self.p6(c5)            # Compute P6 from C5\n","    p[7] = self.p7(p[6])          # Compute P7 from C6\n","    p[4] = self.p4(p[5])          # Compute P4 from P5 on the top-down path\n","    p[3] = self.p3(p[4])          # Compute P3 from P4 on the top-down path\n","\n","    # Apply smoother layers to P3-P5\n","    for i, smoother in enumerate(self.smoothers):\n","      p[3 + i] = smoother(p[3 + i])\n","    \n","    # Apply the Detector head on each of P3-P7, and then concatenate all the outputs.\n","    # In Pytorch, Conv2d() shape is (batch, depth channel, height, width), which we permute to (batch, height, width, depth)\n","    #    For the classifier, the depth is KA which is then reshaped to (batch, height * width * A, K)\n","    #    For the bbox regressor, the depth is 4A which is then reshaped to (batch, height * width * A, 4)\n","    # Now the outputs from P3-P7 can be concatenated together.\n","    #\n","    # In other words, every grid cell (ie. height x width) and every anchor for each grid cell (ie. A) gets unrolled and stacked linearly one below the other in the 2nd dimension\n","    \n","    class_pred = torch.cat([self.classifier(act).permute(0,2,3,1).contiguous().view(act.size(0), -1, self.n_classes) for act in p[3:8]],1)\n","    bbox_pred = torch.cat([self.bbox(act).permute(0,2,3,1).contiguous().view(act.size(0), -1, 4) for act in p[3:8]],1)\n","    \n","    # Get a list of all the height and width of P3-P7 as [[32, 32], [16, 16], ...]\n","    p_sizes = [[layer.size(2), layer.size(3)] for layer in p[3:8]]\n","\n","    return([class_pred, bbox_pred, p_sizes])\n","\n","  #-----------------------------------------------------\n","  # Get information about some of the internal layers of the encoder, which we're calling C1-C5\n","  #-----------------------------------------------------\n","  def _encoder_layers(self):\n","    # Assume image size\n","    imsize = (256,256)\n","    \n","    # Use a Fastai utility function from the Unet model, to get the sizes of all the main Conv layers of the encoder\n","    # These layers are: [1, 64, 128, 128], [1, 64, 128, 128], [1, 64, 128, 128], [1, 64, 64, 64], [1, 256, 64, 64]), \n","    #                      [1, 512, 32, 32], [1, 1024, 16, 16], [1, 2048, 8, 8]\n","    enc_sizes = model_sizes(self.encoder, size=imsize)\n","    \n","    # Use another Fastai Unet function to get the indices of the layers in the above list, where the size changes ie. [2, 4, 5, 6]\n","    # These are the layers which we are designating as C1-C4, and then the final layer ie. index #7, is designated as C5\n","    enc_layer_idxs = _get_sfs_idxs(enc_sizes)\n","    \n","    # Use another Fastai Unet function to get the 'hooks' for each of the C1-C4 layers\n","    self.hooks = hook_outputs([self.encoder[i] for i in enc_layer_idxs])\n","    \n","    # Append -1 to get the index of the last layer C5. Then get the depth of all layers C1-C5\n","    enc_layer_idxs.append(-1)\n","    enc_layer_channels = [enc_sizes[i][1] for i in enc_layer_idxs]\n","    \n","    # Return the depths of C3-C5 (we don't use C1 and C2 for anything)\n","    return (enc_layer_channels[2:5])\n","\n","  #-----------------------------------------------------\n","  # Build the Detector Head\n","  #-----------------------------------------------------\n","  def _head_detector(self, n_predictor_channels, final_bias=0., n_conv=4):\n","    # 4 Conv layers with 1x1 kernels and stride=1\n","    layers = [conv_layer(self.n_channels, self.n_channels, bias=True, norm_type=None) for _ in range(n_conv)]\n","    \n","    # Final predictor layer, also a 1x1 Conv layer with stride=1\n","    predictor_layer = conv2d(self.n_channels, n_predictor_channels, bias=True)\n","    \n","    # Initialise the Weigths and Bias of the predictor layer.\n","    predictor_layer.bias.data.zero_().add_(final_bias)\n","    predictor_layer.weight.data.fill_(0)\n","    \n","    # Build a Sequential module from all five layers\n","    layers += [predictor_layer]\n","    return nn.Sequential(*layers)\n","\n","  def __del__(self):\n","    if hasattr(self, \"sfs\"): self.sfs.remove()\n","\n","# Build the Resnet50 encoder, and then the RetinaNet model\n","encoder = create_body(models.resnet50, cut=-2)\n","# This is temporary. Initialise the random number seed so that we can get reproducible results between my version of\n","# the Retinanet model and the Fastai version\n","torch.manual_seed(0)\n","# Why final_bias=-4? That's because we want the network to predict background easily at the beginning (since it's the \n","# most common class). At first the final convolution of the classifier is initialized with weights=0 and that bias, so \n","# it will return -4 for everyone. If go though a sigmoid it'll give a corresponding probability of 0.02 roughly.\n","model = RetinaNet(encoder, data.c, final_bias=-4)\n","test_img = torch.randn(1, 3, 256, 256)\n","out = model(test_img)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rHnxZ4bGD1hJ","colab_type":"code","colab":{}},"source":["class LateralUpsampleMerge_f(nn.Module):\n","    \"Merge the features coming from the downsample path (in `hook`) with the upsample path.\"\n","    def __init__(self, ch, ch_lat, hook):\n","        super().__init__()\n","        self.hook = hook\n","        self.conv_lat = conv2d(ch_lat, ch, ks=1, bias=True)\n","    \n","    def forward(self, x):\n","        #db.set_trace()\n","        a = self.conv_lat(self.hook.stored)\n","        b = F.interpolate(x, self.hook.stored.shape[-2:], mode='nearest')\n","        c = a + b\n","        return c\n","      \n","class RetinaNet_f(nn.Module):\n","    \"Implements RetinaNet from https://arxiv.org/abs/1708.02002\"\n","    def __init__(self, encoder:nn.Module, n_classes, final_bias=0., chs=256, n_anchors=9, flatten=True):\n","        super().__init__()\n","        self.n_classes,self.flatten = n_classes,flatten\n","        imsize = (256,256)\n","        sfs_szs = model_sizes(encoder, size=imsize)\n","        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n","        self.sfs = hook_outputs([encoder[i] for i in sfs_idxs])\n","        self.encoder = encoder\n","        self.c5top5 = conv2d(sfs_szs[-1][1], chs, ks=1, bias=True)\n","        self.c5top6 = conv2d(sfs_szs[-1][1], chs, stride=2, bias=True)\n","        self.p6top7 = nn.Sequential(nn.ReLU(), conv2d(chs, chs, stride=2, bias=True))\n","        print(sfs_idxs, sfs_idxs[0:2], sfs_szs[2], sfs_szs[4], sfs_szs[5], sfs_szs[6])\n","        self.merges = nn.ModuleList([LateralUpsampleMerge_f(chs, sfs_szs[idx][1], hook) \n","                                     for idx,hook in zip(sfs_idxs[0:2], self.sfs[0:2])])\n","        self.smoothers = nn.ModuleList([conv2d(chs, chs, 3, bias=True) for _ in range(3)])\n","        self.classifier = self._head_subnet(n_classes, n_anchors, final_bias, chs=chs)\n","        self.box_regressor = self._head_subnet(4, n_anchors, 0., chs=chs)\n","        \n","    def _head_subnet(self, n_classes, n_anchors, final_bias=0., n_conv=4, chs=256):\n","        \"Helper function to create one of the subnet for regression/classification.\"\n","        layers = [conv_layer(chs, chs, bias=True, norm_type=None) for _ in range(n_conv)]\n","        layers += [conv2d(chs, n_classes * n_anchors, bias=True)]\n","        layers[-1].bias.data.zero_().add_(final_bias)\n","        layers[-1].weight.data.fill_(0)\n","        return nn.Sequential(*layers)\n","    \n","    def _apply_transpose(self, func, p_states, n_classes):\n","        #Final result of the classifier/regressor is bs * (k * n_anchors) * h * w\n","        #We make it bs * h * w * n_anchors * k then flatten in bs * -1 * k so we can contenate\n","        #all the results in bs * anchors * k (the non flatten version is there for debugging only)\n","        if not self.flatten: \n","            sizes = [[p.size(0), p.size(2), p.size(3)] for p in p_states]\n","            return [func(p).permute(0,2,3,1).view(*sz,-1,n_classes) for p,sz in zip(p_states,sizes)]\n","        else:\n","            return torch.cat([func(p).permute(0,2,3,1).contiguous().view(p.size(0),-1,n_classes) for p in p_states],1)\n","    \n","    def forward(self, x):\n","        c5 = self.encoder(x)\n","        p_states = [self.c5top5(c5.clone()), self.c5top6(c5)]\n","        p_states.append(self.p6top7(p_states[-1]))\n","        for merge in self.merges: p_states = [merge(p_states[0])] + p_states\n","        for i, smooth in enumerate(self.smoothers[:3]):\n","            p_states[i] = smooth(p_states[i])\n","        return [self._apply_transpose(self.classifier, p_states, self.n_classes), \n","                self._apply_transpose(self.box_regressor, p_states, 4),\n","                [[p.size(2), p.size(3)] for p in p_states]]\n","    \n","    def __del__(self):\n","        if hasattr(self, \"sfs\"): self.sfs.remove()\n","          \n","torch.manual_seed(0)\n","model_f = RetinaNet_f(encoder, 10, final_bias=-4)\n","out_f = model_f(test_img)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6uFqIzDnFFaG","colab_type":"text"},"source":["**===================== Loss Function =====================**"]},{"cell_type":"code","metadata":{"id":"THAMOIGy1jAQ","colab_type":"code","colab":{}},"source":["#-----------------------------------------------------\n","# The RetinaNet model spits out an absurdly high number of predictions: for the features P3 to P7 with an image size of 256, we \n","# have 32*32 + 16*16 + 8*8 + 4*4 +2*2 locations possible in one of the five feature maps, which gives 1,364 possible detections, \n","# multiplied by the number of anchors we choose to attribute to each location (9 below), which makes 12,276 possible hits.\n","#\n","# The model itself isn't actually aware of things like grid cells, anchor boxes and coordinates. It just generates a lot of numbers as its output\n","# predictions. We choose to interpret those numbers as predicted bounding boxes per anchor box and so on, and all of that intelligence\n","# is in the Loss Function.\n","#\n","# Many of the 12276 predicted bounding boxes aren't going to correspond to any object in the picture, and we need to somehow match \n","# all those predictions to either nothing or a given bounding box in the picture.\n","#\n","# The Loss Function has two parts, one for the classifier and one for the regressor. For the regression, we will use the \n","# L1 (potentially smoothed) loss between the predicted activations for an anchor that matches a given object (we ignore \n","# the no match or matches to background) and the corresponding bounding box (after going through bbox_to_prediction).\n","# For the classification, we use the focal loss, which is a variant of the binary cross entropy used when we have a lot \n","# imbalance between the classes to predict (here we will very often have to predict 'background').\n","#-----------------------------------------------------\n","class RetinaNetFocalLoss(nn.Module):\n","    \n","    def __init__(self, gamma:float=2., alpha:float=0.25, scales:Collection[float]=None, \n","                 ratios:Collection[float]=None, reg_loss:LossFunction=F.smooth_l1_loss):\n","      super().__init__()\n","      self.gamma, self.alpha, self.reg_loss = gamma, alpha, reg_loss\n","      self.scales, self.ratios = scales, ratios\n","      self.pad_idx = 0\n","\n","    #-----------------------------------------------------\n","    # The grid sizes are different depending on the size of the input image. So figure out whether\n","    # the grid sizes for the current image are different than what the Loss Function was initialised with. \n","    # If the sizes are different, we have to recreate the anchors.\n","    #\n","    # We return True if the passed in grid sizes are different than what we had saved previously\n","    #-----------------------------------------------------\n","    def _change_anchors(self, sizes:Sizes) -> bool:\n","        if not hasattr(self, 'sizes'): return True\n","\n","        # Check if any of the grid sizes are different than what we had saved previously\n","        for sz1, sz2 in zip(self.sizes, sizes):\n","            if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True\n","\n","        return False\n","\n","    #-----------------------------------------------------\n","    # Create the anchor boxes\n","    #-----------------------------------------------------\n","    def _create_anchors(self, sizes:Sizes, device:torch.device):\n","      self.sizes = sizes\n","      self.anchors = create_anchor_boxes(self.sizes, self.ratios, self.scales).to(device)\n","\n","    #-----------------------------------------------------\n","    # This is meant to reverse the bb_pad_collate function used when creating the\n","    # databunch. Don't fully understand the pad_idx logic\n","    #-----------------------------------------------------\n","    def _unpad(self, bbox_tgt, class_tgt):\n","        i = torch.min(torch.nonzero(class_tgt-self.pad_idx))\n","        return tl_br_to_ctr_hw(bbox_tgt[i:]), class_tgt[i:]-1+self.pad_idx\n","\n","    #-----------------------------------------------------\n","    # Calculate the bounding box loss\n","    #-----------------------------------------------------\n","    def _bbox_loss(self, bp, bt, matches):\n","      # Get all anchor boxes which match a ground truth object (ie. we filter out matches with background)\n","      bbox_mask = matches >= 0\n","      \n","      # Count of anchor boxes which match some ground truth object\n","      # Note that multiple anchor boxes might match the same ground truth object\n","      n_anchors_matching_target = bbox_mask.sum()\n","      \n","      if n_anchors_matching_target != 0:\n","        # Get the corresponding bounding box predictions \n","        # bp reduces in length from (number of anchor boxes, 4) -> (number of matching anchor boxes, 4)\n","        bp = bp[bbox_mask]\n","        \n","        # Get the corresponding bounding box ground truth targets\n","        # This changes the length of bt from (number of ground truth objects, 4) -> (number of matching anchor boxes, 4)\n","        # Note that matches contains the index of the object in the target array ie. which-th ground truth object\n","        bt = bt[matches[bbox_mask]]\n","        \n","        # Convert the target bounding box coordinates to activations\n","        bt = bbox_to_prediction (bt, self.anchors[bbox_mask])\n","        \n","        # Calculate loss using L1 loss\n","        bb_loss = self.reg_loss(bp, bt)\n","      else:\n","        # Zero loss if no anchor boxes matched any ground truth\n","        bb_loss = 0\n","      \n","      return(bb_loss, n_anchors_matching_target)\n","\n","    #-----------------------------------------------------\n","    # Calculate the classification loss\n","    #-----------------------------------------------------\n","    def _class_loss(self, cp, ct, matches):\n","      # Get the mask of all anchor boxes which match either a ground truth object or background\n","      class_mask = matches >= 0\n","      \n","      # We treat background as Class 0. So now the real classes which went from 0 to n_classes-1\n","      # will now go from 1 to n_classes.\n","      ct = ct + 1\n","      \n","      # We also prepend one element to the beginning of the class target array. This element\n","      # corresponds to 'background' and will have a value of 0\n","      # ct is now [0, class of first ground truth object, class of second ground truth object, ...]\n","      # ct has shape (number of ground truth objects + 1)\n","      ct = torch.cat([ct.new_zeros(1), ct])\n","      \n","      # For the matching anchor boxes in mask, get the corresponding class targets\n","      # This changes the length of ct from (number of ground truth objects + 1) -> (number of matching anchor boxes)\n","      ct = ct[matches[class_mask]]\n","      \n","      # One-hot encode each value in ct. et has shape (number of matching anchor boxes, number of classes)\n","      et = onehot_encode (ct, cp.shape[1])\n","      \n","      # For the matching anchor boxes in mask, get the corresponding class predictions\n","      # cp reduces in length from (number of anchor boxes) -> (number of matching anchor boxes)\n","      cp = cp[class_mask]\n","      \n","      # Calculate loss using focal loss\n","      class_loss = self._focal_loss(cp, et)\n","      return(class_loss)\n","\n","    def _focal_loss(self, cp, et):\n","      sig_cp = torch.sigmoid(cp)\n","\n","      y_1 = self.alpha * et * ((1 - sig_cp) ** self.gamma) * torch.log(sig_cp)\n","      y_0 = (1 - self.alpha) * (1 - et) * (sig_cp ** self.gamma) * torch.log(1 - sig_cp)\n","      f_loss = torch.sum (- y_1 - y_0) \n","\n","      f_loss_fizyr_one, f_loss_fizyr_two = self._focal_loss_fizyr(sig_cp, et)\n","      f_loss_kdp_t = self._focal_loss_kdp_t(sig_cp, et)\n","      f_loss_kdbasic_log = self._focal_loss_kdbasic_log(sig_cp, et)\n","      #print(f_loss, f_loss_fizyr_one, f_loss_fizyr_two, f_loss_kdp_t, f_loss_kdbasic_log)\n","\n","      return(f_loss)\n","\n","    #-----------------------------------------------------\n","    # This is the main method which drives the loss calculation. We are given the final\n","    # outputs from the model along with the ground truth classifier and bounding box targets.\n","    #\n","    # We match each anchor box to a ground truth bounding box, or to the background. If it matches\n","    # a bounding box, then the anchor box is also associated with the corresponding class of that\n","    # bounding box.\n","    #\n","    # We then calculate the bounding box loss and the classification loss. For the classification\n","    # loss we use a focal loss calculation which is an improved version of binary cross entropy loss\n","    #-----------------------------------------------------\n","    def forward(self, output, bbox_tgts, class_tgts):\n","        # Bounding Box Target shape is (batch, number of ground truth objects, 4)\n","        # Class Target shape is (batch, number of ground truth objects)\n","        \n","        # Get the output class predictions and bounding box predictions\n","        # Class Pred shape is (batch, number of anchor boxes, number of classes)\n","        # Bounding Box Pred shape is (batch, number of anchor boxes, 4)\n","        class_pred, bbox_pred, p_sizes = output\n","        \n","        # Create anchor boxes for the grid sizes in p_sizes\n","        if self._change_anchors(p_sizes): self._create_anchors(p_sizes, class_pred.device)\n","          \n","        # Initialise the total loss\n","        total_loss = 0.\n","        \n","        # Go through the prediction and target values for each image\n","        for (cp, bp, ct, bt) in zip(class_pred, bbox_pred, class_tgts, bbox_tgts):\n","          # Shapes are:\n","          # cp - (number of anchor boxes, number of classes). Gives probability for each class\n","          # bp - (number of anchor boxes, 4)\n","          # ct - (number of ground truth objects)\n","          # bt - (number of ground truth objects, 4)\n","          \n","          # Reverse the padding that was applied when the Databunch was created\n","          bt, ct = self._unpad(bt,ct)\n","          \n","          # Match the anchors with ground truth bounding boxes\n","          # Matches has shape (number of anchor boxes). Gives the matching class, or background (or Ignore)\n","          matches = match_anchors(self.anchors, bt)\n","          \n","          # Calculate the bounding box loss\n","          bb_loss, n_anchors_matching_target = self._bbox_loss(bp, bt, matches)\n","         \n","          # In 'matches', anchor boxes which match 'background' have a value of -1. And anchor boxes which match\n","          # an object have a value of the index of the object in the target array ie. which-th ground truth \n","          # object in class_tgts and bbox_tgts. We add 1 to each value, so that now 'background' matches have a \n","          # value of 0. The class_tgts now has to be extended, which happens inside _class_loss()\n","          matches = matches + 1\n","          \n","          # Calculate the classification loss\n","          class_loss = self._class_loss(cp, ct, matches) / torch.clamp(n_anchors_matching_target, min=1.)\n","          \n","          # Loss for this image is Bounding Box Loss + Classification Loss\n","          image_loss = bb_loss + class_loss\n","          \n","          # Total loss (for all images) is the sum of loss for each image\n","          total_loss += image_loss\n","\n","        # Finally we get the average loss by dividing the summed total loss by the number of images\n","        n_images = class_tgts.shape[0]\n","        return(total_loss / n_images)\n","\n","    def _focal_loss_fizyr (self, sig_cp, et):\n","      alpha_k = torch.ones_like(et) * self.alpha\n","      alpha_k = torch.where(et > 0., alpha_k, 1 - alpha_k)\n","      p_k = torch.where(et > 0., 1 - sig_cp, sig_cp)\n","      weight_k = alpha_k * (p_k ** self.gamma)\n","      weight_k = weight_k.detach()\n","\n","      f_loss_kw = torch.sum (weight_k * F.binary_cross_entropy(sig_cp, et, reduction='none'))\n","      f_loss_kred = F.binary_cross_entropy(sig_cp, et, weight_k, reduction='sum')\n","      return (f_loss_kw, f_loss_kred)\n","    \n","    def _focal_loss_kdp_t (self, sig_cp, et):\n","      alpha_t = self.alpha * et + (1 - self.alpha) * (1 - et)\n","      p_t = sig_cp * et + (1 - sig_cp) * (1 - et)\n","      weight = alpha_t * ((1 - p_t) ** self.gamma)\n","      weight = weight.detach()\n","      #f_loss_bce = F.binary_cross_entropy(p_t, et, weight)\n","      f_loss_bce_red = F.binary_cross_entropy(sig_cp, et, weight, reduction='sum')\n","      return (f_loss_bce_red)\n","\n","    def _focal_loss_kdbasic_log (self, sig_cp, et):\n","      alpha_t = self.alpha * et + (1 - self.alpha) * (1 - et)\n","      p_t = sig_cp * et + (1 - sig_cp) * (1 - et)\n","      weight = alpha_t * ((1 - p_t) ** self.gamma)\n","      weight = weight.detach()\n","\n","      f_loss_log = torch.sum (-weight * torch.log(p_t))\n","      return (f_loss_log)\n","\n","    def _focal_loss_fastai (self, cp, sig_cp, et):\n","      weight_f = et * (1-sig_cp) + (1-et) * sig_cp\n","      weight_f = weight_f.detach()\n","      alpha_f = (1-et) * self.alpha + et * (1-self.alpha)\n","      weight_f.pow_(self.gamma).mul_(alpha_f)\n","      clas_loss = F.binary_cross_entropy_with_logits(cp, et, weight_f, reduction='sum')\n","      return (clas_loss)\n","\n","ratios = [1/2,1,2]\n","scales = [1,2**(-1/3), 2**(-2/3)] \n","bbox_targ = torch.randn(1, 5, 4)\n","clas_tgt = torch.randint(1, data.c + 1, (1, 5))\n","\n","crit = RetinaNetFocalLoss(scales=scales, ratios=ratios)\n","loss = crit(out, bbox_targ, clas_tgt)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PtOK7UbzGcnq","colab_type":"code","colab":{}},"source":["def tlbr2cthw(boxes):\n","    \"Convert top/left bottom/right format `boxes` to center/size corners.\"\n","    center = (boxes[:,:2] + boxes[:,2:])/2\n","    sizes = boxes[:,2:] - boxes[:,:2]\n","    return torch.cat([center, sizes], 1)\n","  \n","def cthw2tlbr(boxes):\n","    \"Convert center/size format `boxes` to top/left bottom/right corners.\"\n","    top_left = boxes[:,:2] - boxes[:,2:]/2\n","    bot_right = boxes[:,:2] + boxes[:,2:]/2\n","    return torch.cat([top_left, bot_right], 1)\n","\n","def bbox_to_activ(bboxes, anchors, flatten=True):\n","    \"Return the target of the model on `anchors` for the `bboxes`.\"\n","    if flatten:\n","        t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] \n","        t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) \n","        return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]]))\n","    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n","    return res\n","\n","def encode_class(idxs, n_classes):\n","    target = idxs.new_zeros(len(idxs), n_classes).float()\n","    mask = idxs != 0\n","    i1s = LongTensor(list(range(len(idxs))))\n","    target[i1s[mask],idxs[mask]-1] = 1\n","    return target\n","\n","def create_grid(size):\n","    \"Create a grid of a given `size`.\"\n","    H, W = size if is_tuple(size) else (size,size)\n","    grid = FloatTensor(H, W, 2)\n","    linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W > 1 else tensor([0.])\n","    grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0])\n","    linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H > 1 else tensor([0.])\n","    grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1])\n","    return grid.view(-1,2)\n","\n","def create_anchors(sizes, ratios, scales, flatten=True):\n","    \"Create anchor of `sizes`, `ratios` and `scales`.\"\n","    aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios]\n","    aspects = torch.tensor(aspects).view(-1,2)\n","    anchors = []\n","    for h,w in sizes:\n","        #4 here to have the anchors overlap.\n","        sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0)\n","        base_grid = create_grid((h,w)).unsqueeze(1)\n","        n,a = base_grid.size(0),aspects.size(0)\n","        ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2)\n","        anchors.append(ancs.view(h,w,a,4))\n","    return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors\n","\n","def intersection_f(anchors, targets):\n","    \"Compute the sizes of the intersections of `anchors` by `targets`.\"\n","    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)\n","    a, t = ancs.size(0), tgts.size(0)\n","    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)\n","    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])\n","    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])\n","    sizes = torch.clamp(bot_right_i - top_left_i, min=0) \n","    return sizes[...,0] * sizes[...,1]\n","\n","def IoU_values(anchors, targets):\n","    \"Compute the IoU values of `anchors` by `targets`.\"\n","    inter = intersection_f(anchors, targets)\n","    anc_sz, tgt_sz = anchors[:,2] * anchors[:,3], targets[:,2] * targets[:,3]\n","    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter\n","    return inter/(union+1e-8)\n","\n","def match_anchors_f(anchors, targets, match_thr=0.5, bkg_thr=0.4):\n","    \"Match `anchors` to targets. -1 is match to background, -2 is ignore.\"\n","    matches = anchors.new(anchors.size(0)).zero_().long() - 2\n","    if targets.numel() == 0: return matches\n","    ious = IoU_values(anchors, targets)\n","    vals,idxs = torch.max(ious,1)\n","    matches[vals < bkg_thr] = -1\n","    matches[vals > match_thr] = idxs[vals > match_thr]\n","    #Overwrite matches with each target getting the anchor that has the max IoU.\n","    #vals,idxs = torch.max(ious,0)\n","    #If idxs contains repetition, this doesn't bug and only the last is considered.\n","    #matches[idxs] = targets.new_tensor(list(range(targets.size(0)))).long()\n","    return matches\n","\n","class RetinaNetFocalLoss_f(nn.Module):\n","    \n","    def __init__(self, gamma:float=2., alpha:float=0.25,  pad_idx:int=0, scales:Collection[float]=None, \n","                 ratios:Collection[float]=None, reg_loss:LossFunction=F.smooth_l1_loss):\n","        super().__init__()\n","        self.gamma,self.alpha,self.pad_idx,self.reg_loss = gamma,alpha,pad_idx,reg_loss\n","        self.scales = ifnone(scales, [1,2**(-1/3), 2**(-2/3)])\n","        self.ratios = ifnone(ratios, [1/2,1,2])\n","        \n","    def _change_anchors(self, sizes:Sizes) -> bool:\n","        if not hasattr(self, 'sizes'): return True\n","        for sz1, sz2 in zip(self.sizes, sizes):\n","            if sz1[0] != sz2[0] or sz1[1] != sz2[1]: return True\n","        return False\n","    \n","    def _create_anchors(self, sizes:Sizes, device:torch.device):\n","        self.sizes = sizes\n","        anchors = create_anchors(sizes, self.ratios, self.scales)\n","        self.anchors = anchors.to(device)\n","    \n","    def _unpad(self, bbox_tgt, clas_tgt):\n","        #db.set_trace()\n","        i = torch.min(torch.nonzero(clas_tgt-self.pad_idx))\n","        return tlbr2cthw(bbox_tgt[i:]), clas_tgt[i:]-1+self.pad_idx\n","    \n","    def _focal_loss(self, clas_pred, clas_tgt):\n","        #db.set_trace()\n","        encoded_tgt = encode_class(clas_tgt, clas_pred.size(1))\n","        ps = torch.sigmoid(clas_pred.detach())\n","        weights = encoded_tgt * (1-ps) + (1-encoded_tgt) * ps\n","        alphas = (1-encoded_tgt) * self.alpha + encoded_tgt * (1-self.alpha)\n","        weights.pow_(self.gamma).mul_(alphas)\n","        clas_loss = F.binary_cross_entropy_with_logits(clas_pred, encoded_tgt, weights, reduction='sum')\n","        print(clas_loss)\n","        return clas_loss\n","        \n","    def _one_loss(self, clas_pred, bbox_pred, clas_tgt, bbox_tgt):\n","        db.set_trace()\n","        bbox_tgt, clas_tgt = self._unpad(bbox_tgt, clas_tgt)\n","        matches = match_anchors_f(self.anchors, bbox_tgt)\n","        bbox_mask = matches>=0\n","        if bbox_mask.sum() != 0:\n","            bbox_pred = bbox_pred[bbox_mask]\n","            bbox_tgt = bbox_tgt[matches[bbox_mask]]\n","            b = bbox_to_activ(bbox_tgt, self.anchors[bbox_mask]).float()\n","            #db.set_trace()\n","            bb_loss = self.reg_loss(bbox_pred, b)\n","        else: bb_loss = 0.\n","        matches.add_(1)\n","        clas_tgt = clas_tgt + 1\n","        clas_mask = matches>=0\n","        clas_pred = clas_pred[clas_mask]\n","        ddd = clas_tgt.new_zeros(1).long()\n","        #db.set_trace()\n","        clas_tgt = torch.cat([ddd, clas_tgt])\n","        clas_tgt = clas_tgt[matches[clas_mask]]\n","        return bb_loss + self._focal_loss(clas_pred, clas_tgt)/torch.clamp(bbox_mask.sum(), min=1.)\n","    \n","    def forward(self, output, bbox_tgts, clas_tgts):\n","        #db.set_trace()\n","        clas_preds, bbox_preds, sizes = output\n","        if self._change_anchors(sizes): self._create_anchors(sizes, clas_preds.device)\n","        n_classes = clas_preds.size(2)\n","        return sum([self._one_loss(cp, bp, ct, bt)\n","                    for (cp, bp, ct, bt) in zip(clas_preds, bbox_preds, clas_tgts, bbox_tgts)])/clas_tgts.size(0)\n","\n","crit_f = RetinaNetFocalLoss_f(scales=scales, ratios=ratios)\n","loss_f = crit_f(out_f, bbox_targ, clas_tgt)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r9l6ds_xCtso","colab_type":"code","colab":{}},"source":["def retina_net_split(model):\n","    groups = [list(model.encoder.children())[:6], list(model.encoder.children())[6:]]\n","    return groups + [list(model.children())[1:]]\n","\n","learn = Learner(data, model, loss_func=crit)\n","learn = learn.split(retina_net_split)\n","learn.freeze()\n","learn.lr_find()\n","#learn.recorder.plot(skip_end=5)\n","learn.recorder.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TeuW6WkLwjzb","colab_type":"code","colab":{}},"source":["learn.fit_one_cycle(5, 1e-4)\n","learn.save('stage1-128')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVz56J5o4voS","colab_type":"code","colab":{}},"source":["learn.unfreeze()\n","learn.fit_one_cycle(10, slice(1e-6, 5e-5))\n","learn.save('stage2-128')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l_Ht-akX42ku","colab_type":"code","colab":{}},"source":["learn.data = get_data(32,192)\n","learn.freeze()\n","learn.lr_find()\n","learn.recorder.plot()\n","learn.fit_one_cycle(5, 1e-4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QfolReHr454h","colab_type":"code","colab":{}},"source":["learn.unfreeze()\n","learn.fit_one_cycle(10, slice(1e-6, 5e-5))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3hl1v_WR5CYJ","colab_type":"code","colab":{}},"source":["learn.data = get_data(24,256)\n","learn.freeze()\n","learn.fit_one_cycle(5, 1e-4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjRP3rb35NLw","colab_type":"code","colab":{}},"source":["learn.unfreeze()\n","learn.fit_one_cycle(10, slice(1e-6, 5e-5))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Ukla1o65X2u","colab_type":"code","colab":{}},"source":["def _draw_outline(o:Patch, lw:int):\n","    \"Outline bounding box onto image `Patch`.\"\n","    o.set_path_effects([patheffects.Stroke(\n","        linewidth=lw, foreground='black'), patheffects.Normal()])\n","\n","def draw_rect(ax:plt.Axes, b:Collection[int], color:str='white', text=None, text_size=14):\n","    \"Draw bounding box on `ax`.\"\n","    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n","    _draw_outline(patch, 4)\n","    if text is not None:\n","        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n","        _draw_outline(patch,1)\n","\n","def unpad(bbox_tgt, class_tgt, pad_idx=0):\n","  i = torch.min(torch.nonzero(class_tgt-pad_idx))\n","  return tl_br_to_ctr_hw(bbox_tgt[i:]), class_tgt[i:]-1+pad_idx\n","\n","def process_output (output, i, detect_thresh=0.25):\n","  class_preds, bbox_activ_preds, p_sizes = output\n","  class_pred, bbox_activ_pred = class_preds[i], bbox_activ_preds[i]\n","  anchors = create_anchor_boxes(p_sizes, ratios, scales).to(class_pred.device)\n","  bbox_pred = prediction_to_bbox (bbox_activ_pred, anchors)\n","  class_pred = torch.sigmoid(class_pred)\n","  class_max, class_idx = torch.max(class_pred, 1)\n","  detect_mask = class_max > detect_thresh\n","\n","  bbox_detect_ctrhw = bbox_pred[detect_mask]\n","  bbox_detect_tlbr = torch.clamp (ctr_hw_to_tl_br(bbox_detect_ctrhw), min=-1, max=1)\n","  bbox_detect = tl_br_to_ctr_hw(bbox_detect_tlbr)\n","  class_detect = class_idx[detect_mask]\n","  class_max_detect = class_max[detect_mask]\n","  if class_detect.numel() == 0: return [],[],[]\n","  return (bbox_detect, class_max_detect, class_detect)\n","  #bbox_detect_tlbr = ctr_hw_to_tl_br(bbox_detect)\n","\n","def show_preds(img, output, idx, detect_thresh=0.25, classes=None, ax=None):\n","    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)\n","    if len(scores) != 0:\n","      to_keep = nms(bbox_pred, scores)\n","      to_keep_f = nms_f(bbox_pred, scores)\n","      print (to_keep, to_keep_f)\n","      bbox_pred, preds, scores = bbox_pred[to_keep].cpu(), preds[to_keep].cpu(), scores[to_keep].cpu()\n","      t_sz = torch.Tensor([*img.size])[None].float()\n","      bbox_pred[:,:2] = bbox_pred[:,:2] - bbox_pred[:,2:]/2\n","      bbox_pred[:,:2] = (bbox_pred[:,:2] + 1) * t_sz/2\n","      bbox_pred[:,2:] = bbox_pred[:,2:] * t_sz\n","      bbox_pred = bbox_pred.long()\n","    if ax is None: _, ax = plt.subplots(1,1)\n","    img.show(ax=ax)\n","    for bbox, c, scr in zip(bbox_pred, preds, scores):\n","        txt = str(c.item()) if classes is None else classes[c.item()+1]\n","        draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=f'{txt} {scr:.2f}')\n","\n","def nms(bbox_detect, scores, thresh=0.3):\n","  keep = []\n","  bbox_remain, scores_remain = bbox_detect, scores\n","  #idxs = torch.argsort(scores)\n","  #keep = bbox_detect[idxs[0]]\n","  #bbox_remain, scores_remain = bbox_detect[idxs[1:]], scores[idxs[1:]]\n","  while (len(bbox_remain) > 0):\n","    idxs = scores_remain.argsort(descending=True)\n","    keep.append(idxs[0])\n","    bbox_max = bbox_remain[idxs[0]].view(-1, 4)\n","    bbox_remain, scores_remain = bbox_remain[idxs[1:]], scores_remain[idxs[1:]]\n","    iou_val = iou(bbox_max, bbox_remain).view(-1)\n","    bbox_remain, scores_remain = bbox_remain[iou_val < thresh], scores_remain[iou_val < thresh]\n","  return(LongTensor(keep))\n","\n","def nms_f(boxes, scores, thresh=0.3):\n","    idx_sort = scores.argsort(descending=True)\n","    boxes, scores = boxes[idx_sort], scores[idx_sort]\n","    to_keep, indexes = [], torch.LongTensor(range_of(scores))\n","    while len(scores) > 0:\n","        to_keep.append(idx_sort[indexes[0]])\n","        iou_vals = iou(boxes, boxes[:1]).squeeze()\n","        mask_keep = iou_vals < thresh\n","        if len(mask_keep.nonzero()) == 0: break\n","        boxes, scores, indexes = boxes[mask_keep], scores[mask_keep], indexes[mask_keep]\n","    return LongTensor(to_keep)\n","\n","def show_results(learn, start=0, n=5, detect_thresh=0.35, figsize=(10,25)):\n","    x,y = learn.data.one_batch(DatasetType.Valid, cpu=False)\n","    with torch.no_grad():\n","        z = learn.model.eval()(x)\n","    _,axs = plt.subplots(n, 2, figsize=figsize)\n","    for i in range(n):\n","        img,bbox = learn.data.valid_ds[start+i]\n","        img.show(ax=axs[i,0], y=bbox)\n","        show_preds(img, z, start+i, detect_thresh=detect_thresh, classes=learn.data.classes, ax=axs[i,1])\n","\n","img,target = next(iter(data.valid_dl))\n","with torch.no_grad():\n","    output = learn.model(img)\n","idx = 0\n","img2 = data.valid_ds[idx][0]\n","show_preds(img2, output, idx, detect_thresh=0.25, classes=data.classes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vqU2yJnNK4VM","colab_type":"code","colab":{}},"source":["show_results(learn, start=10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YU0zu0E21q5i","colab_type":"code","colab":{}},"source":["def get_predictions(output, idx, detect_thresh=0.05):\n","    bbox_pred, scores, preds = process_output(output, idx, detect_thresh)\n","    if len(scores) == 0: return [],[],[]\n","    to_keep = nms(bbox_pred, scores)\n","    return bbox_pred[to_keep], preds[to_keep], scores[to_keep]\n","\n","def compute_ap(precision, recall):\n","    \"Compute the average precision for `precision` and `recall` curve.\"\n","    recall = np.concatenate(([0.], list(recall), [1.]))\n","    precision = np.concatenate(([0.], list(precision), [0.]))\n","    for i in range(len(precision) - 1, 0, -1):\n","        precision[i - 1] = np.maximum(precision[i - 1], precision[i])\n","    idx = np.where(recall[1:] != recall[:-1])[0]\n","    ap = np.sum((recall[idx + 1] - recall[idx]) * precision[idx + 1])\n","    return ap\n","\n","def class_AP(model, dl, n_classes, iou_thresh=0.5, detect_thresh=0.35, num_keep=100):\n","    #tps, clas, p_scores = [], [], []\n","    #classes, n_gts = LongTensor(range(n_classes)),torch.zeros(n_classes).long()\n","    with torch.no_grad():\n","        full_df = pd.DataFrame()\n","        for input, target in progress_bar(dl):\n","            output = model(input)\n","            n_images, ni = len(input), target[0].size(0)\n","            #print ('n images', n_images, ni)\n","            for i in range(10):\n","                bbox_detect, class_detect, class_max_detect = get_predictions(output, i, detect_thresh)\n","                tgt_bbox, tgt_class = unpad(target[0][i], target[1][i])\n","                if len(bbox_detect) != 0 and len(tgt_bbox) != 0:\n","                    #db.set_trace()\n","                    iou_val = iou(bbox_detect, tgt_bbox)\n","                    max_iou, matches = iou_val.max(1)\n","                    tps = max_iou > iou_thresh\n","                    ap = torch.cat((class_detect.view(-1, 1).float(), class_max_detect.view(-1, 1), max_iou.view(-1, 1), matches.view(-1, 1).float(), tgt_class[matches].view(-1, 1).float()), 1)\n","                    #print(ap)\n","                    #print('shapes', bbox_detect.shape[0], class_detect.shape, class_max_detect.shape, tgt_bbox.shape[0], iou_val.shape[0], max_iou.shape[0], matches.shape[0], tps.shape[0], ap.shape)\n","                    ap_df = pd.DataFrame(ap.cpu().numpy(), columns=['Class Pred', 'Class Score', 'IOU', 'IOU IDX', 'IOU Class'])\n","                    ap_df['TP'] = (ap_df['Class Pred'] == ap_df['IOU Class']) & (ap_df['IOU'] > iou_thresh)\n","                    ap_df['IMG'] = i\n","                    full_df = full_df.append(ap_df)\n","        full_df = full_df.sort_values(['Class Pred', 'Class Score'], ascending=[True, False])\n","        full_df['Pred Seq'] = full_df.groupby('Class Pred')['Class Score'].rank(method='first', ascending=False)\n","        return (full_df)\n","  \n","def compute_class_AP_f(model, dl, n_classes, iou_thresh=0.5, detect_thresh=0.35, num_keep=100):\n","    db.set_trace()\n","    tps, clas, p_scores = [], [], []\n","    classes, n_gts = LongTensor(range(n_classes)),torch.zeros(n_classes).long()\n","    with torch.no_grad():\n","        for input,target in progress_bar(dl):\n","            output = model(input)\n","            for i in range(target[0].size(0)):\n","                bbox_pred, preds, scores = get_predictions(output, i, detect_thresh)\n","                tgt_bbox, tgt_clas = unpad(target[0][i], target[1][i])\n","                if len(bbox_pred) != 0 and len(tgt_bbox) != 0:\n","                    ious = iou(bbox_pred, tgt_bbox)\n","                    max_iou, matches = ious.max(1)\n","                    detected = []\n","                    for i in range_of(preds):\n","                        if max_iou[i] >= iou_thresh and matches[i] not in detected and tgt_clas[matches[i]] == preds[i]:\n","                            detected.append(matches[i])\n","                            tps.append(1)\n","                        else: tps.append(0)\n","                    clas.append(preds.cpu())\n","                    p_scores.append(scores.cpu())\n","                n_gts += (tgt_clas.cpu()[:,None] == classes[None,:]).sum(0)\n","    tps, p_scores, clas = torch.tensor(tps), torch.cat(p_scores,0), torch.cat(clas,0)\n","    fps = 1-tps\n","    idx = p_scores.argsort(descending=True)\n","    tps, fps, clas = tps[idx], fps[idx], clas[idx]\n","    aps = []\n","    #return tps, clas\n","    for cls in range(n_classes):\n","        tps_cls, fps_cls = tps[clas==cls].float().cumsum(0), fps[clas==cls].float().cumsum(0)\n","        if tps_cls.numel() != 0 and tps_cls[-1] != 0:\n","            precision = tps_cls / (tps_cls + fps_cls + 1e-8)\n","            recall = tps_cls / (n_gts[cls] + 1e-8)\n","            aps.append(compute_ap(precision, recall))\n","        else: aps.append(0.)\n","    return aps\n","\n","L = class_AP(learn.model, data.valid_dl, data.c-1)\n","#L_f = compute_class_AP(learn.model, data.valid_dl, data.c-1)\n","#for ap,cl in zip(L_f, data.classes[1:]): print(f'{cl}: {ap:.6f}')\n","L"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrNHka5DYo78","colab_type":"code","colab":{}},"source":["data."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2bJE6kRHrthy","colab_type":"code","colab":{}},"source":["# Verify that the outputs from Fastai and my implementations are identical\n","class_pred_f, bbox_pred_f, sizes_f, acts_f = out_f\n","print (class_pred_f.shape, bbox_pred_f.shape, sizes_f, 'f done')\n","\n","my_class_pred, my_bbox_pred, my_sizes, my_acts = out\n","print (my_class_pred.shape, my_bbox_pred.shape, my_sizes, 'my done')\n","\n","for af, a in zip(acts_f, my_acts):\n","  print (af.shape, a.shape, torch.all(torch.eq(af, a)))\n","print (torch.all(torch.eq(class_pred_f, my_class_pred)), torch.all(torch.eq(bbox_pred_f, my_bbox_pred)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PZqVyxIyDsI8","colab_type":"text"},"source":["### Temporary"]},{"cell_type":"code","metadata":{"id":"zVkxw4kTKTWD","colab_type":"code","colab":{}},"source":["def tlbr2cthw(boxes):\n","    \"Convert top/left bottom/right format `boxes` to center/size corners.\"\n","    center = (boxes[:,:2] + boxes[:,2:])/2\n","    sizes = boxes[:,2:] - boxes[:,:2]\n","    return torch.cat([center, sizes], 1)\n","  \n","def cthw2tlbr(boxes):\n","    \"Convert center/size format `boxes` to top/left bottom/right corners.\"\n","    top_left = boxes[:,:2] - boxes[:,2:]/2\n","    bot_right = boxes[:,:2] + boxes[:,2:]/2\n","    return torch.cat([top_left, bot_right], 1)\n","\n","def encode_class(idxs, n_classes):\n","    target = idxs.new_zeros(len(idxs), n_classes).float()\n","    mask = idxs != 0\n","    i1s = LongTensor(list(range(len(idxs))))\n","    target[i1s[mask],idxs[mask]-1] = 1\n","    return target\n","\n","def bbox_to_activ(bboxes, anchors, flatten=True):\n","    \"Return the target of the model on `anchors` for the `bboxes`.\"\n","    if flatten:\n","        t_centers = (bboxes[...,:2] - anchors[...,:2]) / anchors[...,2:] \n","        t_sizes = torch.log(bboxes[...,2:] / anchors[...,2:] + 1e-8) \n","        return torch.cat([t_centers, t_sizes], -1).div_(bboxes.new_tensor([[0.1, 0.1, 0.2, 0.2]]))\n","    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n","    return res\n","\n","def intersection_f(anchors, targets):\n","    \"Compute the sizes of the intersections of `anchors` by `targets`.\"\n","    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)\n","    a, t = ancs.size(0), tgts.size(0)\n","    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)\n","    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])\n","    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])\n","    sizes = torch.clamp(bot_right_i - top_left_i, min=0) \n","    return sizes[...,0] * sizes[...,1]\n","\n","def IoU_values(anchors, targets):\n","    \"Compute the IoU values of `anchors` by `targets`.\"\n","    inter = intersection_f(anchors, targets)\n","    anc_sz, tgt_sz = anchors[:,2] * anchors[:,3], targets[:,2] * targets[:,3]\n","    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter\n","    return inter/(union+1e-8)\n","\n","def match_anchors_f(anchors, targets, match_thr=0.5, bkg_thr=0.4):\n","    \"Match `anchors` to targets. -1 is match to background, -2 is ignore.\"\n","    matches = anchors.new(anchors.size(0)).zero_().long() - 2\n","    if targets.numel() == 0: return matches\n","    ious = IoU_values(anchors, targets)\n","    vals,idxs = torch.max(ious,1)\n","    matches[vals < bkg_thr] = -1\n","    matches[vals > match_thr] = idxs[vals > match_thr]\n","    #Overwrite matches with each target getting the anchor that has the max IoU.\n","    #vals,idxs = torch.max(ious,0)\n","    #If idxs contains repetition, this doesn't bug and only the last is considered.\n","    #matches[idxs] = targets.new_tensor(list(range(targets.size(0)))).long()\n","    return matches\n","\n","match_anchors_f(anchors, targets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8q9zP3CNVbXO","colab_type":"code","colab":{}},"source":["size=(3,4)\n","anchors_np = create_grid_centres(size)\n","anchors = torch.from_numpy(anchors_np).float()\n","anchors = torch.cat([anchors, torch.tensor([2/size[0],2/size[1]]).expand_as(anchors)], 1)\n","activations = 0.1 * torch.randn(size[0]*size[1], 4)\n","bboxes_np = prediction_to_bbox (activations, anchors)\n","bboxes = torch.from_numpy(bboxes_np)\n","\n","match_anchors_f(anchors,bboxes)\n","match_anchors(anchors,bboxes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbIs9royWAcF","colab_type":"code","colab":{}},"source":["anchors_np = create_grid_centres((2,2))\n","anchors = torch.from_numpy(anchors_np).float()\n","anchors = torch.cat([anchors, torch.tensor([1.,1.]).expand_as(anchors)], 1)\n","targets = anchors.clone()\n","anchors = torch.cat([anchors, torch.tensor([[-0.5,0.,1.,1.8]])], 0)\n","\n","match_anchors_f(anchors,targets)\n","match_anchors(anchors,targets)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6_g_XkjxVaZa","colab_type":"text"},"source":["### Obsolete"]},{"cell_type":"code","metadata":{"id":"ylBxqGYsCwV3","colab_type":"code","colab":{}},"source":["torch.arange(0,16).long().view(4,4)\n","\n","def create_grid(size):\n","    \"Create a grid of a given `size`.\"\n","    H, W = size if is_tuple(size) else (size,size)\n","    grid = FloatTensor(H, W, 2)\n","    linear_points = torch.linspace(-1+1/W, 1-1/W, W) if W > 1 else tensor([0.])\n","    grid[:, :, 1] = torch.ger(torch.ones(H), linear_points).expand_as(grid[:, :, 0])\n","    linear_points = torch.linspace(-1+1/H, 1-1/H, H) if H > 1 else tensor([0.])\n","    grid[:, :, 0] = torch.ger(linear_points, torch.ones(W)).expand_as(grid[:, :, 1])\n","    return grid.view(-1,2)\n","\n","\n","def create_anchors(sizes, ratios, scales, flatten=True):\n","    \"Create anchor of `sizes`, `ratios` and `scales`.\"\n","    aspects = [[[s*math.sqrt(r), s*math.sqrt(1/r)] for s in scales] for r in ratios]\n","    aspects = torch.tensor(aspects).view(-1,2)\n","    anchors = []\n","    for h,w in sizes:\n","        #4 here to have the anchors overlap.\n","        sized_aspects = 4 * (aspects * torch.tensor([2/h,2/w])).unsqueeze(0)\n","        base_grid = create_grid((h,w)).unsqueeze(1)\n","        n,a = base_grid.size(0),aspects.size(0)\n","        ancs = torch.cat([base_grid.expand(n,a,2), sized_aspects.expand(n,a,2)], 2)\n","        anchors.append(ancs.view(h,w,a,4))\n","    return torch.cat([anc.view(-1,4) for anc in anchors],0) if flatten else anchors\n","\n","size = (4,4)\n","show_anchors(create_grid(size), size)\n","\n","ratios = [1/2,1,2]\n","scales = [1,2**(-1/3), 2**(-2/3)] \n","#Paper used [1,2**(1/3), 2**(2/3)] but a bigger size (600) too, so the largest feature map gave anchors that cover less of the image.\n","sizes = [(2**i,2**i) for i in range(5)]\n","sizes.reverse() #Predictions come in the order of the smallest feature map to the biggest\n","anchors = create_anchors(sizes, ratios, scales)\n","anchors.size()\n","\n","show_boxes(anchors[900:909])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OA0FgJ5PwyU4","colab_type":"code","colab":{}},"source":["def activ_to_bbox(acts, anchors, flatten=True):\n","    \"Extrapolate bounding boxes on anchors from the model activations.\"\n","    if flatten:\n","        #acts.mul_(acts.new_tensor([[0.1, 0.1, 0.2, 0.2]])) #Can't remember where those scales come from, but they help regularize\n","        centers = anchors[...,2:] * acts[...,:2] + anchors[...,:2]\n","        sizes = anchors[...,2:] * torch.exp(acts[...,:2])\n","        return torch.cat([centers, sizes], -1)\n","    else: return [activ_to_bbox(act,anc) for act,anc in zip(acts, anchors)]\n","    return res\n","  \n","size=(3,4)\n","anchors = create_grid(size)\n","anchors = torch.cat([anchors, torch.tensor([2/size[0],2/size[1]]).expand_as(anchors)], 1)\n","activations = torch.randn(size[0]*size[1], 4) * 0.1\n","bboxes = activ_to_bbox(activations, anchors)\n","\n","show_boxes(bboxes)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UBGPjWuVxhHo","colab_type":"code","colab":{}},"source":["def cthw2tlbr(boxes):\n","    \"Convert center/size format `boxes` to top/left bottom/right corners.\"\n","    top_left = boxes[:,:2] - boxes[:,2:]/2\n","    bot_right = boxes[:,:2] + boxes[:,2:]/2\n","    return torch.cat([top_left, bot_right], 1)\n","\n","  def intersection(anchors, targets):\n","    \"Compute the sizes of the intersections of `anchors` by `targets`.\"\n","    ancs, tgts = cthw2tlbr(anchors), cthw2tlbr(targets)\n","    a, t = ancs.size(0), tgts.size(0)\n","    ancs, tgts = ancs.unsqueeze(1).expand(a,t,4), tgts.unsqueeze(0).expand(a,t,4)\n","    top_left_i = torch.max(ancs[...,:2], tgts[...,:2])\n","    bot_right_i = torch.min(ancs[...,2:], tgts[...,2:])\n","    sizes = torch.clamp(bot_right_i - top_left_i, min=0) \n","    return sizes[...,0] * sizes[...,1]\n","\n","def IoU_values(anchors, targets):\n","    \"Compute the IoU values of `anchors` by `targets`.\"\n","    inter = intersection(anchors, targets)\n","    anc_sz, tgt_sz = anchors[:,2] * anchors[:,3], targets[:,2] * targets[:,3]\n","    union = anc_sz.unsqueeze(1) + tgt_sz.unsqueeze(0) - inter\n","    return inter/(union+1e-8)\n","\n","def match_anchors(anchors, targets, match_thr=0.5, bkg_thr=0.4):\n","    \"Match `anchors` to targets. -1 is match to background, -2 is ignore.\"\n","    matches = anchors.new(anchors.size(0)).zero_().long() - 2\n","    if targets.numel() == 0: return matches\n","    ious = IoU_values(anchors, targets)\n","    vals,idxs = torch.max(ious,1)\n","    matches[vals < bkg_thr] = -1\n","    matches[vals > match_thr] = idxs[vals > match_thr]\n","    #Overwrite matches with each target getting the anchor that has the max IoU.\n","    #vals,idxs = torch.max(ious,0)\n","    #If idxs contains repetition, this doesn't bug and only the last is considered.\n","    #matches[idxs] = targets.new_tensor(list(range(targets.size(0)))).long()\n","    return matches\n"," \n","targets = torch.tensor([[0.,0.,2.,2.], [-0.5,-0.5,1.,1.], [1/3,0.5,0.5,0.5]])\n","show_boxes(targets)\n","\n","match_anchors(anchors, targets)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfXVXV_4yul7","colab_type":"code","colab":{}},"source":["anchors = create_grid((2,2))\n","anchors = torch.cat([anchors, torch.tensor([1.,1.]).expand_as(anchors)], 1)\n","targets = anchors.clone()\n","anchors = torch.cat([anchors, torch.tensor([[-0.5,0.,1.,1.8]])], 0)\n","match_anchors(anchors,targets)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XGtoECVhNcy4","colab_type":"text"},"source":["### Temp"]},{"cell_type":"code","metadata":{"id":"mtvc-2-EiXi1","colab_type":"code","colab":{}},"source":["try_tfms = get_transforms(do_flip=True, flip_vert=True, max_rotate=45., max_zoom=3.2, max_lighting=.8)\n","\n","def get_ex(): return open_image(jpeg_dir/image_dict[17])\n","\n","def plots_f(rows, cols, width, height, **kwargs):\n","    [get_ex().apply_tfms(try_tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n","        rows,cols,figsize=(width,height))[1].flatten())]\n","plots_f(2, 8, 20, 6, size=224)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f47rL3j0k4iz","colab_type":"code","colab":{}},"source":["import pdb\n","import IPython.core.debugger as db\n","\n","_,axs = plt.subplots(1,3,figsize=(9,3))\n","for rsz,ax in zip([ResizeMethod.CROP, ResizeMethod.PAD, ResizeMethod.SQUISH], axs):\n","    db.set_trace()\n","    get_ex().apply_tfms([crop_pad()], size=224, resize_method=rsz, padding_mode='zeros').show(ax=ax, title=rsz.name.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5JMUsQAUJwlG","colab_type":"code","colab":{}},"source":["tfms = get_transforms()\n","data = ImageDataBunch.from_csv(data_dir, 'VOCdevkit/VOC2007/JPEGImages', csv_labels=csv_file, label_delim=' ', label_col=2, ds_tfms=tfms, \n","                               size=224, resize_method=ResizeMethod.SQUISH, bs=64).normalize(imagenet_stats)\n","data.show_batch(rows=3, figsize=(7,6))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I0VH5GICJmk7","colab_type":"code","colab":{}},"source":["data2 = (ImageList.from_df(df, path=data_dir, folder='VOCdevkit/VOC2007/JPEGImages')\n","      .split_by_rand_pct()\n","      .label_from_df(cols=['tlr', 'tlc', 'brr', 'brc'], label_cls=FloatList)\n","      .transform(get_transforms(), resize_method=ResizeMethod.SQUISH, size=224)\n","      .databunch(bs=64)) \n","data2.normalize(imagenet_stats)\n","data2.show_batch(rows=3, figsize=(7,6))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4JBrl2QX1TB","colab_type":"code","colab":{}},"source":["head_reg4 = nn.Sequential(Flatten(), nn.Linear(25088,4))\n","learn = cnn_learner(data2, models.resnet34, custom_head=head_reg4, loss_func=nn.L1Loss(), metrics=[accuracy])"],"execution_count":0,"outputs":[]}]}