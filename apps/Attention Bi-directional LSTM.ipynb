{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Attention Bi-directional LSTM.ipynb","provenance":[],"collapsed_sections":["oDqvMEF25fhd","HsjygTKEfwvV","l0s6fY5mK4hf","9aAvrYcEmgCU","qHVHJ_8gTL0A","ddP0iK4rrpKf","5t1ovSkv-Z0Q","2hhFl5LeHs_8","MifFzIoLHwcl","uFCUY7XbXC0g","M_3IqdiPkYSc","lToMJvqNVMrq","jdhr4w0HcaY5","xhSSNeHcaHDM","xjcX8-8mrK4o"],"authorship_tag":"ABX9TyO3xTcc79emeO6k0OF6nNyW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"_udsuq_1R9q5","colab_type":"text"},"source":["### Sequence-to-Sequence for Machine Translation with Attention and Bi-directional LSTM - [notebook](https://github.com/fastai/course-v3/blob/master/nbs/dl2/translation.ipynb), [video](https://course18.fast.ai/lessons/lesson11.html), [notes](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-11-61477d24dc34)"]},{"cell_type":"markdown","metadata":{"id":"233jN_MC1KLk","colab_type":"text"},"source":["**Todos**\n","*   Import Text Translation Data Bundle from app_lib, and Bleu Metrics\n","*   Try arch with AWD-LSTM\n","*   Track results in DTR\n","*   Print out intermediate results - ie show what the translated text we're outputting is, so we can see what it is really doing internally\n","*   One Cycle scheduler etc\n","*   Cleanup cells with Fastai code\n","*   Make sure App and Arch are clean and as per standard template\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oDqvMEF25fhd","colab_type":"text"},"source":["### Import KD Libraries"]},{"cell_type":"code","metadata":{"id":"EHLPROiP6Lpk","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nx-Xm5ShB3A7","colab_type":"code","colab":{}},"source":["import IPython.core.debugger as db\n","from pathlib import Path\n","import pandas as pd\n","import torch\n","#import torch.nn.functional as F\n","from torch import tensor, nn\n","from torch.nn.utils.rnn import pad_sequence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QwN17d7_DKe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"executionInfo":{"status":"ok","timestamp":1595325861156,"user_tz":-330,"elapsed":27157,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"e6a0055b-a7be-4ba0-8085-add6658883f7"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","gd_path = 'gdrive/My Drive/Colab Data/fastai-v3'  #change dir to your project folder\n","gn_path = 'gdrive/My Drive/Colab Notebooks'  #change dir to your project folder\n","\n","import sys\n","sys.path.insert(1, gn_path + '/exp')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jcLxkgTy_P1L","colab_type":"code","colab":{}},"source":["from nb_data import *\n","from nb_training import *\n","from nb_optimiser import *"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFB0Uiq-4RUN","colab_type":"text"},"source":["### Build Seq2Seq Architecture"]},{"cell_type":"code","metadata":{"id":"CSHB38pzv3Vt","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Encoder Module\n","#----------------------------------------------------\n","class Seq_Encoder(nn.Module):\n","  #----------------------------------------------------\n","  # Build the architecture with an Embedding and LSTM layer, along with some Dropouts.\n","  # Optionally, it has a Linear layer to reshape the LSTM hidden state as needed for\n","  # the Decoder\n","  #----------------------------------------------------\n","  def __init__(self, n_h, n_layers, n_dirs, emb, inp_p=0.15, enc_p=0.25, hid_p=0.05):\n","    super().__init__()\n","\n","    self.n_h, self.n_layers = n_h, n_layers\n","    self.n_dirs = n_dirs\n","\n","    self.enc_emb = emb\n","    _, emb_sz = emb.weight.size()\n","    self.enc_inp_drop = nn.Dropout(p=inp_p)\n","    self.enc_lstm = nn.LSTM(emb_sz, n_h, n_layers, batch_first=True, bidirectional=(n_dirs == 2), dropout=enc_p)\n","    self.enc_hid_drop = nn.Dropout(p=hid_p)\n","\n","    # The Linear layer's input size is determined by the Encoder LSTM while its output\n","    # size is determined by the Decoder LSTM\n","    #\n","    # Since both hidden state (h) and cell state (c) from the encoder are fed to\n","    # the Linear layer, its input size is 'n_h + n_h' and its output size is 'emb_sz + emb_sz'\n","    # In the case of a bidirectional LSTM, its input becomes '2 * (n_h + n_h)' but\n","    # its output remains as before ie. 'emb_sz + emb_sz'. The output is not multiplied\n","    # by 2 for bidirectional because it gets fed to the Decoder's LSTM which is \n","    # always unidirectional.\n","    self.enc_out = nn.Linear(n_dirs * (n_h + n_h), emb_sz + emb_sz, bias=False)\n","\n","  # ----------------------------\n","  # Process the forward pass\n","  # ----------------------------\n","  def forward(self, inp):\n","    if (isinstance(inp, tuple)):\n","      # yb is passed in only during training, by the Teacher Force Callback\n","      xb, yb = inp\n","    else:\n","      # yb is not passed during validation\n","      xb, yb = inp, None\n","\n","    # Input has shape (samples, timestep)\n","    bs, _ = xb.size()\n","\n","    # Initialise the LSTM hidden state and cell state\n","    init_h, init_c = self._init_hc(bs)\n","\n","    # Input goes through embedding layer with dropout\n","    # emb_val and drop_val has shape (samples, timestep, embedding size)\n","    emb_val = self.enc_emb(xb)\n","    drop_val = self.enc_inp_drop(emb_val)\n","\n","    # Without this flatten, the LSTM generates lots of warnings on each pass\n","    # Not sure why this happens\n","    self.enc_lstm.flatten_parameters()\n","\n","    # Process the LSTM layer\n","    # 'enc_out' has hidden states from all timesteps of the last LSTM layer.\n","    # 'h' has hidden state from the last timestep of all LSTM layers and\n","    # 'c' has the cell state from the last timestep of all LSTM layers\n","    #\n","    # 'out' has shape (samples, timestep, hidden size * num directions)\n","    # 'h' and 'c' have shape (num layers * num directions, samples, hidden size)\n","    enc_out, (h, c) = self.enc_lstm(drop_val, (init_h, init_c))\n","\n","    # Separate the hidden state by layers and direction, as per the description in\n","    # the Pytorch LSTM docs.\n","    h = h.view(self.n_layers, self.n_dirs, bs, self.n_h)\n","    # Now re-arrange so the direction dimension is second-last.\n","    h = h.permute(0, 2, 1, 3).contiguous()\n","    # Reshape again so that the direction gets multiplied into the last dimension\n","    # We do this because this will be fed into the Linear layer which expects\n","    # its input feature size in the last dimension.\n","    h = h.view(self.n_layers, bs, self.n_dirs * self.n_h)\n","\n","    # Repeat the above for the cell state\n","    c = c.view(self.n_layers, self.n_dirs, bs, self.n_h)\n","    c = c.permute(0, 2, 1, 3).contiguous()\n","    c = c.view(self.n_layers, bs, self.n_dirs * self.n_h)\n","\n","    hc = torch.cat([h, c], dim=2)\n","    hc = self.enc_out(self.enc_hid_drop(hc))\n","    emb_sz = hc.size(2) // 2\n","    h, c = hc[..., :emb_sz].contiguous(), hc[..., emb_sz:].contiguous()\n","    \n","    # Output a tuple of the hidden and cell states\n","    return h, c, enc_out, yb\n","\n","  # ----------------------------\n","  # Initialise the hidden (h) and cell state (c)\n","  # ----------------------------\n","  def _init_hc(self, bs):\n","    # Use the data type and device from any (we take the first one) parameter \n","    # of our model\n","    first_param = next(self.parameters())\n","\n","    # 'h' and 'c' have shape (num layers * num directions, samples, hidden size)\n","    init_h = first_param.new_zeros(self.n_dirs * self.n_layers, bs, self.n_h)\n","    init_c = first_param.new_zeros(self.n_dirs * self.n_layers, bs, self.n_h)\n","\n","    return init_h, init_c\n","\n","#----------------------------------------------------\n","# Decoder Module\n","#----------------------------------------------------\n","class Seq_Decoder(nn.Module):\n","  #----------------------------------------------------\n","  # Build the architecture with an Embedding and LSTM layer, along with some dropouts\n","  # This is followed by a Linear layer to produce the output predictions\n","  #\n","  # !!!!!!!!!!! Drop probs and tok_idx hardcoded\n","  #----------------------------------------------------\n","  def __init__(self, n_layers, pad_idx, emb, enc_n_h, enc_n_dirs, dec_p=0.1, drop_p=0.35, tok_begin_idx=2, tok_end_idx=1):\n","    super().__init__()\n","\n","    self.pad_idx, self.tok_begin_idx, self.tok_end_idx = pad_idx, tok_begin_idx, tok_end_idx\n","    self.teacher_force = 0\n","\n","    # Embedding layer\n","    self.dec_emb = emb\n","    vocab_sz, emb_sz = emb.weight.size()\n","\n","    # LSTM layer\n","    # We make the hidden size the same as the embedding size\n","    att_ctx_sz = enc_n_dirs * enc_n_h\n","    n_h = emb_sz\n","    self.dec_lstm = nn.LSTM(emb_sz + att_ctx_sz, n_h, n_layers, batch_first=True, dropout=dec_p)\n","    self.dec_out_drop = nn.Dropout(p=drop_p)\n","\n","    # Output Linear layer\n","    self.dec_logit = nn.Linear(n_h, vocab_sz)\n","    self.dec_logit.weight.data = self.dec_emb.weight.data\n","\n","    # Attention module\n","    self.enc_aln = nn.Linear(enc_n_dirs * enc_n_h, n_h, bias=False)\n","    self.dec_hid_aln = nn.Linear(n_h, n_h)\n","    self.param_aln = self.init_aln(n_h)\n","\n","  def init_aln(self, *sz): \n","    return nn.Parameter(torch.randn(sz)/math.sqrt(sz[0]))\n","\n","  def calc_att(self, enc_out, h, dec_inp):\n","    # Calculate Alignment Scores - Encoder outputs and Decoder Hidden state are \n","    # put through a Linear layer. Then add them and do a tanh. Then dot product\n","    # with an alignment vector 'param_aln'.\n","    enc_aln = self.enc_aln(enc_out)\n","    hid_aln = self.dec_hid_aln(h[-1])\n","    u = torch.tanh(enc_aln + hid_aln.unsqueeze(1))\n","    aln_scores = u @ self.param_aln\n","\n","    # Softmax alignment scores to get Attention weights\n","    att_wgts = F.softmax(aln_scores, dim=1)\n","\n","    # Multiply the Attention weights with encoder outputs to get the context vector\n","    att_ctx = (att_wgts.unsqueeze(-1) * enc_out).sum(1)\n","\n","    #Concatenate context vector with embedded input word\n","    emb_val = self.dec_emb(dec_inp)\n","    att_dec_inp = torch.cat([emb_val, att_ctx], 1)\n","    att_dec_inp = att_dec_inp.unsqueeze(1)\n","    return att_dec_inp\n","\n","  # ----------------------------\n","  # Process the forward pass\n","  # ----------------------------\n","  def forward(self, inp):\n","    # We get the tuple of hidden and cell state from the Encoder layer as input\n","    # During training we also get the target data, which we use for teacher forcing\n","    h, c, enc_out, yb = inp\n","    _, bs, _ = h.size()\n","\n","    # Use the data type and device from any (we take the first one) parameter \n","    # of our model to initialise the LSTM 's input word sequence 'dec_inp'\n","    # 'dec_inp' is initialised to a single word (viz. the starting token) and has shape (samples, )\n","    first_param = next(self.parameters())\n","    dec_inp = first_param.new_full((bs, ), self.tok_begin_idx, dtype=torch.long)\n","\n","    # We explicitly go through a loop one word at a time (ie. single LSTM timestep at a time), rather \n","    # rather than use the LSTM's built-in loop. We do this because we want to feed the output word\n","    # of one timestep as the input word of the next time step\n","    sentence = []\n","    i, max_loop = 0, 30\n","    end = False\n","    while ((i < max_loop) and (not end)):\n","      att_dec_inp = self.calc_att(enc_out, h, dec_inp)\n","\n","      # 'x' goes through the embedding layer, which outputs shape (samples, embedding size)\n","      # The unsqueeze inserts a 2nd dimension for the timestep with a value of 1 since\n","      # we are always doing a single timestep (ie. our sequence length is always 1)\n","      #emb_val = self.dec_emb(dec_inp).unsqueeze(1)\n","\n","      # Without this flatten, the LSTM generates lots of warnings on each pass\n","      # Not sure why this happens\n","      self.dec_lstm.flatten_parameters()\n","\n","      # Process the LSTM layer\n","      # 'out' has hidden states from all timesteps of the last LSTM layer.\n","      # 'h' has hidden state from the last timestep of all LSTM layers and\n","      # 'c' has the cell state from the last timestep of all LSTM layers\n","      #\n","      # 'out' has shape (samples, timestep, hidden size * num directions)\n","      # 'h' and 'c' have shape (num layers * num directions, samples, hidden size)\n","      out, (h, c) = self.dec_lstm(att_dec_inp, (h, c))\n","      \n","      # Take only the first timestep of the output\n","      # 'drop_val' has shape (samples, hidden size * num directions)\n","      drop_val = self.dec_out_drop(out[:, 0, ...])\n","\n","      # Linear layer generates logit scores for each word in the vocab\n","      # 'word_logits' shape is (samples, vocab size)\n","      word_logits = self.dec_logit(drop_val)\n","\n","      # Append the scores for the full vocab to our emerging sentence\n","      # Sentence is a list [scores for timestep 1, scores for timestep 2, ...] where\n","      # we have the scores for each word in the vocab at each timestep\n","      sentence.append(word_logits)\n","\n","      # Get the vocab index of the word with the highest score. This is the\n","      # predicted word at this position in the sequence\n","      word_idx = word_logits.argmax(dim=1)\n","\n","      if (word_idx == self.pad_idx).all():\n","        # End the loop if we just predicted a padding token, which means we've\n","        # reached the end of the sequence\n","        end = True\n","\n","      elif ((yb is not None) and (random.random() < self.teacher_force)):\n","        if (i >= yb.size(1)):\n","          end = True\n","        else:\n","          # With teacher forcing, we feed the correct target word as the input word for\n","          # the next timestep.\n","          dec_inp = yb[:, i]\n","\n","      else:\n","        # Take the word we just predicted and feed that as the input word for \n","        # the next timestep. Note that here, as we go through loop, we are \n","        # using just one word with the highest predicted score.\n","        x = word_idx\n","\n","      # Go to the next timestep\n","      i += 1\n","\n","    # Stack up all the timesteps from our generated sentence into one tensor\n","    # of shape (samples, timesteps, vocab size)\n","    #\n","    # Note that here we're keeping scores for all words in the vocab, not just the\n","    # word with the highest score.\n","    return torch.stack(sentence, dim=1)\n","\n","#----------------------------------------------------\n","# Callback to aid with Teacher Forcing\n","#----------------------------------------------------\n","class TeacherForceCB(Callback):\n","  def __init__(self, max_epoch):\n","    self.max_epoch = max_epoch\n","\n","  # ----------------------------\n","  # During training, package the input data as a tuple (xb, yb) containing both the\n","  # 'x' and 'y' targets\n","  # ----------------------------\n","  def begin_tr_batch(self, ctx):\n","    ctx.xb = (ctx.xb, ctx.yb)\n","\n","  # ----------------------------\n","  # Adjust the teacher forcing hyper parameter as we progress through training\n","  # ----------------------------\n","  def begin_tr(self, ctx):\n","    # Find the Decoder module by traversing the model's children\n","    dec = None\n","    for name, module in ctx.model.named_children():\n","      if (isinstance(module, Seq_Decoder)):\n","        dec = module\n","    assert (dec is not None)\n","\n","    # Decay the teacher forcing so that we do less of it as we progress through\n","    # training epochs. Since the model is now better trained it should be able to\n","    # produce the correct output without relying on teacher forcing \n","    dec.teacher_force = 1 - 0.5 * ctx.i_epoch/self.max_epoch\n","\n","#----------------------------------------------------\n","# Create the Translation Encoder-Decoder architecture\n","#----------------------------------------------------\n","class ArchTextTranslation():\n","  #----------------------------------------------------\n","  # Create the Encoder ('x') and Decoder ('y') models\n","  #----------------------------------------------------\n","  def create_model(self, vocab_y, enc_n_h, n_layers, enc_n_dirs, pad_idx_y):\n","    # Embedding layers should be pre-loaded\n","    assert ((self.emb_x is not None) and (self.emb_y is not None))\n","\n","    # Create the Encoder and Decoder\n","    self.enc = Seq_Encoder(enc_n_h, n_layers, enc_n_dirs, self.emb_x)\n","    self.dec = Seq_Decoder(n_layers, pad_idx_y, self.emb_y, enc_n_h, enc_n_dirs)\n","\n","    # And wrap them in a Sequential\n","    self.model = nn.Sequential(self.enc, self.dec)\n","\n","  #----------------------------------------------------\n","  # Set the embedding layers for the Encoder and Decoder\n","  # These must be done before creating the model\n","  #----------------------------------------------------\n","  def load_emb(self, emb_x, emb_y):\n","    # Encoder's embedding\n","    self.emb_x = emb_x\n","    #self.emb_sz_x = emb_sz_x\n","\n","    # Decoder's embedding\n","    self.emb_y = emb_y\n","    #self.emb_sz_y = emb_sz_y\n","\n","  #----------------------------------------------------\n","  # Pre-create the Embedding layer and initialise its weights using\n","  # the pre-trained embedding vectors\n","  #----------------------------------------------------\n","  @staticmethod\n","  def pre_process_embedding(vocab, pre_trained_wgts, emb_sz, pad_idx):\n","    vocab_sz = len(vocab)\n","\n","    # Create the Embedding layer of shape [vocab, embedding size]\n","    emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_idx)\n","    \n","    # Go through each word in our vocab, and look up it's embedding\n","    # vector in the pre-trained vectors package. If we find it there\n","    # we use it to initialise the Embedding layer weight\n","    emb_data = emb.weight.data\n","    for i, word in enumerate(vocab):\n","      word_wgt = pre_trained_wgts.get_word_vector(word)\n","      if (word_wgt is not None):\n","        emb_data[i] = tensor(word_wgt)\n","\n","    return emb\n","\n","  #----------------------------------------------------\n","  # Utility method to pad the sequences in the predicted output\n","  # and the target output so that they are the same length\n","  # Used during loss and accuracy calculations to compare predicted output with target\n","  #----------------------------------------------------\n","  @staticmethod\n","  def _tt_pad_pred(out, targ, pad_idx=1):\n","    # Target has shape (samples, timestep) ie. (samples, sequence length)\n","    bs, targ_seq_len = targ.size()\n","    # Predicted output has shape (samples, timestep, vocab size) ie. (samples, sequence length, vocab size)\n","    out_bs, out_seq_len, out_vocab_sz = out.size()\n","\n","    if (targ_seq_len > out_seq_len):\n","      # Pad the output if target is longer\n","      # F.pad arguments start from the last dimension and go backwards\n","      # ie. F.pad (3rd dim start, 3rd dim end, 2nd dim start, 2nd dim end, 1st dim start, 1st dim end)\n","      # ie. we pad the end of the output's 2nd dimension viz. sequence length\n","      out = F.pad(out, (0, 0, 0, targ_seq_len - out_seq_len, 0, 0), value=pad_idx)\n","\n","    elif (out_seq_len > targ_seq_len):\n","      # Pad the target if output is longer\n","      # ie. F.pad (2nd dim start, 2nd dim end, 1st dim start, 1st dim end)\n","      # ie. we pad the end of the target's 2nd dimension viz. sequence length\n","      targ = F.pad(targ, (0, out_seq_len - targ_seq_len, 0, 0), value=pad_idx)\n","    return out, targ\n","  \n","\n","  #----------------------------------------------------\n","  # Loss function uses cross_entropy loss between the predicted output and\n","  # targets. Note the cross_entropy internally calls softmax of the output\n","  # and then computes loss\n","  #----------------------------------------------------\n","  @classmethod\n","  def tt_loss(cls, out, targ, pad_idx=1):\n","    # First make output and target be the same length\n","    pad_out, pad_targ = cls._tt_pad_pred(out, targ, pad_idx)\n","\n","    # Flatten the output and targets and then calculate cross_entropy\n","    return F.cross_entropy(pad_out.view(-1, out.size(2)), pad_targ.view(-1))\n","\n","  #----------------------------------------------------\n","  # Loss function uses cross_entropy loss between the predicted output and\n","  # targets. Note the cross_entropy internally calls softmax of the output\n","  # and then computes loss\n","  #----------------------------------------------------\n","  @classmethod\n","  def tt_acc(cls, out, targ, pad_idx=1):\n","    # First make output and target be the same length\n","    pad_out, pad_targ = cls._tt_pad_pred(out, targ, pad_idx)\n","\n","    # Target has shape (samples, timestep)\n","    # Predicted output has shape (samples, timestep, vocab size)\n","\n","    # Get the word with the highest score at each timestep in the sequence\n","    pred_out = pad_out.argmax(2)\n","\n","    # Compare the word at each timestep with the corresponding word in the target\n","    res = (pred_out == pad_targ).float().mean()\n","    return res"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsjygTKEfwvV","colab_type":"text"},"source":["### Define Text Translation Data Bundle"]},{"cell_type":"code","metadata":{"id":"5s95O1BhkMn3","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Text Translation from CSV data preparation pipeline\n","#----------------------------------------------------\n","class TextTranslationCSVDataBundle(DataBundle):\n","  def __init__(self, csv_path, bs):\n","    print ('--------- Text Translation DataBundle init', csv_path)\n","\n","    # Load all rows from the given CSV file\n","    # Split randomly based on a percentage ratio for training and validation\n","    # 'x' items are taken from 'fr' column as text sentences and\n","    # 'y' labels are taken from 'en' column as class name labels\n","    # Convert the 'x' items from Sentences to Words to Word Ids\n","    # Convert the 'y' items from Sentences to Words to Word Ids\n","\n","    load_params = {'source': CSVItemContainer, 'target_cls': DfItemList, 'csv_path': csv_path}\n","    split_params = {'split_procedure': 'split_random', 'train_ratio': 0.8, 'valid_ratio': 0.2}\n","    extract_x_params = {'extract_procedure': 'extract_colval', 'target_cls': SentenceItemList, 'col': 'fr', 'lang': 'fr'}\n","    extract_y_params = {'extract_procedure': 'extract_colval', 'target_cls': SentenceItemList, 'col': 'en'}\n","    convert_x_params = [\n","        {'target_cls': SentenceWordItemList, 'convert_procedure': 'SentenceToWord'},\n","        {'target_cls': SentenceWordIdItemList, 'convert_procedure': 'WordToWordId'}\n","    ]\n","    convert_y_params = [\n","        {'target_cls': SentenceWordItemList, 'convert_procedure': 'SentenceToWord'},\n","        {'target_cls': SentenceWordIdItemList, 'convert_procedure': 'WordToWordId'}\n","    ]\n","    # We use different samples for training and validation. Also, the custom sampler functions need to take a sort-key-function\n","    # as an extra argument. The key function needs the 'data' argument pre-bound using a partial.\n","    len_key_fn = lambda i,data: len(data[i])\n","    dl_params = (\n","        {'bs': bs, 'sampler_fn': SortishSampler, 'key_fn': len_key_fn, 'collate_fn': seq_collate},    # for training\n","        {'bs': bs, 'sampler_fn': SortSampler, 'key_fn': len_key_fn, 'collate_fn': seq_collate}        # for valid/test\n","    )\n","    WAIT_dl_params = (\n","        {'bs': bs, 'shuffle': False, 'collate_fn': seq_collate},    # for training\n","        {'bs': bs, 'shuffle': False, 'collate_fn': seq_collate}     # for valid/test\n","    )\n","    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, dl_params=dl_params)\n","\n","  # ----------------------------\n","  # Since we use Sorting Samplers which require a key function (which also requires a partial wrapper)\n","  # we have to override the parent DataBundle with a custom get_sampler() \n","  # ----------------------------\n","  def get_sampler(self, ds, in_train, bs, sampler_fn, key_fn, **kwargs):\n","    key=partial(key_fn, data=ds.x)\n","    # The two sampler functions take different arguments. Since we know the sampler\n","    # functions we can hardcode their names. The sampler_fn that is passed in is the\n","    # same, but we ignore it.\n","    if (in_train):\n","      sampler = SortishSampler(ds.x, key=key, bs=bs)\n","    else:\n","      sampler = SortSampler(ds.x, key=key)\n","    return sampler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOwyRga53ocV","colab_type":"code","colab":{}},"source":["# ----------------------------\n","# Collate function to convert a list of item tuples into a single tensor which is fed\n","# as input to the model\n","#\n","# 'Samples' is a list of tuples ie. [(x1, y1), (x2, y2), ...] where 'xn' and 'yn' are \n","# both lists of word ids in the sentence. All the sentences could have different lengths, so\n","# after each 'xn' and 'yn' are converted to tensors, they are padded at the end to\n","# make them the same length.\n","# ----------------------------\n","def seq_collate(samples, pad_idx=1, pad_first=False):\n","  # Convert each sample sentence into a tensor and then create two lists of \n","  # tensors ie. [tensor x1, tensor x2, ...] and [tensor y1, tensor y2, ...]\n","  tx, ty = [torch.tensor(s[0]) for s in samples], [torch.tensor(s[1]) for s in samples]\n","\n","  # Use the Pytorch pad_sequence function to pad each sample tensor to the same length and then\n","  # concatenate them into a single tensor. So 'px' and 'py' have shape (samples, max sequence length)\n","  px, py = pad_sequence(tx, batch_first=True, padding_value=pad_idx), pad_sequence(ty, batch_first=True, padding_value=pad_idx)\n","  return px, py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0s6fY5mK4hf","colab_type":"text"},"source":["### Define Text Translation application class "]},{"cell_type":"code","metadata":{"id":"t5hWAJmEvUEf","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Text Translation Application. It translates text sentences from an 'x' language into\n","# a 'y' language, using a Sequence-to-Sequence Encoder-Decoder architecture.\n","#\n","# Here, we translate from French to English\n","#\n","# To use it, the steps are:\n","#   1. Download and pre-process the data, saving it in a CSV file\n","#   2. Download embedding vectors for both languages and pre-process them to \n","#        initialise two Embedding layers and save them\n","#   3. Now, load the pre-processed data\n","#   4. Load the saved initialised Embedding layers\n","#   5. Create the architecture and train it\n","#   6. Run predictions\n","#----------------------------------------------------\n","class AppTextTranslation():\n","\n","  def __init__(self):\n","    self._arch = None\n","    self.db = None\n","    self.vocab = None\n","\n","  #----------------------------------------------------\n","  # Pre-process the text data for both languages. The dataset is huge, so we filter it\n","  # by selecting only sentences which are questions. The filtered data is then saved\n","  # into a CSV file.\n","  #----------------------------------------------------\n","  def pre_process_data(self, data_x_path, data_y_path, out_data_path):\n","    # Regular expressions to select questions for the 'y' (English) language by\n","    # searching for sentences beginning with 'Wh' and ending with '?'. For the 'x'\n","    # language we pick the full sentence.\n","    re_yq = re.compile('^(Wh[^?.!]+\\?)')\n","    re_xq = re.compile('^([^?.!]+\\?)')\n","\n","    # Find all matching sentences from 'x' and 'y'\n","    xf = open(data_x_path, encoding='utf-8')\n","    yf = open(data_y_path, encoding='utf-8')\n","    lines = ((re_yq.search(yq), re_xq.search(xq)) for yq, xq in zip(yf, xf))\n","\n","    # Pick out the sentences which matched\n","    qs = [(y.group(), x.group()) for y,x in lines if y and x]\n","\n","    # Prepare a Pandas dataframe and save it to a CSV file\n","    df = pd.DataFrame({'fr': [q[1] for q in qs], 'en': [q[0] for q in qs]}, columns = ['en', 'fr'])\n","    #df['en'] = df['en'].apply(lambda x:x.lower())\n","    #df['fr'] = df['fr'].apply(lambda x:x.lower())\n","    df.to_csv(out_data_path, index=False)\n","\n","    # Free memory as the dataset is huge\n","    del lines, qs\n","    return df\n","\n","  #----------------------------------------------------\n","  # Pre-create the Embedding layer and initialise its weights using\n","  # the pre-trained embedding vectors.\n","  # \n","  # Save the initialised Embedding layer\n","  #----------------------------------------------------\n","  def pre_process_embedding(self, pre_trained_wgts_path, emb_wgts_path, is_emb_x):\n","    assert ((self.vocab_x is not None) and (self.vocab_y is not None))\n","\n","    # Load the pre-trained embedding vectors\n","    word_vecs = ft.load_model(str(pre_trained_wgts_path))\n","\n","    # Get the appropriate vocab\n","    emb_vocab = self.vocab_x if is_emb_x else self.vocab_y\n","    emb_sz, pad_idx = 300, emb_vocab.index(PAD)\n","\n","    # Create the Embedding layer and initialise its weights\n","    emb = ArchTextTranslation.pre_process_embedding(emb_vocab, word_vecs, emb_sz, pad_idx)\n","\n","    # Save the initialised Embedding layer\n","    torch.save(emb, emb_wgts_path)\n","\n","    # Free some memory as the pre-trained vectors are huge\n","    del word_vecs\n","\n","  #----------------------------------------------------\n","  # Load the data using the Text Translation Data Bundle\n","  #----------------------------------------------------\n","  def load_data(self, file_path, bs):\n","    self.db = TextTranslationCSVDataBundle(file_path, bs)\n","    self.db.do()\n","    self.vocab_x = self.db.convert_state_x['vocab_i2w']\n","    self.vocab_y = self.db.convert_state_y['vocab_i2w']\n","\n","  #----------------------------------------------------\n","  # Load the initialised Embedding layer that we saved during pre-processing\n","  #----------------------------------------------------\n","  def load_emb(self, emb_x_wgts_path, emb_y_wgts_path):\n","    # Load the saved 'x' and 'y' embedding data\n","    emb_x = torch.load(emb_x_wgts_path)\n","    emb_y = torch.load(emb_y_wgts_path)\n","\n","    # Add them into our architecture\n","    self._arch = ArchTextTranslation()\n","    self._arch.load_emb(emb_x, emb_y)\n","\n","  #----------------------------------------------------\n","  # Create the architecture\n","  # This assumes that the Embedding layers have already been loaded\n","  #----------------------------------------------------\n","  def create_arch(self):\n","    assert(self._arch is not None)\n","    assert ((self.vocab_x is not None) and (self.vocab_y is not None))\n","    enc_n_h, n_layers, n_dirs, pad_idx_y = 256, 2, 2, self.vocab_y.index(PAD)\n","    self._arch.create_model(self.vocab_y, enc_n_h, n_layers, n_dirs, pad_idx_y)\n","    return self._arch\n","\n","  def create_fastai_gru(self):\n","    emb_x, emb_y = self._arch.emb_x, self._arch.emb_y\n","    self._arch.model = Seq2SeqGRU(emb_x, emb_y, 256, 30, n_layers=2, bos_idx=2)\n","\n","  #----------------------------------------------------\n","  # Train the model\n","  #----------------------------------------------------\n","  def run_train(self, num_epochs=1, bleu=False):\n","    train_dl = self.db.train_dl\n","    valid_dl = self.db.valid_dl\n","\n","    # Loss function\n","    loss_func = self._arch.tt_loss\n","\n","    # Compute accuracy and optionally, Bleu scores\n","    metrics_dict = {\"acc\": self._arch.tt_acc}\n","    if (bleu):\n","      metrics_dict['bleu'] = corpus_bleu\n","\n","    callbs=[CudaCB(device = torch.device('cuda',0)), ProgressCallback(), MetricsCB(metrics_dict), TeacherForceCB(max_epoch=num_epochs)]\n","    one_cycle = False\n","    if (one_cycle):\n","      one_cycle_callbs = create_OneCycleCB(split_lr, phases=[0.5, 0.5], mom_start=0.8, mom_mid=0.7, mom_end=0.8)\n","      callbs = callbs + one_cycle_callbs\n","\n","    # Model\n","    model = self._arch.model\n","\n","    # Optimiser\n","    opt_func=adam_opt_func\n","    lr = 1e-2\n","    opt_groups = None\n","    opt = get_optimiser(model, lr, opt_func, opt_groups)\n","\n","    # Debug Tracker\n","    dtr = None\n","    if (False):\n","      dtr = DebugTracker(disp=(True, False))\n","      debug_cbs = [dtr, DebugYhatLossCB()]\n","      callbs = callbs + debug_cbs\n","\n","    loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs, dtr=dtr)\n","    loop.fit(num_epochs=num_epochs)\n","    return loop\n","\n","  def run_predict(self):\n","    valid_dl = self.db.valid_dl\n","    self._arch.model.eval()\n","    device = list(self._arch.model.parameters())[0].device\n","\n","    inps, outs, targs = [], [], []\n","    with torch.no_grad():\n","      for _, (xb, yb) in enumerate(valid_dl):\n","        xb = xb.to(device)\n","        yhat = self._arch.model(xb)\n","        pred = yhat.argmax(dim=2)\n","        for x, y, p in zip (xb, yb, pred):\n","          inp = [self.vocab_x[w] for w in x]\n","          inps.append(inp)\n","          out = [self.vocab_y[w] for w in p]\n","          outs.append(out)\n","          targ = [self.vocab_y[w] for w in y]\n","          targs.append(targ)\n","    return inps, outs, targs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9aAvrYcEmgCU","colab_type":"text"},"source":["### Define Data File Paths"]},{"cell_type":"code","metadata":{"id":"x6w6DBZvfFoR","colab_type":"code","colab":{}},"source":["root_path = Path.cwd()\n","data_path = root_path/'giga-fren'\n","data_file_path = data_path/'questions_easy.csv'\n","emb_x_wgts_path = data_path/'fr_emb.pth'\n","emb_y_wgts_path = data_path/'en_emb.pth'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHVHJ_8gTL0A","colab_type":"text"},"source":["### Fetch and Pre-Process Data"]},{"cell_type":"code","metadata":{"id":"VqHVXHcAj6Fr","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Download and unzip the data set. It consists of two huge\n","# files, one for English sentences and one for French sentences\n","#----------------------------------------------------\n","\n","\n","! wget https://s3.amazonaws.com/fast-ai-nlp/giga-fren.tgz -P {root_path}\n","! tar xf {root_path}/giga-fren.tgz -C {root_path} \n","\n","data_path.ls()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SokjPS8eCyYw","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Pre-process the data, filtering it to a manageable size and save it to\n","# a CSV file\n","#----------------------------------------------------\n","\n","ttpp_app = AppTextTranslation()\n","data_x_path = data_path/'giga-fren.release2.fixed.fr'\n","data_y_path = data_path/'giga-fren.release2.fixed.en'\n","ppdf = ttpp_app.pre_process_data(data_x_path, data_y_path, data_file_path)\n","\n","!rm {data_x_path}\n","!rm {data_y_path}\n","\n","data_path.ls()\n","ppdf[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vH1ga_q-KIS4","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Load the data bundle, because we need the vocabs when we\n","# pre-process embeddings in the next step\n","#----------------------------------------------------\n","\n","ttpp_app.load_data(data_file_path, bs=32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ddP0iK4rrpKf","colab_type":"text"},"source":["### Fetch FastText word embeddings"]},{"cell_type":"code","metadata":{"id":"HOL0BDvFrl1c","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Download and install the Fasttext embedding library\n","#   https://fasttext.cc/docs/en/crawl-vectors.html\n","#----------------------------------------------------\n","\n","! git clone https://github.com/facebookresearch/fastText.git\n","! cd fastText; pip install .\n","import fasttext as ft"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMWxxzJ4sPqZ","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Now get fastText pre-trained word embedding vectors for French\n","# NB: Each downloaded gzipped file is 4.2G and takes 6 minutes to download. \n","# After unzipping, each file is about 7GB.\n","#\n","# Then, initialise an Embedding layer with those pre-trained weights\n","# Since this step consumes a lot of disk and memory, we can only pre-process one\n","# language at a time, then immediately free the disk space and memory.\n","#----------------------------------------------------\n","\n","! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz -P {data_path}\n","! gzip -d {data_path}/cc.fr.300.bin.gz \n","data_path.ls()\n","\n","ttpp_app.pre_process_embedding(data_path/'cc.fr.300.bin', emb_x_wgts_path, True)\n","\n","!rm {data_path}/cc.fr.300.bin"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6NKgDNHrWY0","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Now get the fastText pre-trained word embedding vectors for English\n","#----------------------------------------------------\n","\n","! wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz -P {data_path}\n","! gzip -d {data_path}/cc.en.300.bin.gz\n","data_path.ls()\n","\n","ttpp_app.pre_process_embedding(data_path/'cc.en.300.bin', emb_y_wgts_path, False)\n","\n","!rm {data_path}/cc.en.300.bin"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5t1ovSkv-Z0Q","colab_type":"text"},"source":["### Bleu Score Metric"]},{"cell_type":"code","metadata":{"id":"R2pXTRx7-dHj","colab_type":"code","colab":{}},"source":["from math import exp\n","\n","class NGram():\n","    def __init__(self, ngram, max_n=5000):\n","      #print('===', ngram)\n","      self.ngram,self.max_n = ngram,max_n\n","    def __eq__(self, other):\n","        if len(self.ngram) != len(other.ngram): return False\n","        return np.all(np.array(self.ngram) == np.array(other.ngram))\n","    def __hash__(self):\n","      hml = [o * self.max_n**i for i,o in enumerate(self.ngram)]\n","      hm = int(sum(hml))\n","      return hm\n","\n","def get_grams(x, n, max_n=5000):\n","    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]\n","\n","def get_correct_ngrams(pred, targ, n, max_n=5000):\n","    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)\n","    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)\n","    precm = [(c, targ_cnt[g]) for g,c in pred_cnt.items()]\n","    precl = [min(c, g) for c, g in precm]\n","    prec, ln = sum(precl),len(pred_grams)\n","    return prec, ln\n","\n","def corpus_bleu(preds, targs, max_n=5000):\n","    pred_len,targ_len,n_precs,counts = 0,0,[0]*4,[0]*4\n","    tmp_preds = preds.argmax(dim=-1)\n","    tmp_preds, tmp_targs = tmp_preds.cpu().numpy(), targs.cpu().numpy()\n","    for pred,targ in zip(tmp_preds, tmp_targs):\n","        pred_len += len(pred)\n","        targ_len += len(targ)\n","        for i in range(4):\n","            c,t = get_correct_ngrams(pred, targ, i+1, max_n=max_n)\n","            n_precs[i] += c\n","            counts[i] += t\n","    #db.set_trace()\n","    n_precs = [c/t if (t > 0) else 0 for c,t in zip(n_precs,counts)]\n","    len_penalty = exp(1 - targ_len/pred_len) if pred_len < targ_len else 1\n","    return len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)\n","\n","class CorpusBLEU(Callback):\n","    def __init__(self, vocab_sz):\n","        self.vocab_sz = vocab_sz\n","        self.name = 'bleu'\n","    \n","    def on_epoch_begin(self, **kwargs):\n","        self.pred_len,self.targ_len,self.n_precs,self.counts = 0,0,[0]*4,[0]*4\n","    \n","    def on_batch_end(self, last_output, last_target, **kwargs):\n","        last_output = last_output.argmax(dim=-1)\n","        for pred,targ in zip(last_output.cpu().numpy(),last_target.cpu().numpy()):\n","            self.pred_len += len(pred)\n","            self.targ_len += len(targ)\n","            for i in range(4):\n","                c,t = get_correct_ngrams(pred, targ, i+1, max_n=self.vocab_sz)\n","                self.n_precs[i] += c\n","                self.counts[i] += t\n","    \n","    def on_epoch_end(self, last_metrics, **kwargs):\n","        n_precs = [c/t for c,t in zip(n_precs,counts)]\n","        len_penalty = exp(1 - targ_len/pred_len) if pred_len < targ_len else 1\n","        bleu = len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)\n","        return add_metrics(last_metrics, bleu)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcCsDyCn5WqX","colab_type":"text"},"source":["### Test run the model"]},{"cell_type":"markdown","metadata":{"id":"QgU-FibyHnjp","colab_type":"text"},"source":["#### KD Model and Data"]},{"cell_type":"code","metadata":{"id":"7aWLj3VKv91R","colab_type":"code","colab":{}},"source":["torch.manual_seed(0)\n","tt_app = AppTextTranslation()\n","tt_app.load_data(data_file_path, bs=64)\n","tt_app.load_emb(emb_x_wgts_path, emb_y_wgts_path)\n","tt_app.create_arch()\n","loop = tt_app.run_train(num_epochs=8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F2c8FJNuW3y7","colab":{"base_uri":"https://localhost:8080/","height":165},"executionInfo":{"status":"ok","timestamp":1586158813810,"user_tz":-330,"elapsed":309362,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"8624888b-a55f-42cf-f1c5-6bdfec78a040"},"source":["tt_app.run_train(num_epochs=2, bleu=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["33 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>train_bleu</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>valid_bleu</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>2.263853</td>\n","      <td>0.655897</td>\n","      <td>0.474985</td>\n","      <td>4.947572</td>\n","      <td>0.159318</td>\n","      <td>0.067105</td>\n","      <td>02:35</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>2.501406</td>\n","      <td>0.625128</td>\n","      <td>0.452113</td>\n","      <td>3.404715</td>\n","      <td>0.349272</td>\n","      <td>0.217035</td>\n","      <td>02:32</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["<nb_training.Trainer at 0x7ff48003c438>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"BcqZSyCHyRZF","colab_type":"code","colab":{}},"source":["inps, outs, targs = tt_app.run_predict()\n","inps[70], outs[70], targs[70]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hhFl5LeHs_8","colab_type":"text"},"source":["#### KD model, Fastai data"]},{"cell_type":"code","metadata":{"id":"pNgVCWEjDcUl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"ok","timestamp":1586156779613,"user_tz":-330,"elapsed":616218,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"682f855e-1968-46b0-d8dd-612e3cb86b1e"},"source":["torch.manual_seed(0)\n","tt_app = AppTextTranslation()\n","tt_app.db = data\n","tt_app.vocab_x = data.x.vocab.itos\n","tt_app.vocab_y = data.y.vocab.itos\n","tt_app.load_emb(emb_x_wgts_path, emb_y_wgts_path)\n","tt_app.create_arch()\n","loop = tt_app.run_train(num_epochs=8)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["33 {'momentum': 0.9, 'sqr_momentum': 0.99, 'eps': 1e-05, 'weight_decay': 0.0, 'lr': 0.01}\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","        <style>\n","            /* Turns off some styling */\n","            progress {\n","                /* gets rid of default border in Firefox and Opera. */\n","                border: none;\n","                /* Needs to be in here for Safari polyfill so background images work as expected. */\n","                background-size: auto;\n","            }\n","            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n","                background: #F44336;\n","            }\n","        </style>\n","      <progress value='0' class='' max='8', style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      \n","    </div>\n","    \n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>valid_loss</th>\n","      <th>valid_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>4.509823</td>\n","      <td>0.427305</td>\n","      <td>6.036685</td>\n","      <td>0.096519</td>\n","      <td>01:11</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>3.154150</td>\n","      <td>0.555705</td>\n","      <td>3.871483</td>\n","      <td>0.141544</td>\n","      <td>01:19</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>3.085337</td>\n","      <td>0.558915</td>\n","      <td>4.096089</td>\n","      <td>0.129306</td>\n","      <td>01:18</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>3.119189</td>\n","      <td>0.558114</td>\n","      <td>4.306132</td>\n","      <td>0.122286</td>\n","      <td>01:20</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>3.161039</td>\n","      <td>0.555018</td>\n","      <td>4.191979</td>\n","      <td>0.298654</td>\n","      <td>01:19</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>3.189976</td>\n","      <td>0.550393</td>\n","      <td>3.801612</td>\n","      <td>0.440781</td>\n","      <td>01:17</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>3.241537</td>\n","      <td>0.544031</td>\n","      <td>5.011482</td>\n","      <td>0.325765</td>\n","      <td>01:14</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>3.378329</td>\n","      <td>0.528495</td>\n","      <td>5.470052</td>\n","      <td>0.284828</td>\n","      <td>01:13</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"MifFzIoLHwcl","colab_type":"text"},"source":["#### KD Trainer, Fastai data and model"]},{"cell_type":"code","metadata":{"id":"j3Zcqp_6Hz9P","colab_type":"code","colab":{}},"source":["import warnings\n","#warnings.filterwarnings('ignore')\n","warnings.filterwarnings(action='once')\n","\n","torch.manual_seed(0)\n","tt_app = AppTextTranslation()\n","tt_app.db = data\n","tt_app.vocab_x = data.x.vocab.itos\n","tt_app.vocab_y = data.y.vocab.itos\n","tt_app.load_emb(emb_x_wgts_path, emb_y_wgts_path)\n","tt_app.create_fastai_gru()\n","loop = tt_app.run_train(num_epochs=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uFCUY7XbXC0g","colab_type":"text"},"source":["#### KD Trainer, KD data, Fastai GRU model"]},{"cell_type":"code","metadata":{"id":"fvGlifuWXI0-","colab_type":"code","colab":{}},"source":["import warnings\n","#warnings.filterwarnings('ignore')\n","warnings.filterwarnings(action='once')\n","\n","def one_param(mdl):\n","  first_param = next(mdl.parameters())\n","  return first_param\n","\n","torch.manual_seed(0)\n","tt_app = AppTextTranslation()\n","tt_app.load_data(data_file_path, bs=64)\n","tt_app.load_emb(emb_x_wgts_path, emb_y_wgts_path)\n","tt_app.create_fastai_gru()\n","loop = tt_app.run_train(num_epochs=8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxP3263Sa5Fy","colab_type":"code","colab":{}},"source":["tt_app.create_fastai_gru()\n","loop = tt_app.run_train(num_epochs=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lToMJvqNVMrq","colab_type":"text"},"source":["### Fastai Data load"]},{"cell_type":"code","metadata":{"id":"f25KfI8DXbbH","colab_type":"code","colab":{}},"source":["from fastai.text import *"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkiTDZ0kWJmL","colab_type":"code","colab":{}},"source":["path = Path.cwd()/'giga-fren'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FilYlf0FXbbb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1586156119210,"user_tz":-330,"elapsed":3330,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"e89fbc8c-a35b-4c16-bbd9-261f56d424db"},"source":["df = pd.read_csv(path/'questions_easy.csv')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>en</th>\n","      <th>fr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>What is light ?</td>\n","      <td>Qu’est-ce que la lumière?</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Who are we?</td>\n","      <td>Où sommes-nous?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Where did we come from?</td>\n","      <td>D'où venons-nous?</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>What would we do without it?</td>\n","      <td>Que ferions-nous sans elle ?</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>What is the absolute location (latitude and lo...</td>\n","      <td>Quelle sont les coordonnées (latitude et longi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                  en                                                 fr\n","0                                    What is light ?                          Qu’est-ce que la lumière?\n","1                                        Who are we?                                    Où sommes-nous?\n","2                            Where did we come from?                                  D'où venons-nous?\n","3                       What would we do without it?                       Que ferions-nous sans elle ?\n","4  What is the absolute location (latitude and lo...  Quelle sont les coordonnées (latitude et longi..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"8bJOjlOeXbbe","colab_type":"code","colab":{}},"source":["df['en'] = df['en'].apply(lambda x:x.lower())\n","df['fr'] = df['fr'].apply(lambda x:x.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsmEOGBvXbbi","colab_type":"code","colab":{}},"source":["def seq2seq_collate(samples:BatchSamples, pad_idx:int=1, pad_first:bool=True, backwards:bool=False) -> Tuple[LongTensor, LongTensor]:\n","    \"Function that collect samples and adds padding. Flips token order if needed\"\n","    #db.set_trace()\n","    samples = to_data(samples)\n","    max_len_x,max_len_y = max([len(s[0]) for s in samples]),max([len(s[1]) for s in samples])\n","    res_x = torch.zeros(len(samples), max_len_x).long() + pad_idx\n","    res_y = torch.zeros(len(samples), max_len_y).long() + pad_idx\n","    if backwards: pad_first = not pad_first\n","    for i,s in enumerate(samples):\n","        if pad_first: \n","            res_x[i,-len(s[0]):],res_y[i,-len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n","        else:         \n","            res_x[i,:len(s[0]):],res_y[i,:len(s[1]):] = LongTensor(s[0]),LongTensor(s[1])\n","    if backwards: res_x,res_y = res_x.flip(1),res_y.flip(1)\n","    return res_x,res_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wBRG5mnXXbbl","colab_type":"code","colab":{}},"source":["import IPython.core.debugger as db\n","class Seq2SeqDataBunch(TextDataBunch):\n","    \"Create a `TextDataBunch` suitable for training an RNN classifier.\"\n","    @classmethod\n","    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=32, val_bs:int=None, pad_idx=1,\n","               pad_first=False, device:torch.device=None, no_check:bool=False, backwards:bool=False, **dl_kwargs) -> DataBunch:\n","        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n","        #db.set_trace()\n","        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n","        val_bs = ifnone(val_bs, bs)\n","        collate_fn = partial(seq2seq_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n","        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs//2)\n","        del dl_kwargs['dl_tfms']\n","        # !!!!!!!\n","        dl_kwargs['num_workers']=0\n","        #train_sampler=None\n","        #collate_fn = partial(seq_collate, pad_idx=pad_idx)\n","        #dl_kwargs['collate_fn'] = collate_fn\n","\n","        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n","        dataloaders = [train_dl]\n","        for ds in datasets[1:]:\n","            lengths = [len(t) for t in ds.x.items]\n","            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n","  \n","            # !!!!!!!!!!!\n","            #sampler=None\n","\n","            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n","        return cls(*dataloaders, path=path, device=device, collate_fn=collate_fn, no_check=no_check)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C50M1_c-Xbbr","colab_type":"code","colab":{}},"source":["class Seq2SeqTextList(TextList):\n","    _bunch = Seq2SeqDataBunch\n","    _label_cls = TextList"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Tf2ulzpXbbv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1586156139482,"user_tz":-330,"elapsed":23582,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"97122e69-2809-4905-d086-139b42948e38"},"source":["src = Seq2SeqTextList.from_df(df, path = path, cols='fr').split_by_rand_pct().label_from_df(cols='en', label_cls=TextList)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ZzzpU_HsXbb7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1586156139484,"user_tz":-330,"elapsed":23575,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"c19b2fba-9dc9-4b2f-b607-c0036c13457e"},"source":["len(src.train) + len(src.valid)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["52331"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"7aZeJNC3Xbb-","colab_type":"code","colab":{}},"source":["data = src.databunch()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gN92IpnqwPXn","colab_type":"code","colab":{}},"source":["del src, df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OfE5Z3UjXbcC","colab_type":"code","colab":{}},"source":["data.save()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zIji_yOKK6Bl","colab_type":"code","colab":{}},"source":["del data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tm-zQub_XbcE","colab_type":"code","colab":{}},"source":["data = load_data(path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3ZLbWr4XbcG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1586156148324,"user_tz":-330,"elapsed":32391,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"5cb563e2-f469-479d-860c-0dff96078234"},"source":["data.show_batch()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>xxbos xxmaj dans un tel cas , où il s’agit d ’ xxunk si un nom commercial a un fondement juridique antérieur à celui d ’ une marque aux fins de l’article 16 , paragraphe 1 , troisième phrase , de l’accord xxup adpic , peut - on considérer comme décisif : i ) le fait que , dans l’état où la marque est enregistrée et sa protection réclamée ,</td>\n","      <td>xxbos xxmaj when assessing , in such a case , whether a trade name has a legal basis prior to a trade mark for the purposes of the third sentence of xxmaj article 16(1 ) of the trips xxmaj agreement , may it thus be considered as decisive : ( i ) whether the trade name was well known at least to some extent among the relevant trade xxunk in the xxmaj state in which the trade mark is registered and in which protection is sought for it , before the point in time at which registration of the trade mark was applied for in the xxmaj state in question ; or whether the trade name was used in commerce directed to the xxmaj state in which the trade mark is registered and in which protection is sought for it , before the point in time at which registration of the trade mark was applied for in the xxmaj state in question ; or what other factor may decide whether the trade name is to be regarded as an existing prior right within the meaning of the third sentence of xxmaj article 16(1 ) of the trips xxmaj agreement ?</td>\n","    </tr>\n","    <tr>\n","      <td>xxbos xxmaj et selon vous , quels sont actuellement les principaux obstacles au bien - être en xxmaj europe : le manque d'argent , la pénurie d'emplois satisfaisants , la pression excessive sur la vie familiale et les loisirs , la pauvreté et xxunk , xxunk des services publics comme ceux de la santé et l'éducation , l'importance de la xxunk et de la criminalité , xxunk , les problèmes</td>\n","      <td>xxbos xxmaj what else ?</td>\n","    </tr>\n","    <tr>\n","      <td>xxbos xxmaj qu’est - ce qui xxunk les fournisseurs de soins de santé à utiliser le système et qu’est - ce qui pourrait servir en tant xxunk non xxunk ou en tant que barrière à la xxup xxunk ; la nécessité de répondre serait - elle perçue comme une perte de temps si , là où les soins sont prodigués , on recherche un accès rapide à des informations en</td>\n","      <td>xxbos xxmaj what would motivate health care providers to use the system , and what would serve as a xxunk or barrier to xxup kt – would the need to respond be perceived of as a waste of time if , at the point of care , they are looking for quick access to xxunk - sized information ?</td>\n","    </tr>\n","    <tr>\n","      <td>xxbos xxmaj pourquoi , compte tenu de tout ce que nous savons au sujet des causes de xxunk , du fait de l’existence de techniques de diagnostic simples et précises et du fait que nous avons des interventions préventives et des traitements efficaces , xxunk demeure - t - elle encore aujourd’hui non xxunk , non traitée et contrôlée de façon inefficace pour tant de xxmaj canadiens et de xxmaj</td>\n","      <td>xxbos xxmaj why , given how much we know about the causes of hypertension , the fact that simple and accurate techniques for diagnosis exist , and that we have effective preventive and treatment interventions , does hypertension remain xxunk , xxunk or not effectively controlled for so many xxmaj canadians ?</td>\n","    </tr>\n","    <tr>\n","      <td>xxbos xxmaj qui peut nier les exigences et les inquiétudes de ceux qui se préoccupent de la reddition des comptes , qui soutiennent que cette nouvelle ère de mondialisation a créé un « déficit démocratique » , les gouvernements xxunk une partie de leur pouvoir et de leur influence , tandis que les regroupements xxunk – et non démocratiques – de tous genres voient xxunk leur pouvoir et leur influence</td>\n","      <td>xxbos xxmaj who can deny the claims and concerns of those xxunk with accountability , who maintain that this new era of globalization has brought a \" democratic deficit , \" with governments losing power and influence while horizontal – and non - democratic – bodies of all types see their power and influence grow ?</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"jdhr4w0HcaY5","colab_type":"text"},"source":["### Fastai model"]},{"cell_type":"code","metadata":{"id":"5m37-YYDXbcl","colab_type":"code","colab":{}},"source":["class Seq2SeqGRU(nn.Module):\n","    def __init__(self, emb_enc, emb_dec, n_hid, max_len, n_layers=2, p_inp:float=0.15, p_enc:float=0.25, \n","                 p_dec:float=0.1, p_out:float=0.35, p_hid:float=0.05, bos_idx:int=0, pad_idx:int=1):\n","        super().__init__()\n","        self.n_layers,self.n_hid,self.max_len,self.bos_idx,self.pad_idx = n_layers,n_hid,max_len,bos_idx,pad_idx\n","        self.emb_enc = emb_enc\n","        self.emb_enc_drop = nn.Dropout(p_inp)\n","        self.encoder = nn.GRU(emb_enc.weight.size(1), n_hid, n_layers, batch_first=True, dropout=p_enc)\n","        self.out_enc = nn.Linear(n_hid, emb_enc.weight.size(1), bias=False)\n","        self.hid_dp  = nn.Dropout(p_hid)\n","        self.emb_dec = emb_dec\n","        self.decoder = nn.GRU(emb_dec.weight.size(1), emb_dec.weight.size(1), n_layers, batch_first=True, dropout=p_dec)\n","        self.out_drop = nn.Dropout(p_out)\n","        self.out = nn.Linear(emb_dec.weight.size(1), emb_dec.weight.size(0))\n","        self.out.weight.data = self.emb_dec.weight.data\n","        \n","    def forward(self, inp):\n","        bs,sl = inp.size()\n","        #self.encoder.reset()\n","        #self.decoder.reset()\n","        hid = self.initHidden(bs)\n","        emb = self.emb_enc_drop(self.emb_enc(inp))\n","        #db.set_trace()\n","        enc_out, hid = self.encoder(emb, hid)\n","        hid = self.out_enc(self.hid_dp(hid))\n","\n","        dec_inp = inp.new_zeros(bs).long() + self.bos_idx\n","        outs = []\n","        for i in range(self.max_len):\n","            emb = self.emb_dec(dec_inp).unsqueeze(1)\n","            out, hid = self.decoder(emb, hid)\n","            out = self.out(self.out_drop(out[:,0]))\n","            outs.append(out)\n","            dec_inp = out.max(1)[1]\n","            if (dec_inp==self.pad_idx).all(): break\n","        return torch.stack(outs, dim=1)\n","    \n","    def initHidden(self, bs): \n","      h = one_param(self).new_zeros(self.n_layers, bs, self.n_hid)\n","      c = one_param(self).new_zeros(self.n_layers, bs, self.n_hid)\n","      return h"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YC9XOCNSXbcr","colab_type":"code","colab":{}},"source":["def seq2seq_loss(out, targ, pad_idx=1):\n","    bs,targ_len = targ.size()\n","    _,out_len,vs = out.size()\n","    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n","    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n","    return CrossEntropyFlat()(out, targ)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jmc2El1ZXbcu","colab_type":"code","colab":{}},"source":["def seq2seq_acc(out, targ, pad_idx=1):\n","    bs,targ_len = targ.size()\n","    _,out_len,vs = out.size()\n","    if targ_len>out_len: out  = F.pad(out,  (0,0,0,targ_len-out_len,0,0), value=pad_idx)\n","    if out_len>targ_len: targ = F.pad(targ, (0,out_len-targ_len,0,0), value=pad_idx)\n","    out = out.argmax(2)\n","    return (out==targ).float().mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4WiBP58-Xbc7","colab_type":"code","colab":{}},"source":["emb_enc = torch.load(path/'fr_emb.pth')\n","emb_dec = torch.load(path/'en_emb.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4xAv7H9Xbc-","colab_type":"code","colab":{}},"source":["model = Seq2SeqGRU(emb_enc, emb_dec, 256, 30, n_layers=2)\n","learn = Learner(data, model, loss_func=seq2seq_loss, metrics=[seq2seq_acc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e1o-cRwnXbdG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1585900438276,"user_tz":-330,"elapsed":299490,"user":{"displayName":"Ketan Doshi","photoUrl":"","userId":"05524438722017440561"}},"outputId":"05df4a05-aa11-4e12-fe8c-aa7ac3ede4c7"},"source":["learn.fit_one_cycle(8, 1e-2)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>epoch</th>\n","      <th>train_loss</th>\n","      <th>valid_loss</th>\n","      <th>seq2seq_acc</th>\n","      <th>time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>0</td>\n","      <td>6.523319</td>\n","      <td>6.444037</td>\n","      <td>0.198191</td>\n","      <td>00:33</td>\n","    </tr>\n","    <tr>\n","      <td>1</td>\n","      <td>6.386384</td>\n","      <td>6.044800</td>\n","      <td>0.239319</td>\n","      <td>00:31</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>5.841321</td>\n","      <td>5.976840</td>\n","      <td>0.240092</td>\n","      <td>00:34</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>5.475042</td>\n","      <td>5.442400</td>\n","      <td>0.289550</td>\n","      <td>00:37</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>5.054673</td>\n","      <td>5.665747</td>\n","      <td>0.267344</td>\n","      <td>00:38</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>4.912147</td>\n","      <td>5.174346</td>\n","      <td>0.313426</td>\n","      <td>00:39</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>4.529460</td>\n","      <td>4.900515</td>\n","      <td>0.339440</td>\n","      <td>00:41</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>4.320996</td>\n","      <td>4.802443</td>\n","      <td>0.349645</td>\n","      <td>00:42</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"xhSSNeHcaHDM","colab_type":"text"},"source":["### Results"]},{"cell_type":"markdown","metadata":{"id":"RTGWLYdJaM4w","colab_type":"text"},"source":["Fast**ai notebook QRNN**"]},{"cell_type":"code","metadata":{"id":"pW-njR8qaKXq","colab_type":"code","colab":{}},"source":["epoch\ttrain_loss\tvalid_loss\tseq2seq_acc\tbleu\ttime\n","0\t6.548347\t6.254890\t0.202597\t0.089246\t01:01\n","1\t5.807833\t5.634194\t0.261195\t0.209837\t01:00\n","2\t4.971908\t5.273254\t0.294139\t0.227867\t01:09\n","3\t4.652781\t4.496987\t0.370694\t0.282690\t01:13\n","4\t4.132740\t4.727255\t0.343164\t0.272475\t01:15\n","5\t3.622683\t4.128991\t0.403503\t0.314859\t01:20\n","6\t3.116825\t3.976531\t0.422250\t0.332448\t01:24\n","7\t2.673768\t3.869482\t0.434256\t0.340629\t01:27\n","\n","epoch\ttrain_loss\tvalid_loss\tseq2seq_acc\tbleu\ttime\n","0\t6.628302\t6.688766\t0.171176\t0.049567\t01:01\n","1\t5.611732\t5.369258\t0.288863\t0.202848\t01:03\n","2\t5.017492\t5.475257\t0.274479\t0.208970\t01:11\n","3\t4.563599\t4.983734\t0.320529\t0.251945\t01:19\n","4\t4.277009\t4.489471\t0.370596\t0.298212\t01:19\n","5\t3.556834\t4.304761\t0.387561\t0.310245\t01:21\n","6\t3.186942\t4.047068\t0.415123\t0.328579\t01:27\n","7\t2.954225\t3.975845\t0.423042\t0.333376\t01:27"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18VF14DWbvPA","colab_type":"text"},"source":["**Fastai notebook GRU**"]},{"cell_type":"code","metadata":{"id":"oTekG4Xab6OY","colab_type":"code","colab":{}},"source":["epoch\ttrain_loss\tvalid_loss\tseq2seq_acc\tbleu\ttime\n","0\t5.858428\t5.708938\t0.252170\t0.125969\t00:52\n","1\t5.357081\t5.876677\t0.238593\t0.182234\t00:51\n","2\t4.927831\t5.268726\t0.293074\t0.235639\t00:54\n","3\t4.610146\t5.358156\t0.283638\t0.224705\t00:53\n","4\t4.505199\t4.985784\t0.318191\t0.242152\t00:53\n","5\t4.107853\t4.537851\t0.364606\t0.295078\t00:58\n","6\t3.589562\t4.352857\t0.383214\t0.300911\t01:00\n","7\t3.393849\t4.046872\t0.415622\t0.327335\t01:02\n","\n","epoch\ttrain_loss\tvalid_loss\tseq2seq_acc\tbleu\ttime\n","0\t6.015546\t6.040129\t0.225212\t0.148938\t00:53\n","1\t5.336372\t5.852052\t0.243333\t0.169163\t00:52\n","2\t5.237682\t4.871711\t0.336286\t0.244732\t00:54\n","3\t5.080400\t4.824254\t0.339621\t0.263153\t00:55\n","4\t4.967921\t5.072886\t0.313548\t0.233096\t00:54\n","5\t4.413047\t4.835425\t0.336780\t0.254155\t00:56\n","6\t4.215590\t4.685118\t0.352372\t0.267846\t00:57\n","7\t4.235335\t4.585082\t0.362711\t0.275471\t00:55"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Roz5ezxb6q-","colab_type":"text"},"source":["**KD notebook, Fastai GRU model, Fastai data**"]},{"cell_type":"code","metadata":{"id":"HtYzD2av9MSs","colab_type":"code","colab":{}},"source":["epoch\ttrain_loss\tvalid_loss\tseq2seq_acc\ttime\n","0\t6.572704\t6.686001\t0.185240\t00:33\n","1\t6.240807\t6.094747\t0.233083\t00:33\n","2\t5.142201\t5.782446\t0.257133\t00:36\n","3\t5.209909\t5.562961\t0.276281\t00:40\n","4\t4.950634\t4.924454\t0.337162\t00:41\n","5\t4.485311\t4.694997\t0.358365\t00:41\n","6\t4.078117\t4.806749\t0.347632\t00:42\n","7\t3.657755\t4.505972\t0.378077\t00:43\n","\n","epoch\ttrain_loss\tvalid_loss\tseq2seq_acc\ttime\n","0\t6.623886\t6.648812\t0.184976\t00:32\n","1\t6.121196\t6.895894\t0.173997\t00:32\n","2\t5.800183\t5.728360\t0.262508\t00:35\n","3\t5.212308\t6.200798\t0.222457\t00:37\n","4\t4.995849\t5.994400\t0.235996\t00:39\n","5\t4.877837\t5.750614\t0.258163\t00:39\n","6\t4.419581\t5.157444\t0.314034\t00:41\n","7\t4.353944\t5.026139\t0.327703\t00:41\n","\n","epoch\ttrain_loss\tvalid_loss\tseq2seq_acc\ttime\n","0\t6.523319\t6.444037\t0.198191\t00:33\n","1\t6.386384\t6.044800\t0.239319\t00:31\n","2\t5.841321\t5.976840\t0.240092\t00:34\n","3\t5.475042\t5.442400\t0.289550\t00:37\n","4\t5.054673\t5.665747\t0.267344\t00:38\n","5\t4.912147\t5.174346\t0.313426\t00:39\n","6\t4.529460\t4.900515\t0.339440\t00:41\n","7\t4.320996\t4.802443\t0.349645\t00:42"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-sRk1J4cbqm","colab_type":"text"},"source":["**KD notebook, KD Trainer, KD data, Fastai GRU model**"]},{"cell_type":"code","metadata":{"id":"a-b_CvBBZ5Hi","colab_type":"code","colab":{}},"source":["epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.303920\t0.219989\t4.949887\t0.334334\t00:35\n","1\t5.550987\t0.271474\t5.841299\t0.237362\t00:38\n","2\t5.202934\t0.296545\t4.933283\t0.320496\t00:39\n","3\t4.997673\t0.313595\t5.334086\t0.276459\t00:40\n","4\t4.803404\t0.328898\t4.728201\t0.335890\t00:41\n","5\t5.031213\t0.364444\t6.475720\t0.266065\t00:44\n","6\t5.580777\t0.361593\t4.434995\t0.336225\t00:47\n","7\t5.672724\t0.336017\t4.458024\t0.428072\t00:45\n","\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.584579\t0.200409\t5.604022\t0.286170\t00:33\n","1\t6.219455\t0.261237\t3.918784\t0.506129\t00:39\n","2\t6.089532\t0.285646\t4.404509\t0.444510\t00:41\n","3\t5.939508\t0.299954\t4.831132\t0.156349\t00:42\n","4\t5.846473\t0.309220\t4.352937\t0.377930\t00:43\n","5\t5.821789\t0.311438\t4.186678\t0.342489\t00:43\n","6\t5.794986\t0.315718\t4.859937\t0.422939\t00:42\n","7\t5.747003\t0.319018\t4.219995\t0.325707\t00:44"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0qLdyCdHUy3X","colab_type":"text"},"source":["**KD notebook, KD Trainer, Fastai data, Fastai GRU model**"]},{"cell_type":"code","metadata":{"id":"0G0rTk3T4y00","colab_type":"code","colab":{}},"source":["epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.365257\t0.235857\t6.005346\t0.232613\t00:38\n","1\t5.213785\t0.332450\t4.885948\t0.337639\t00:46\n","2\t4.809270\t0.369385\t6.621243\t0.199825\t00:46\n","3\t5.111947\t0.357351\t3.876555\t0.380233\t00:49\n","4\t4.923338\t0.364496\t3.482557\t0.499148\t00:48\n","5\t4.982679\t0.359597\t3.757394\t0.466488\t00:48\n","6\t4.945480\t0.363537\t3.863272\t0.362346\t00:49\n","7\t4.993021\t0.362597\t4.038907\t0.471316\t00:50\n","\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t7.317437\t0.148277\t7.418691\t0.137883\t00:32\n","1\t6.872702\t0.235538\t7.856993\t0.119390\t00:36\n","2\t6.667437\t0.254984\t4.610329\t0.102703\t00:42\n","3\t6.731552\t0.249281\t4.564480\t0.451309\t00:42\n","4\t6.749056\t0.249370\t5.770372\t0.349853\t00:40\n","5\t6.352491\t0.282954\t7.820367\t0.128445\t00:41\n","6\t6.003761\t0.319668\t7.120711\t0.186599\t00:45\n","7\t6.067753\t0.310924\t5.295717\t0.377609\t00:46"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3aS1EYhl465n","colab_type":"text"},"source":["**KD notebook, KD model, Fastai data**"]},{"cell_type":"code","metadata":{"id":"gdBF6G0946GB","colab_type":"code","colab":{}},"source":["epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t7.095515\t0.140976\t7.180361\t0.138115\t00:28\n","1\t6.954101\t0.150736\t7.077180\t0.140501\t00:29\n","2\t6.509344\t0.184172\t6.831913\t0.150081\t00:33\n","3\t6.512704\t0.182783\t6.799678\t0.149093\t00:33\n","4\t6.481158\t0.184784\t6.855497\t0.148470\t00:33\n","5\t6.296695\t0.202436\t6.656479\t0.157314\t00:34\n","6\t6.042625\t0.227045\t6.396843\t0.179056\t00:37\n","7\t5.818643\t0.251454\t6.193115\t0.195729\t00:38\n","\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t7.144305\t0.144120\t7.115142\t0.139994\t00:31\n","1\t7.200583\t0.132423\t7.085549\t0.141792\t00:28\n","2\t7.234757\t0.127989\t6.967595\t0.145785\t00:27\n","3\t7.228754\t0.128988\t7.073987\t0.142612\t00:27\n","4\t7.204069\t0.130177\t7.017547\t0.143231\t00:26\n","5\t7.135895\t0.136813\t7.021854\t0.151495\t00:27\n","6\t6.947525\t0.154177\t6.898959\t0.159648\t00:28\n","7\t6.428139\t0.198624\t6.427719\t0.183806\t00:33\n","\n","# ---- After tying decoder weights to the embedding\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t7.388091\t0.137258\t6.970419\t0.176699\t00:30\n","1\t6.929803\t0.174572\t6.884960\t0.168512\t00:32\n","2\t5.901417\t0.264275\t5.856327\t0.245887\t00:41\n","3\t5.683479\t0.282867\t5.919020\t0.240537\t00:42\n","4\t5.632364\t0.283608\t5.929564\t0.239965\t00:42\n","5\t5.354821\t0.315040\t5.572811\t0.274488\t00:44\n","6\t5.304469\t0.315708\t5.532499\t0.277871\t00:43\n","7\t5.095870\t0.337317\t5.445774\t0.285496\t00:44\n","\n","# ---- After linear layer to encoder\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t7.329444\t0.136586\t7.392494\t0.137778\t00:29\n","1\t7.256671\t0.140552\t6.821441\t0.160257\t00:30\n","2\t6.483342\t0.199900\t7.003404\t0.153175\t00:36\n","3\t6.385310\t0.207547\t6.501233\t0.181700\t00:37\n","4\t6.088735\t0.233530\t6.362590\t0.191171\t00:39\n","5\t5.880104\t0.251751\t6.183613\t0.206138\t00:40\n","6\t5.775515\t0.260812\t5.982253\t0.228642\t00:40\n","7\t5.646072\t0.276575\t5.791481\t0.248229\t00:41\n","\n","# ---- After bidirectional\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t7.324201\t0.144511\t7.050848\t0.155531\t00:36\n","1\t7.234653\t0.145592\t7.012827\t0.159076\t00:35\n","2\t6.178216\t0.232374\t6.090143\t0.223361\t00:44\n","3\t5.653195\t0.279862\t5.989206\t0.235150\t00:47\n","4\t5.496807\t0.295095\t5.844776\t0.247888\t00:47\n","5\t5.202925\t0.328509\t5.687572\t0.260018\t00:51\n","6\t5.071029\t0.341414\t5.250099\t0.303111\t00:51\n","7\t5.036983\t0.341353\t5.198389\t0.310094\t00:49\n","\n","# ---- After teacher forcing\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t3.204214\t0.560256\t7.267510\t0.253753\t00:57\n","1\t2.933727\t0.586231\t5.940575\t0.315763\t00:58\n","2\t2.915251\t0.584706\t4.612234\t0.493744\t00:59\n","3\t2.980927\t0.577221\t4.132407\t0.512331\t00:59\n","4\t3.058429\t0.568003\t3.860979\t0.512101\t00:58\n","5\t3.219140\t0.550020\t3.861038\t0.461460\t00:58\n","6\t3.301164\t0.540253\t5.424986\t0.292765\t00:56\n","7\t3.484237\t0.519709\t6.199779\t0.221711\t00:56\n","\n","# ---- After attention\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t4.509823\t0.427305\t6.036685\t0.096519\t01:11\n","1\t3.154150\t0.555705\t3.871483\t0.141544\t01:19\n","2\t3.085337\t0.558915\t4.096089\t0.129306\t01:18\n","3\t3.119189\t0.558114\t4.306132\t0.122286\t01:20\n","4\t3.161039\t0.555018\t4.191979\t0.298654\t01:19\n","5\t3.189976\t0.550393\t3.801612\t0.440781\t01:17\n","6\t3.241537\t0.544031\t5.011482\t0.325765\t01:14\n","7\t3.378329\t0.528495\t5.470052\t0.284828\t01:13"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vY02O4TtwyjD","colab_type":"text"},"source":["**KD notebook KD model and data**"]},{"cell_type":"code","metadata":{"id":"xWVZKbISzL8L","colab_type":"code","colab":{}},"source":["epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.442913\t0.188435\t6.430400\t0.174685\t00:35\n","1\t6.223257\t0.206093\t6.504838\t0.170128\t00:35\n","2\t5.900857\t0.230201\t6.116207\t0.205968\t00:36\n","3\t5.791644\t0.241393\t5.771810\t0.240969\t00:36\n","4\t5.724015\t0.250458\t5.722351\t0.246915\t00:36\n","5\t5.600645\t0.261821\t5.902134\t0.230649\t00:36\n","6\t5.470247\t0.274700\t5.717124\t0.247558\t00:36\n","7\t5.330138\t0.287736\t5.445066\t0.272561\t00:37\n","\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.606551\t0.184464\t6.415832\t0.181752\t00:33\n","1\t6.082189\t0.216943\t6.462260\t0.184262\t00:33\n","2\t5.850541\t0.240375\t5.907950\t0.231549\t00:35\n","3\t5.657823\t0.258734\t5.456781\t0.274527\t00:36\n","4\t5.476200\t0.274681\t5.639965\t0.256353\t00:35\n","5\t5.347548\t0.285866\t5.469547\t0.273026\t00:35\n","6\t5.222646\t0.297312\t5.450500\t0.274432\t00:36\n","7\t5.111280\t0.307473\t5.185486\t0.300422\t00:36\n","\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.427548\t0.190131\t6.196699\t0.198735\t00:34\n","1\t5.895729\t0.233361\t6.493158\t0.185520\t00:35\n","2\t5.713185\t0.254113\t5.520290\t0.268809\t00:36\n","3\t5.550801\t0.269458\t5.710424\t0.252127\t00:36\n","4\t5.427881\t0.279963\t5.456533\t0.275197\t00:36\n","5\t5.304070\t0.289925\t5.427710\t0.277752\t00:36\n","6\t5.254131\t0.294898\t5.284402\t0.290690\t00:37\n","7\t5.139466\t0.304394\t5.261429\t0.291899\t00:37\n","\n","# ---- After tying decoder weights to the embedding\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.336802\t0.191463\t6.325757\t0.190538\t00:35\n","1\t6.079963\t0.218550\t5.971920\t0.225640\t00:35\n","2\t5.712619\t0.253319\t5.692638\t0.252419\t00:37\n","3\t5.525562\t0.269920\t5.607968\t0.260689\t00:37\n","4\t5.336722\t0.286840\t5.378273\t0.281291\t00:38\n","5\t5.208682\t0.299469\t5.162830\t0.301583\t00:38\n","6\t5.081091\t0.311044\t5.352839\t0.284253\t00:39\n","7\t4.934848\t0.324427\t5.264331\t0.291669\t00:39\n","\n","# ---- After linear layer to encoder\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.111099\t0.218719\t5.632525\t0.247189\t00:38\n","1\t5.847243\t0.236785\t6.023604\t0.218529\t00:38\n","2\t5.626212\t0.260971\t6.067066\t0.214797\t00:38\n","3\t5.417837\t0.281631\t5.644879\t0.254899\t00:39\n","4\t5.328231\t0.291000\t5.320411\t0.285774\t00:39\n","5\t5.136227\t0.308858\t5.074691\t0.307235\t00:39\n","6\t5.039217\t0.317713\t5.244615\t0.292767\t00:39\n","7\t4.949056\t0.326647\t4.955501\t0.318856\t00:41\n","\n","# ---- After bidirectional\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t6.667135\t0.175773\t6.637607\t0.172441\t00:36\n","1\t6.357167\t0.191023\t6.621116\t0.167819\t00:37\n","2\t6.140676\t0.209039\t5.976311\t0.220839\t00:38\n","3\t5.926323\t0.230477\t5.918930\t0.229221\t00:39\n","4\t5.702494\t0.254005\t5.926701\t0.229961\t00:40\n","5\t5.502676\t0.273406\t6.267332\t0.204831\t00:40\n","6\t5.295349\t0.294311\t5.349190\t0.284071\t00:42\n","7\t5.163258\t0.307266\t5.279955\t0.291542\t00:42\n","\n","# ---- After teacher forcing\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t3.136916\t0.565318\t6.485387\t0.428519\t00:53\n","1\t2.812268\t0.600114\t6.470203\t0.313464\t00:53\n","2\t2.768636\t0.602410\t6.242881\t0.302663\t00:53\n","3\t2.792644\t0.598249\t4.832201\t0.427251\t00:53\n","4\t2.869761\t0.588713\t4.197283\t0.439709\t00:54\n","5\t2.951091\t0.578210\t5.040192\t0.380652\t00:53\n","6\t3.061481\t0.564932\t5.360363\t0.300452\t00:52\n","7\t3.168579\t0.551636\t4.226570\t0.363828\t00:52\n","\n","# ---- After attention\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t3.329652\t0.546453\t7.376829\t0.097947\t01:09\n","1\t2.818685\t0.596728\t6.224333\t0.108751\t01:11\n","2\t2.749254\t0.601359\t5.560364\t0.149290\t01:10\n","3\t2.757350\t0.598105\t4.307948\t0.210291\t01:10\n","4\t2.773471\t0.594558\t3.415860\t0.496982\t01:09\n","5\t2.818466\t0.588552\t3.518212\t0.512504\t01:09\n","6\t2.902072\t0.579083\t5.071352\t0.313786\t01:08\n","7\t3.015603\t0.564599\t4.657800\t0.347938\t01:07\n","\n","epoch\ttrain_loss\ttrain_acc\tvalid_loss\tvalid_acc\ttime\n","0\t3.275162\t0.553106\t5.916556\t0.496572\t01:09\n","1\t2.745907\t0.608469\t3.327456\t0.500166\t01:09\n","2\t2.668602\t0.615200\t4.880656\t0.173311\t01:09\n","3\t2.630811\t0.617174\t3.602422\t0.420391\t01:09\n","4\t2.650112\t0.613514\t3.356412\t0.484790\t01:09\n","5\t2.710541\t0.604049\t3.387840\t0.488266\t01:08\n","6\t2.760253\t0.596044\t3.960105\t0.426867\t01:07\n","7\t2.862263\t0.583537\t4.520898\t0.363927\t01:07\n","\n","# ---- Continue for epoch 9-10, with Bleu\n","epoch\ttrain_loss\ttrain_acc\ttrain_bleu\tvalid_loss\tvalid_acc\tvalid_bleu\ttime\n","0\t2.332421\t0.649901\t0.470472\t3.477965\t0.401052\t0.207105\t02:34\n","1\t2.536170\t0.621716\t0.449206\t3.123379\t0.507017\t0.298915\t02:32"],"execution_count":null,"outputs":[]}]}