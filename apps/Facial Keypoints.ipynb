{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Facial Keypoints.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":["PB5m7JNHeMBh","wHhhCrjReU0z","3gu9rMESfi-3","9aAvrYcEmgCU","f5RbJYsWeZlq","hFB0Uiq-4RUN","HsjygTKEfwvV","l0s6fY5mK4hf","UXRpjqtnUAHN"],"authorship_tag":"ABX9TyOY20PUvZRoncc42gOl/ng8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"eOaWapIXdcy4","colab_type":"text"},"source":["### Detect key points on facial images from a captured video. Implemented in Keras - [article](https://towardsdatascience.com/facial-keypoints-detection-deep-learning-737547f73515), [code](https://github.com/acl21/Selfie_Filters_OpenCV), [article](http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/)"]},{"cell_type":"markdown","metadata":{"id":"233jN_MC1KLk","colab_type":"text"},"source":["**Todos**\n","*   Image augmentation\n","*   Dropout\n","*   Batchnorm\n","*   Resnet transfer learning\n","*   Use the data rows which were discarded in dropna. Dp specialists as in Daniel Nouri tutorial\n","*   Save subset data for use in data_lib\n","*   One cycle? parameter groups?\n","*   Move keypt and video utils \n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PB5m7JNHeMBh","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"EHLPROiP6Lpk","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kI5e1uoLdVMQ","colab_type":"code","colab":{}},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"koCvGQJihAhH","colab_type":"code","colab":{}},"source":["import IPython.core.debugger as db\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import math\n","\n","from sklearn.utils import shuffle\n","from keras.models import Sequential\n","#from keras.models import load_model\n","from keras.layers import Convolution2D, MaxPooling2D, Dropout\n","from keras.layers import Flatten, Dense\n","#from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wHhhCrjReU0z","colab_type":"text"},"source":["### Fetch Data from Kaggle"]},{"cell_type":"code","metadata":{"id":"akJ8fvsfGVdn","colab_type":"code","colab":{}},"source":["# Run this cell and select the kaggle.json file downloaded\n","# from the Kaggle account settings page.\n","from google.colab import files\n","files.upload()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cDLXrqcNGlU7","colab_type":"code","colab":{}},"source":["# Let's make sure the kaggle.json file is present.\n","!ls -lha kaggle.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QITWh1N5GoqG","colab_type":"code","colab":{}},"source":["# Next, install the Kaggle API client after forcing an upgrade\n","!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6\n","!kaggle -v\n","\n","# Reason for doing a force-upgrade. The underlying problem: Colab installs both py2 and py3 \n","# packages, and (for historical reasons) the py2 packages are installed second. kaggle is a \n","# wrapper installed by the kaggle python package; since we do py2 second, the py2 wrapper \n","# is in /usr/local/bin, and happens to be an older version."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KixiXyagGy7Y","colab_type":"code","colab":{}},"source":["# The Kaggle API client expects this file to be in ~/.kaggle,\n","# so move it there.\n","!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","\n","# This permissions change avoids a warning on Kaggle tool startup.\n","!chmod 600 ~/.kaggle/kaggle.json"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPZaJ7oOGuyh","colab_type":"code","colab":{}},"source":["# List available datasets.\n","!kaggle competitions list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hyJ4Rb5HMwH","colab_type":"code","colab":{}},"source":["# First, you have to login to Kaggle, go to that competition's page, navigate to \n","# the Rules tab and accept the terms and conditions. Unless you do that, you will get\n","# a 403-Forbidden error when you run the command below\n","\n","# Copy the carvana data set locally.\n","!kaggle competitions download -c facial-keypoints-detection"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gu9rMESfi-3","colab_type":"text"},"source":["### Pre-process Kaggle data"]},{"cell_type":"code","metadata":{"id":"cWs0Rp7NfqJw","colab_type":"code","colab":{}},"source":["!zipinfo facial-keypoints-detection.zip\n","\n","!unzip facial-keypoints-detection.zip -d facial >> /dev/null\n","!unzip \"facial/*.zip\" -d facial >> /dev/null\n","!ls -l facial"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9aAvrYcEmgCU","colab_type":"text"},"source":["### Define Data File Paths"]},{"cell_type":"code","metadata":{"id":"x6w6DBZvfFoR","colab_type":"code","colab":{}},"source":["root_path = Path.cwd()\n","data_path = root_path/'facial'\n","data_path.mkdir(exist_ok=True)\n","\n","metadata_file_path = data_path/'IdLookupTable.csv'\n","training_imgs_file_path = data_path/'training.csv'\n","test_imgs_file_path = data_path/'test.csv'\n","\n","list(data_path.iterdir())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f5RbJYsWeZlq","colab_type":"text"},"source":["### Explore Data"]},{"cell_type":"code","metadata":{"id":"Dwgrqlzmk0r_","colab_type":"code","colab":{}},"source":["meta_df = pd.read_csv(metadata_file_path)\n","meta_df.head()\n","\n","imgs_df = pd.read_csv(training_imgs_file_path)\n","imgs_df.columns\n","\n","lr_eye_pts = list(imgs_df.columns[:4])\n","img_data = [imgs_df.columns[-1]]\n","imgs_df[lr_eye_pts + img_data]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NwgHKe01w4vf","colab_type":"code","colab":{}},"source":["test_df = pd.read_csv(test_imgs_file_path)\n","test_df.columns\n","test_df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hFB0Uiq-4RUN","colab_type":"text"},"source":["### Build Architecture"]},{"cell_type":"code","metadata":{"id":"CSHB38pzv3Vt","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Create the Unet architecture\n","#----------------------------------------------------\n","class ArchFacialKeypoints():\n","  def __init__(self):\n","    self.model = None\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def unet_model(self):\n","    # \n","    self.model = None\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def compile_model(self, optimizer, loss, metrics):\n","    self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","  # ----------------------------\n","  # Returns a History object. History.history attribute is a record of training loss values and metrics\n","  # values at successive epochs, as well as validation loss values and validation metrics values (if applicable).\n","  # ----------------------------\n","  def train_model(self, X_train, y_train, num_epochs):\n","    return self.model.fit(X_train, y_train, epochs=num_epochs, batch_size=200, verbose=1, validation_split=0.2)\n","\n","  # ----------------------------\n","  # Create a trivial model\n","  # Accept a 96x96 grayscale image as input, and output points with 30 entries,\n","  # for the predicted (horizontal and vertical) locations of 15 facial keypoints.\n","  # ----------------------------\n","  def create_trivial_model(self):\n","    model = Sequential()\n","\n","    model.add(Convolution2D(32, (5, 5), input_shape=(96, 96, 1), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","    self._conv_block(model, filters=64, drop_p=0.1)\n","    self._conv_block(model, filters=128, drop_p=0.2)\n","    self._conv_block(model, filters=30, drop_p=0.3)\n","\n","    model.add(Flatten())\n","\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(128, activation='relu'))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(30))\n","\n","    self.model = model\n","\n","  def _conv_block(self, model, filters, drop_p):\n","    model.add(Convolution2D(filters, kernel_size=(3, 3), activation='relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(drop_p))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HsjygTKEfwvV","colab_type":"text"},"source":["### Define Data Preparation class"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rQubDndlsefc","colab":{}},"source":["class DataPrep():\n","  def __init__(self, data_file, test=False):\n","    self.data_file = data_file\n","    self.test = test\n","  \n","  def prep(self):\n","    data_df = self._load(self.data_file)\n","    np_img, np_pts = self._extract(data_df)\n","    np_img, np_pts = self._scale(np_img, np_pts)\n","    self.np_img, self.np_pts = np_img, np_pts\n","\n","  def _load(self, data_file):\n","    data_df = pd.read_csv(data_file)\n","    data_df = data_df.dropna()  # drop all rows that have missing values in them\n","    return data_df\n","\n","  def _extract(self, data_df):\n","    img_df = data_df['Image']\n","    img_df = img_df.apply(lambda im_txt: np.fromstring(im_txt, sep = ' '))\n","    np_img = np.stack(img_df.values, axis=0)\n","    img_w = int(math.sqrt(np_img.shape[1]))\n","    np_img = np_img.reshape(np_img.shape[0], img_w, -1)\n","\n","    np_pts = None\n","    if (not self.test):\n","      pts_cols = data_df.columns[:-1]\n","      np_pts = data_df[pts_cols].values\n","\n","    return np_img, np_pts\n","\n","  def _scale(self, np_img, np_pts):\n","    # scale pixel values to [0, 1]\n","    np_img = np_img / 255.\n","\n","    if (not self.test):\n","      # scale target coordinates to [-1, 1]\n","      np_pts = (np_pts - 48) / 48\n","    else:\n","      np_pts = None\n","\n","    return np_img, np_pts\n","\n","  def _unscale(self, np_img, np_pts):\n","    # reverse the scale before display\n","    np_img = np_img * 255.\n","    np_pts = (np_pts * 48.) + 48.\n","    return np_img, np_pts \n","\n","  def _show_img_pts(self, ax, img, pts_x, pts_y):\n","    ax.axis('off')\n","    ax.imshow(img)\n","    ax.scatter(pts_x, pts_y, color='red')\n","\n","  def _show_grid(self, disp_img, disp_pts):\n","    num_imgs = disp_img.shape[0]\n","    num_cols = 10\n","    num_rows = int (math.ceil (num_imgs / num_cols))\n","    figsize=(num_cols * 3, num_rows * 3)\n","    fig,axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n","\n","    for img, pts, ax in zip (disp_img, disp_pts, axes.flat):\n","      pts_x, pts_y= pts.T\n","      self._show_img_pts(ax, img, pts_x, pts_y)\n","\n","    for i in range(num_imgs, len(axes.flat)): axes.flat[i].set_visible(False)\n","\n","  def display(self, np_img=None, np_pts=None, idxs=None):\n","    np_img = np_img if (np_img is not None) else self.np_img\n","    np_pts = np_pts if (np_pts is not None) else self.np_pts\n","    if (idxs is None):\n","      num_batch = 16\n","      idxs = list(range(num_batch))\n","\n","    disp_img = np_img[idxs]\n","    disp_pts = np_pts[idxs]\n","    disp_img, disp_pts = self._unscale(disp_img, disp_pts)\n","    disp_pts = disp_pts.reshape(disp_pts.shape[0], -1, 2)\n","    self._show_grid(disp_img, disp_pts)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l0s6fY5mK4hf","colab_type":"text"},"source":["### Define Facial Keypoint application class "]},{"cell_type":"code","metadata":{"id":"t5hWAJmEvUEf","colab_type":"code","colab":{}},"source":["#----------------------------------------------------\n","# Facial Keypoints Application\n","#----------------------------------------------------\n","class AppFacialKeypoints():\n","\n","  def __init__(self):\n","    self._arch = None\n","    self.db = None\n","\n","  # ----------------------------\n","  # Pre-process images by reducing the image size to a manageable size\n","  # ----------------------------\n","  def pre_process_data(self):\n","    pass\n","\n","  # ----------------------------\n","  # Load the data using the Data Prep class\n","  # ----------------------------\n","  def load_data(self, data_file):\n","    self.db = DataPrep(data_file)\n","    self.db.prep()\n","\n","  # ----------------------------\n","  # Create the architecture\n","  # ----------------------------\n","  def create_arch(self):\n","    self._arch = ArchFacialKeypoints()\n","    self._arch.unet_model()\n","    return self._arch\n","\n","  # ----------------------------\n","  # Create a simplified architecture with a very basic upsampling decoder\n","  # ----------------------------\n","  def create_trivial_arch(self):\n","    self._arch = ArchFacialKeypoints()\n","    self._arch.create_trivial_model()\n","    return self._arch\n","\n","  # ----------------------------\n","  # Train the model\n","  # ----------------------------\n","  def run_train(self, num_epochs=1):\n","    # Load training set\n","    X_train, y_train = self.db.np_img, self.db.np_pts\n","    X_train, y_train = shuffle(X_train, y_train, random_state=42)  # shuffle train data\n","    X_train = X_train.reshape(-1, 96, 96, 1) # reshape each image as 96 x 96 x 1\n","\n","    self._arch.compile_model(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n","    hist = self._arch.train_model(X_train, y_train, num_epochs)\n","\n","  # ----------------------------\n","  # ----------------------------\n","  def run_predict(self, test_file):\n","    test_prep = DataPrep(test_file, test=True)\n","    test_prep.prep()\n","\n","    # Predict the keypoints using the model\n","    test_imgs = test_prep.np_img[:2]\n","    test_imgs = test_imgs.reshape(-1, 96, 96, 1)\n","    test_pts = self._arch.model.predict(test_imgs)\n","    return test_imgs, test_pts\n","\n","  def OLD_run_predict(self, test_file):\n","    valid_dl = self.db.valid_dl\n","    self._arch.model.eval()\n","    device = list(self._arch.model.parameters())[0].device\n","\n","    inps, outs, targs = [], [], []\n","    with torch.no_grad():\n","      for _, (xb, yb) in enumerate(valid_dl):\n","        xb = xb.to(device)\n","        yhat = self._arch.model(xb)\n","\n","        for x, y, p in zip (xb, yb, yhat):\n","          inps.append(x.cpu())\n","          outs.append(p.cpu())\n","          targs.append(y.cpu())\n","    return inps, outs, targs"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dfxbny7edl-","colab_type":"text"},"source":["### Load Data"]},{"cell_type":"code","metadata":{"id":"SmGSUSu3BpUM","colab_type":"code","colab":{}},"source":["fk_app = AppFacialKeypoints()\n","fk_app.load_data(training_imgs_file_path)\n","fk_app.db.np_img.shape, fk_app.db.np_pts.shape, fk_app.db.np_img.dtype, fk_app.db.np_pts.dtype\n","fk_app.db.display()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ilWZNBIumuZA","colab_type":"text"},"source":["### Test run the model"]},{"cell_type":"code","metadata":{"id":"TRR-tqc2mzXI","colab_type":"code","colab":{}},"source":["fk_app.create_trivial_arch()\n","#fk_app._arch.model\n","fk_app.run_train(num_epochs=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJG8ZDagK4NT","colab_type":"code","colab":{}},"source":["fk_app.run_train(num_epochs=70)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pl4UCVNyX1y","colab_type":"code","colab":{}},"source":["test_imgs, test_pts = fk_app.run_predict(test_imgs_file_path)\n","test_imgs.shape, test_pts.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BgFfTq_0acT","colab_type":"code","colab":{}},"source":["img_i = 0\n","foo_img = test_imgs[img_i].reshape(96, 96)\n","foo_img = foo_img * 255.\n","foo_pts = test_pts[img_i].reshape(-1, 2)\n","foo_pts = (foo_pts * 48.) + 48.\n","foopx, foopy = foo_pts.T\n","#foop[img_i], foo_pts, foopx, foopy\n","simg(foo_img, foopx, foopy)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SoybDeMCPC04","colab_type":"text"},"source":["### Video processing utilities"]},{"cell_type":"code","metadata":{"id":"cKzz2JifPGBa","colab_type":"code","colab":{}},"source":["import cv2\n","\n","#----------------------------------------------------\n","# Video Download and Display utility\n","#----------------------------------------------------\n","class ShowVid():\n","  def __init__(self, video_file=None):\n","    self.youtube_id = None\n","    self.video_file = video_file\n","\n","  #----------------------------------------------------\n","  # Get stats on a Youtube video\n","  #----------------------------------------------------\n","  def _youtube_info(self, youtube):\n","    # get video information\n","    print (f'ID: {youtube.video_id}, Title: {youtube.title}, Length: {youtube.length}\\n')\n","\n","    # Get video stream formats\n","    stream = youtube.streams.all()\n","    print ('Streams:\\n')\n","    for i in stream:\n","      print(i)\n","\n","    # Thumbnail image URL\n","    return youtube.thumbnail_url\n","\n","  #----------------------------------------------------\n","  # Download a video from Youtube to a local file using Pytube library\n","  #----------------------------------------------------\n","  def youtube_download(self, video_id, download_dir, video_stem):\n","    # Install pytube\n","    !pip install pytube3\n","    import pytube\n","\n","    # Get Pytube's youtube object for the video\n","    video_url = f'https://www.youtube.com/watch?v={video_id}'\n","    youtube = pytube.YouTube(video_url)\n","    \n","    # Show some stats about the video\n","    self._youtube_info(youtube)\n","\n","    # Choose the first stream format (format contains mime_type, resolution, fps, vcodec, acodec)\n","    video = youtube.streams.first()\n","\n","    # Download the video\n","    self.video_file = video.download(download_dir, video_stem)\n","    self.youtube_id = video_id\n","\n","    return self.video_file\n","\n","  #----------------------------------------------------\n","  # Extract just the first few 'duration' seconds of a video\n","  #----------------------------------------------------\n","  def extract_short(self, short_file, duration):\n","    assert (self.video_file is not None)\n","    !ffmpeg -y -loglevel info -i {self.video_file} -t {duration} {short_file}\n","    self.video_file = short_file\n","\n","  #----------------------------------------------------\n","  # Display a locally downloaded video file\n","  #----------------------------------------------------\n","  def show_mp4(self, width=640, height=480):\n","    import io\n","    import base64\n","    from IPython.display import HTML\n","\n","    assert (self.video_file is not None)\n","    video_file = self.video_file\n","\n","    video_encoded = base64.b64encode(io.open(video_file, 'rb').read())\n","    return HTML(data='''<video width=\"{0}\" height=\"{1}\" alt=\"test\" controls>\n","                          <source src=\"data:video/mp4;base64,{2}\" type=\"video/mp4\" />\n","                        </video>'''.format(width, height, video_encoded.decode('ascii')))\n","    \n","  #----------------------------------------------------\n","  # Display a video on Youtube\n","  #----------------------------------------------------\n","  def show_youtube_video(self, youtube_id):\n","    from IPython.display import YouTubeVideo\n","    YouTubeVideo(youtube_id)\n","\n","  #----------------------------------------------------\n","  # Read a video frame-by-frame and call a callback function to process\n","  # each frame. Stop when we have completed the given number of frames.\n","  # Write an output video file if required.\n","  #----------------------------------------------------\n","  def process_video(self, process_cb, num_frames, vid_out_name=None):\n","    assert (self.video_file is not None)\n","    video_file = self.video_file\n","\n","    i_frames = 0\n","    vid = cv2.VideoCapture(str(video_file))\n","\n","    # Get video width, height and frames per second\n","    width, height = int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)), int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","    fps = vid.get(cv2.CAP_PROP_FPS)\n","    print (f'Video size {width}x{height} at {fps} fps')\n","\n","    if (vid_out_name is not None):\n","      # Write an AVI file and convert to MP4 later\n","      vid_out_avi = f'{vid_out_name}.avi'\n","      vid_out = cv2.VideoWriter(vid_out_avi, cv2.VideoWriter_fourcc(*'MJPG'), fps, (width, height))\n","\n","    while(vid.isOpened()):\n","      ret, frame = vid.read()\n","      \n","      # Bail out when the video file ends\n","        if not ret:\n","            break\n","\n","      frame_out = process_cb(frame)\n","\n","      if (vid_out_name is not None):\n","        vid_out.write(frame_out)\n","\n","      i_frames += 1\n","      if (i_frames > num_frames):\n","        vid.release()\n","        if (vid_out_name is not None):\n","          vid_out.release()\n","          # convert AVI to MP4\n","          vid_out_mp4 = f'{vid_out_name}.mp4'\n","          !ffmpeg -y -loglevel info -i {vid_out_avi} {vid_out_mp4}\n","        break"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UXRpjqtnUAHN"},"source":["### Download Facial Video. Define Video IDs, File Paths"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jZTiSdE8UAHP","colab":{}},"source":["root_path = Path.cwd()\n","video_dir = root_path/'videos'\n","video_dir.mkdir(exist_ok=True)\n","\n","video_stem = 'myvideo'\n","short_file = video_dir/'short.mp4'\n","\n","obama_youtube_id = '9V7yi2Q8mJo'\n","tom_jerry_youtube_id = 'tXOIvjbNhts'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"egMhnwUDbAWq","colab_type":"code","colab":{}},"source":["facial_video_id = obama_youtube_id\n","facial_video_dir = video_dir\n","facial_stem = video_stem\n","facial_short = short_file\n","\n","#sv.show_youtube_video(facial_video_id)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xtSLZdJOSYn8","colab_type":"code","colab":{}},"source":["sv = ShowVid()\n","\n","facial_video = sv.youtube_download(facial_video_id, facial_video_dir, facial_stem)\n","sv.extract_short(facial_short, duration=10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-1x62E1D5nXG","colab_type":"code","colab":{}},"source":["sv.show_mp4(width=480, height=360)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K3I4QFWwBAeB","colab_type":"code","colab":{}},"source":["# Show a local file (not downloaded from youtube)\n","sv_tj = ShowVid(video_dir/'tj.mp4')\n","sv_tj.show_mp4(width=480, height=360)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CppGT_99RyiM","colab_type":"text"},"source":["### Video facial keypoints"]},{"cell_type":"code","metadata":{"id":"2vm3y6PAbxOU","colab_type":"code","colab":{}},"source":["!wget 'https://raw.githubusercontent.com/acl21/Selfie_Filters_OpenCV/master/cascades/haarcascade_frontalface_default.xml' -P facial\n","face_cascade = 'facial/haarcascade_frontalface_default.xml'\n","from functools import partial\n","import matplotlib.patches as patches\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GH8Su1odSZ0s","colab_type":"code","colab":{}},"source":["import matplotlib.patches as patches\n","def simg(img, pts_x=None, pts_y=None, bbox=None, figsize=(20, 5)):\n","    _, ax = plt.subplots(1, 1, figsize=figsize)\n","    ax.axis('off')\n","    ax.imshow(img)\n","    if (pts_x is not None):\n","      ax.scatter(pts_x, pts_y, color='red')\n","    if (bbox is not None):\n","      # Create a Rectangle patch\n","      x, y, w, h = bbox\n","      rect = patches.Rectangle((x,y),w,h, linewidth=1, edgecolor='r',facecolor='none')\n","\n","      # Add the patch to the Axes\n","      ax.add_patch(rect)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PNzGA40Eut0","colab_type":"code","colab":{}},"source":["def add_keypts(frame, face_coords, resized_face_pts):\n","  face_x, face_y, face_w, face_h = face_coords\n","\n","  face_img = frame[face_y: face_y + face_h, face_x: face_x + face_w]\n","  resized_face_img = cv2.resize(face_img, (96, 96), interpolation = cv2.INTER_AREA)\n","\n","  resized_face_pts = resized_face_pts.reshape(-1, 2)\n","  for pt_x, pt_y in resized_face_pts:\n","    green = (0,255,0)\n","    cv2.circle(resized_face_img, (pt_x, pt_y), 1, green, 1)\n","\n","  resized_face_img = cv2.resize(resized_face_img, (face_w, face_h), interpolation = cv2.INTER_CUBIC)\n","  frame[face_y:face_y + face_h, face_x:face_x + face_w] = resized_face_img\n","  return (frame)\n","\n","def myfunc(frame, cascade):\n","  # Detect faces using the haar cascade object\n","  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","  faces = cascade.detectMultiScale(gray, 1.25, 6)\n","\n","  for (face_x, face_y, face_w, face_h) in faces:\n","    gray_face = gray[face_y: face_y + face_h, face_x: face_x + face_w]\n","        \n","    # Normalize to match the input format of the model - Range of pixel to [0, 1]\n","    gray_normalized = gray_face / 255\n","\n","    # Resize it to 96x96 to match the input format of the model\n","    gray_resized = cv2.resize(gray_normalized, (96, 96), interpolation = cv2.INTER_AREA)\n","    gray_resized = gray_resized.reshape(1, 96, 96, 1)\n","\n","    # Predict the keypoints using the model\n","    keypoints = fk_app._arch.model.predict(gray_resized)\n","\n","    # De-Normalize the keypoints values\n","    keypoints = keypoints * 48 + 48\n","\n","    frame = add_keypts(frame, (face_x, face_y, face_w, face_h), keypoints)\n","\n","  return frame\n","\n","# Face cascade to detect faces\n","face_cascade = cv2.CascadeClassifier('facial/haarcascade_frontalface_default.xml')\n","facial_func = partial(myfunc, cascade=face_cascade)\n","svf = ShowVid(facial_short)\n","svf.process_video(facial_func, 250, 'outvid')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z86hDwec63s5","colab_type":"code","colab":{}},"source":["svo = ShowVid('outvid.mp4')\n","svo.show_mp4(width=720, height=720)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5q2FjKJmsbE","colab_type":"text"},"source":["### Temp"]},{"cell_type":"code","metadata":{"id":"QWjaeA5Blwwr","colab_type":"code","colab":{}},"source":["fk_app.db.display()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lharn0bO5v1r","colab_type":"code","colab":{}},"source":["def simg(img, pts_x, pts_y):\n","    _, ax = plt.subplots(1, 1)\n","    ax.axis('off')\n","    ax.imshow(img)\n","    ax.scatter(pts_x, pts_y, color='red')\n","\n","img_i = 3\n","foo_pts = foop[img_i].reshape(-1, 2)\n","foopx, foopy = foo_pts.T\n","#foop[img_i], foo_pts, foopx, foopy\n","simg(foon[img_i], foopx, foopy)"],"execution_count":0,"outputs":[]}]}