{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nimport sys #access to system parameters\nprint(\"Python version: {}\". format(sys.version))\n\nimport numpy as np # linear algebra\nprint(\"NumPy version: {}\". format(np.__version__))\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nprint(\"pandas version: {}\". format(pd.__version__))\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\n# Suppress warnings from pandas\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#plt.style.use('fivethirtyeight')\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83439f21aac029b33687c547b8a2c0540242f75c"
      },
      "cell_type": "code",
      "source": "def get_feature_names(df):\n    return df.columns.values\n\n# Print a summary of all the features in the dataset\ndef data_summary (df, features_dict):\n    # Get Feature names\n    print('Features are ', get_feature_names(df))\n    \n    print('_'*40, 'Categories of all features', '_'*20)\n    print (features_dict)\n    \n    print('_'*40, 'Datatypes of all features', '_'*20)\n    # Print datatypes of all the features\n    df.info()\n\n    # Stats of the features\n    # Check the quantiles and min/max to see if any of the features have anomalies or outliers\n    df.describe(include = 'all')\n    \ndef data_preview (df):\n    #df.head()\n    #df.tail()\n    df.sample(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "103a6ae9d63c8fb947bc9eba9b48a9ee01f00467"
      },
      "cell_type": "code",
      "source": "# Get a list of all the unique values for a column\ndef get_unique_values (df, target):\n    return df[target].unique()\n\n# Get a count of unique values for 'Object' columns\ndef get_unique_counts (df):\n    return (df.select_dtypes('object').apply(pd.Series.nunique, axis = 0))\n\n# Print the most frequent values of each categorical column\ndef count_freq (df, cat_cols):\n    for col in cat_cols:\n        print (df[col].value_counts().nlargest())\n\n# Get all the datatypes in the given df\ndef get_dtypes (df):\n    dtype_df = df.dtypes.reset_index()\n    dtype_df.columns = [\"Feature\", \"Column Type\"]\n    return (dtype_df)\n        \n# Get all the number-type columns in the dataframe\ndef get_numeric_columns (df):\n    return (df.select_dtypes('number').columns)\n\n# Function to calculate missing values by column\ndef missing_values_summary(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() / len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84ceb1fcdda1af1ccb6c407fea7c6c9aac7b5620"
      },
      "cell_type": "code",
      "source": "# VISUALISATION PLOTTING\n#---------------------\n\nclass Plotter(object):\n    # Plots only the subplots in the grid for which we have data\n    def plot_grid (self, ptype, df, features, target=None):\n        grid_size = 4\n        fig = plt.figure(figsize = (18, 18))\n        plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n\n        for i, feature in enumerate(features):\n            ax = fig.add_subplot(grid_size, grid_size, i + 1)\n            plot_func = self._get_plottype (ptype)\n            plot_func (df, feature, target)\n    \n    def _get_plottype (self, ptype):\n        \"\"\"Dispatch method\"\"\"\n        method_name = '_plot_' + ptype\n        # Get the method from 'self'. Default to a lambda.\n        method = getattr(self, method_name, lambda: \"Invalid plot type\")\n        # Return the method\n        return method\n \n    def _plot_count(self, df, x, y):\n        sns.countplot(x=x, data=df)\n \n    def _plot_dist(self, df, x, y):\n        #sns.distplot(df[x], kde=False)\n        sns.distplot(df[x])\n \n    def _plot_bar(self, df, x, y):\n        sns.barplot(x=x, y=y, data=df)\n\n    def _plot_multicol(self, df, x, y):\n        sns.barplot(df.index, df[x])\n        \n    def _plot_series(self, df, x, y):\n        sns.barplot(df.index, df.values)\n        \n    def _plot_series_horiz(self, df, x, y):\n        sns.barplot(df.values, df.index)\n        \n    def _plot_box(self, df, x, y):\n        sns.boxplot(df[x], df[y])\n        \n    def _plot_reg(self, df, x, y):\n        sns.regplot(x=x, y=y, data=df)\n\n    def _plot_scatter(self, df, x, y):\n        sns.scatterplot(x=x, y=y, data=df)\n        \ndef plot_corr(df):\n    # Heatmap of correlations\n    plt.figure(figsize = (8, 6))\n    sns.heatmap(df.corr(), vmin = -0.25, annot = True, vmax = 0.6, cmap = plt.cm.RdYlBu_r, fmt='.2f', linewidths=.05)\n    plt.title('Correlation Heatmap');",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1eafdd69f59b685aa595cf45d038bb036386457a"
      },
      "cell_type": "code",
      "source": "# BIVARIATE\n#---------------------\n\n# Plots bar plots between a feature and a target pair\ndef plot_pair_target (df, features, target):\n    grid_size = 4\n    fig = plt.figure(figsize = (15, 15))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n        sns.barplot(x = feature, y = target, data=df, ax = ax)\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Totals'.format(feature), fontsize=16)\n\n# Plot Stacked histograms of each feature vs target\ndef plot_stackhist(df, features, target):\n    grid_size = 4\n    fig = plt.figure(figsize = (20, 20))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    # Get list of unique values of the target\n    target_values = get_unique_values (df, target)\n        \n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n        # For each value of the target\n        dist = [df[df[target]==val][feature] for val in target_values]\n        ax.hist(x = dist, stacked=True, label = target_values)\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Distribution'.format(feature), fontsize=16)\n        ax.legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1382d27b7b885a255aeafb8bfce4e84a31e35447"
      },
      "cell_type": "code",
      "source": "def plot_pair(df, features):\n    # Pair-wise Scatter Plots\n    pp = sns.pairplot(df[features], size=1.8, aspect=1.8,\n                  plot_kws=dict(edgecolor=\"k\", linewidth=0.5),\n                  diag_kind=\"kde\", diag_kws=dict(shade=True))\n\n    fig = pp.fig \n    fig.subplots_adjust(top=0.93, wspace=0.3)\n    t = fig.suptitle('Pairwise Plots', fontsize=14)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d4b1e491f2dcff26c73c33794be35c71bcc63eda"
      },
      "cell_type": "code",
      "source": "def data_stats (df):\n    p = Plotter()\n    \n    # Get all the datatypes\n    dd = get_dtypes (df)\n    p.plot_grid ('count', dd, ['Column Type'])\n    \n    # Unique value counts\n    ud = get_unique_counts(train_df)\n    p.plot_grid ('series', ud, ['dummy'])\n\n    # Missing value counts\n    miss_val = missing_values_summary(train_df)\n    cols = get_feature_names(miss_val)\n    p.plot_grid ('multicol', miss_val, cols)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59c924d29bf12a4d2d39681f5b614e2a23df1314"
      },
      "cell_type": "code",
      "source": "def data_distrib (df, cat_cols, num_cols):\n    p = Plotter()\n    #Frequency of categorical values\n    p.plot_grid ('count', df, cat_cols)\n\n    # Histograms of Numerical features\n    p.plot_grid ('dist', df, num_cols)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cf419be5c51a475bb74d33fc0526b748d33cd6dc"
      },
      "cell_type": "code",
      "source": "def is_categorical(df, col):\n    return df[col].dtype.name == 'category'\n\ndef data_target (df, cat_cols, num_cols, target):\n    p = Plotter()\n    if (is_categorical (df, target)):\n        plot_pair_target (train_df, cat_cols, target)\n        #plot_stackhist(train_df, ['Pclass', 'SibSp', 'Parch', 'Fare', 'Age', 'Sex'], 'Survived')\n    else:\n        p.plot_grid ('scatter', df, num_cols, target)\n        p.plot_grid ('box', df, cat_cols, target)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b59754d66b35dff13600d7c43c3a113bfc5b7062"
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import Imputer\ndef impute_missing_values (df, cols):\n    # Get the numeric columns and impute average values for them\n    num_df = df[cols].select_dtypes(exclude=[object])\n    imr = Imputer(missing_values='NaN', strategy='mean', axis=0)\n    imr = imr.fit(num_df)\n    df[num_df.columns] = imr.transform(num_df.values)\n    \n    # Get the categorical columns and impute the mode values for them\n    cat_df = df[cols].select_dtypes('object')\n    cat_df.fillna(cat_df.mode().iloc[0], inplace = True)\n    df[cat_df.columns] = cat_df\n\n# Remove columns which are not selected for the model\ndef remove_cols (dfs, cols):\n    for df in dfs:\n        #print(\"DF shape before\", df.shape)\n        df.drop(cols, axis=1, inplace = True)\n        #print(\"DF shape after\", df.shape)\n\n# Function to fill all missing numeric values with 0\ndef fill_missing (df):\n    # Iterate through the numeric columns \n    for col in df.select_dtypes('number').columns:\n        df[col] = df[col].fillna(0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ae29a6c8db9faae2d7f3f350646d69be0b32013"
      },
      "cell_type": "code",
      "source": "# Convert a numeric column to a categorical column based on ranges defined by bins\ndef num_to_cat (df, num_col, cat_col, bins, labels):\n    df[cat_col] = pd.cut(df[num_col], bins=bins, labels=labels)\n    \nfrom sklearn.preprocessing import LabelEncoder\n\n# Label encode categorical columns\ndef encode_cat (df, cat_col, enc_col):\n    gle = LabelEncoder()\n    # Fill missing values with a dummy string, otherwise fit_transform throws an error\n    #cat_labels = gle.fit_transform(df[cat_col].fillna('Empty'))\n    cat_labels = gle.fit_transform(df[cat_col])\n    #cat_mappings = {index: label for index, label in enumerate(gle.classes_)}\n    df[enc_col] = cat_labels\n    \n# One-hot encode categorical columns\ndef encode_cat_onehot (df, cat_col):\n    # Drop the first encoded value to avoid the \"dummy variable trap\"\n    onehot_df = pd.get_dummies(df[cat_col], drop_first=True)\n    #onehot_df = pd.get_dummies(df[cat_col].fillna('Empty'), drop_first=True)\n    return (pd.concat([df, onehot_df], axis=1))\n\n# Min-Max Scaling\nfrom sklearn import preprocessing \ndef minmax_scaling (df, cols):\n    # Get np array for scaling. \n    # Adjust to 2D if cols contains only a single element\n    x = df[cols].values\n    if (x.ndim == 1):\n        x = x.reshape(-1,1)\n    \n    min_max_scaler = preprocessing.MinMaxScaler(feature_range =(0, 1)) \n    x_after_min_max_scaler = min_max_scaler.fit_transform(x)\n    df[cols] = x_after_min_max_scaler",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef33027cfdabad116b17ed8a467da3f7eb45955a"
      },
      "cell_type": "code",
      "source": "# Plot correlations of features with the target feature\ndef correlations_target(df, target):\n    return (df.corr()[target].sort_values())\n\n# Plot correlations with target and also heatmap of correlation between\n# all features\ndef data_correlations (df, num_cols, target):\n    tc = correlations_target(df, target)\n    p = Plotter()\n    p.plot_grid ('series_horiz', tc, ['dummy'], 'dummy')\n    \n    # Heatmap of correlations between all features\n    plot_corr (df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "952bdb63e85dcb805597668389a707a9b76ee59e"
      },
      "cell_type": "code",
      "source": "def agg_numeric(df, group_var, stats, df_name):\n    \"\"\"Aggregates the numeric values in a dataframe. This can\n    be used to create features for each instance of the grouping variable.\n    \n    Parameters\n    --------\n        df (dataframe): \n            the dataframe to calculate the statistics on\n        group_var (string): \n            the variable by which to group df\n        df_name (string): \n            the variable used to rename the columns\n        \n    Return\n    --------\n        agg (dataframe): \n            a dataframe with the statistics aggregated for \n            all numeric columns. Each instance of the grouping variable will have \n            the statistics (mean, min, max, sum; currently supported) calculated. \n            The columns are also renamed to keep track of features created.\n    \n    \"\"\"\n    # Group by the specified variable and calculate the statistics\n    agg = df.groupby(group_var).agg(stats).reset_index()\n    \n    # Need to create new column names\n    columns = [group_var]\n\n    # Iterate through the variables names\n    for var in agg.columns.levels[0]:\n        # Skip the grouping variable\n        if var != group_var:\n            # Iterate through the stat names\n            for stat in agg.columns.levels[1][:-1]:\n                # Make a new column name for the variable and stat\n                columns.append('%s_%s_%s' % (df_name, var, stat))\n\n    agg.columns = columns\n    return agg",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fb01080254038e422b938b16b04b422986af7374"
      },
      "cell_type": "code",
      "source": "def copy_df (df):\n    #Create a copy of data before manipulation.Python assignment or equal passes by \n    # reference vs values, so we use the copy function\n     return (df.copy(deep = True))\n\n# Get intersection between two dataframes based on values in a column\ndef intersect_col (df1, df2, col):\n    return pd.Series (list (set(df1[col]) & set (df2[col])))\n\n# Get difference between two dataframes based on values in a column\ndef diff_col (df1, df2, col):\n    return df1[~df1[col].isin(df2[col])]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d4961d09176fb422b19d2d8d18c2d0555f59932"
      },
      "cell_type": "code",
      "source": "# Get polynomial features\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef polynomial_features (df, test_df):\n    poly = PolynomialFeatures(degree=2).fit(df)\n    poly_df = poly.transform(df)\n    test_poly_df = None\n    if (test_df):\n        test_poly_df = poly.transform(test_df)\n    return (poly_df, test_poly_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d9125391bd002f96f15cfdbc6952a0ea3cb1ea85"
      },
      "cell_type": "code",
      "source": "# Plot learning curve for different sizes of the training samples\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):    \n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n\n    if ylim is not None:\n        plt.ylim(*ylim)\n    \n    # Create Cross Validation training and test scores for various training set sizes\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    \n    # Create means and standard deviations of training set scores\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    \n    # Create means and standard deviations of test set scores\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Draw curve lines\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Validation score\")\n\n    # Draw bands\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    \n    # Titles, labels, grid, legend etc\n    plt.title(title)\n    plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n    plt.grid()\n\n    return plt",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "123da7f9b130751b8ea8e457c4f72a996254a043"
      },
      "cell_type": "code",
      "source": "# Plot validation curve for different hyperparameter values\ndef plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    # Create validation scores\n    train_scores, test_scores = validation_curve(estimator, X, y, param_name, param_range, cv)\n    \n    # Create means and standard deviations of training set scores\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    \n    # Create means and standard deviations of test set scores\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    \n     # Draw curve lines\n    plt.plot(param_range, train_mean, color='r', marker='o', markersize=5, label='Training score')\n    plt.plot(param_range, test_mean, color='g', linestyle='--', marker='s', markersize=5, label='Validation score')\n    \n    # Draw bands\n    plt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='r')\n    plt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='g')\n    \n    # Titles, labels, grid, legend etc\n    plt.title(title)\n    plt.xlabel('Parameter values'), plt.ylabel('Accuracy Score'), plt.legend(loc='best') \n    plt.grid() \n    plt.xscale('log')\n    plt.ylim(ylim)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6869fe57b14d5b95a731383b2272e4066a30331d"
      },
      "cell_type": "code",
      "source": "def run_model (X, y):\n    # Fit logistic regression\n    logreg = LogisticRegression()\n    logreg.fit(X, y)\n\n    # Model performance\n    scores = cross_val_score(logreg, X, y, cv=10)\n    return (logreg, scores)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77fc5b5542e8c8f93f57e62a535735c1bb57a1f8"
      },
      "cell_type": "code",
      "source": "# Run a cross-validation model with the given data, and plot the learning \n# and validation curves\ndef validate_model (X, y):\n    # Split data into training and test set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)\n\n    logreg, scores = run_model (X_train, y_train)\n    print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))\n\n    # Figure and axes to plot learning and validation curves\n    fig, axs = plt.subplots(1,2, figsize = (12, 5))\n    \n    # Plot learning curves\n    title = \"Learning Curves (Logistic Regression)\"\n    cv = 10\n    plt.sca(axs[0]) # Set the current axes to first subplot\n    plot_learning_curve(logreg, title, X_train, y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=1);\n\n    # Plot validation curve\n    title = 'Validation Curve (Logistic Regression)'\n    param_name = 'C'\n    param_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0] \n    cv = 10\n    plt.sca(axs[1]) # Set the current axes to next subplot\n    plot_validation_curve(estimator=logreg, title=title, X=X_train, y=y_train, param_name=param_name,\n                          ylim=(0.5, 1.01), param_range=param_range);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "133cb1a74d6f6e917fbfa2931274d5f8b7fcc61d"
      },
      "cell_type": "code",
      "source": "# Select features using chi-squared test\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n# Select features\ndef selected_df (X, y, k):\n    select = SelectKBest(score_func=chi2, k=k)\n    select.fit(X, y)\n    X_selected = select.transform(X)\n    return (X_selected)\n\n## Get score using models with feature selection\ndef feature_select (X, y, base_score):\n    highest_score = base_score\n    for i in range(1, X.shape[1]+1, 1):\n        # Select i features\n        X_selected = selected_df (X, y, i)\n\n        # Model with i features selected\n        _, scores = run_model (X_selected, y)\n        print('CV accuracy (number of features = %i): %.3f +/- %.3f' % (i, \n                                                                         np.mean(scores), \n                                                                         np.std(scores)))\n\n            # Save results if best score\n        if np.mean(scores) > highest_score:\n            highest_score = np.mean(scores)\n            std = np.std(scores)\n            k_features_highest_score = i\n        elif np.mean(scores) == highest_score:\n            if np.std(scores) < std:\n                highest_score = np.mean(scores)\n                std = np.std(scores)\n                k_features_highest_score = i\n\n    # Print the number of features\n    print('Number of features when highest score: %i' % k_features_highest_score)\n    return (k_features_highest_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec88783fc9219570bd71ed7ed5cdc37dc196f84c"
      },
      "cell_type": "code",
      "source": "# UNUSED - These functions have been superseded by better implementations and are\n# no longer used\n# --------------------------\n\n# Plot a bar chart of a Series\ndef plot_series (df):\n    fig = plt.figure(figsize = (5, 5))\n    ax = plt.axes()\n    \n    data_x, data_y = df.index, df.values\n    ax.bar(data_x, data_y, color='lightseagreen')\n    ax.set_xlabel(\"Features\")\n    ax.set_ylabel(\"Counts\") \n    ax.set_title('Unique', fontsize=16)\n\n# Plots all the columns in a dataframe against the index\ndef plot_multicolumn (df, features):\n    grid_size = 4\n    fig = plt.figure(figsize = (15, 15))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n        data_x, data_y = df.index, df[feature].values\n        #ax.bar(data_x, data_y, color='teal')\n        sns.barplot (data_x, data_y)\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Totals'.format(feature), fontsize=16)\n\ndef plot_bar (df, features):\n    grid_size = 4\n    plt.subplots_adjust(top=.85, hspace=0.7, wspace=0.4)\n    \n    for i, feature in enumerate(features):\n        plt.subplot(grid_size, grid_size, i + 1)\n        #df.groupby(feature)['Survived'].agg('sum').nlargest().plot (kind = 'bar')\n        #df[feature].value_counts().plot (kind = 'bar')\n        ax = df[feature].value_counts().nlargest().plot.bar(\n            figsize=(15, 15),\n            color='mediumvioletred',\n            fontsize=12\n        )\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Totals'.format(feature), fontsize=16)\n\n# Plot a single column\ndef plot_column (df, col):\n    fig = plt.figure(figsize = (5, 5))\n    ax = plt.axes()\n    \n    data_x, data_y = df.index, df[col].values\n    ax.bar(data_x, data_y, color='mediumvioletred')\n    ax.set_xlabel(\"Features\")\n    ax.set_ylabel(\"Counts\") \n    ax.set_title(col, fontsize=16)\n\n# Plots only the subplots in the grid for which we have data\ndef plot_subplots (df, features):\n    grid_size = 4\n    fig = plt.figure(figsize = (15, 15))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n        ax.text(0.5, 0.5, str(i),\n                      fontsize=12, ha='center')\n        data = df[feature].value_counts().nlargest()\n        data_x, data_y = data.index, data.values\n        ax.bar(data_x, data_y, color='green')\n        #sns.barplot(data_x, data_y)\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Totals'.format(feature), fontsize=16)\n\ndef plot_hist(df, features):\n    grid_size = 4\n    fig = plt.figure(figsize = (15, 15))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n        ax.hist(df[feature], color='orange')\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Distribution'.format(feature), fontsize=16)\n        \n# PIVOT (PAIR)\n# Pivot the nominal, ordinal and discrete features against the Target\ndef plot_pivot_pair (df, features, target):\n    grid_size = 4\n    fig = plt.figure(figsize = (15, 15))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n        pdf = pivot_pair (df, feature, target)\n        ax.bar(pdf[feature], pdf[target], color='olive')\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Totals'.format(feature), fontsize=16)\n\n# Plots a complete grid\ndef plot_grid (df, features):\n    grid_size = 4\n    fig, axs = plt.subplots(grid_size, grid_size, figsize=(15, 15))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    for i, feature in enumerate(features):\n        row, col = i // grid_size, i % grid_size\n        ax = axs[row, col]\n        ax.text(0.5, 0.5, str(i),\n                      fontsize=12, ha='center')\n        data = df[feature].value_counts().nlargest()\n        data_x, data_y = data.index, data.values\n        ax.bar(data_x, data_y, color='mediumvioletred')\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Totals'.format(feature), fontsize=16)        \n       \ndef plot_box(df, features):\n    grid_size = 4\n    fig = plt.figure(figsize = (15, 15))\n    plt.subplots_adjust(top=.85, hspace=0.3, wspace=0.3)\n    \n    for i, feature in enumerate(features):\n        ax = fig.add_subplot(grid_size, grid_size, i + 1)\n        ax.boxplot(df[feature], showmeans = True, meanline = True)\n        ax.set_xlabel(\"Values\")\n        ax.set_ylabel(\"Counts\") \n        ax.set_title('{} Distribution'.format(feature), fontsize=16)\n        \nplot_pivot_pair (train_df, ['Sex', 'Embarked', 'SibSp', 'Parch', 'Pclass'], 'Survived')\n\n#plot_subplots (train_df, ['Survived', 'Sex', 'Embarked', 'Pclass', 'SibSp', 'Parch'])\n#plot_bar(train_df, ['Survived', 'Sex', 'Embarked', 'Pclass', 'SibSp', 'Parch'])\n#plot_hist(train_df, ['Survived', 'Pclass', 'SibSp', 'Parch', 'Fare'])\n\n# Distribution of all features\n#train_df.hist(bins=15, color='steelblue', edgecolor='black', linewidth=1.0,xlabelsize=8, ylabelsize=8, grid=False)    \n#plt.tight_layout(rect=(0, 0, 1.2, 1.2))\n\n# Function to calculate pair-wise correlations between all columns and a target column\n# Function replaced by correlations_target()\ndef target_corrs(df, target):\n\n    # List of correlations\n    corrs = []\n\n    # Iterate through the numeric columns \n    for col in df.select_dtypes('number').columns:\n        # Skip the target column\n        if col != target:\n            # Calculate correlation with the target\n            corr = df[target].corr(df[col])\n\n            # Append the list as a tuple\n            corrs.append((col, corr))\n            \n    # Sort by absolute magnitude of correlations\n    corrs = sorted(corrs, key = lambda x: abs(x[1]), reverse = True)\n    \n    return corrs\n\n# Pair-wise correlations with the Target feature\ntc = target_corrs (train_df, 'Survived')\n\n# Plots the disribution of a variable colored by value of the target\ndef kde_target(var_name, df, target):\n    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df[target].corr(df[var_name])\n    \n    plt.figure(figsize = (12, 6))\n    \n    # Get list of unique values of the target\n    target_values = get_unique_values (df, target)\n    \n    # Plot the distribution for each value of the target\n    for val in target_values:\n        sns.kdeplot(df.ix[df[target] == val, var_name], label = 'TARGET == ' + str(val))\n    \n    # label the plot\n    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n    plt.legend();\n    \n    # print out the correlation\n    print('The correlation between %s and the target %s is %0.4f' % (var_name, target, corr))\n    \n    # Print out median values\n    for val in target_values:\n        avg_val = df.ix[df[target] == val, var_name].median()\n        print('Median value of %s for %s value %d = %0.4f' % (var_name, target, val, avg_val))\n\n# Plot the Density plot of Age for different values of Survived\nkde_target('Age', train_df, 'Survived')\n\n# Pivot between a feature column and the target feature\ndef pivot_pair (df, col, target):\n    return (df[[col, target]]).groupby([col], as_index=False).mean().sort_values(by=target, ascending=False)\n\n# Pivot table to compute Target values for each combination between one or more rows and one or more columns\ndef pivot_multi (df, target, rows, cols):\n    return (df.pivot_table(target, index=rows, columns=cols))\n\ndef plot_num_facet (df, target, row_col, num_col):\n    g = sns.FacetGrid(df, col=target, row = row_col, height=2.2, aspect=1.6)\n    g.map(plt.hist, num_col, alpha=.5, bins=20)\n    g.add_legend()\n\nplot_num_facet (train_df, 'Survived', None, 'Age')\nplot_num_facet (train_df, 'Survived', 'Pclass', 'Age')\npivot_multi (train_df, 'Survived', ['Sex', 'Pclass'], 'Embarked')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9d50b05cf3bb92b2ef74b4e05ead86d578744226"
      },
      "cell_type": "markdown",
      "source": "## Wine Review Dataset"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "869ab89f0a395b88436c057465beba7fd25fc2c0"
      },
      "cell_type": "code",
      "source": "reviews = pd.read_csv(\"../input/wine-reviews/winemag-data_first150k.csv\", index_col=0)\n\n# Keep only the lower-priced wines...\nmd = reviews[reviews['price'] < 120]\n\n# .. and split the numeric Points into bins and convert to a categorical Score...\nbins = [0,84,89,100]\nlabels=[1, 2, 3]\nnum_to_cat (md, 'points', 'score', bins, labels)\n\n# ... and Plot the Density plot of Price for different values of the Score\nkde_target('price', md, 'score')\n\n# Find missing values...\nmt = missing_values_summary(reviews)\n\n# ... and find any columns which have more than 90% missing values, and drop them from the data\nmt_cols = list(mt.index[mt['% of Total Values'] > 90])\nreviews = reviews.drop(columns = mt_cols)\n\n# Compute aggregate statistics for numeric columns ...\nagd = agg_numeric(md, group_var = 'country', stats = ['count', 'mean', 'max', 'min', 'sum'], df_name = 'ctry')\n\n# ...and merge with the main data, and fill in empty values with 0s\nagm = md.merge(agd, on = 'country', how = 'left')\nfill_missing (agm)\n\nagc = target_corrs (agm, 'points')\nagc[:15]\n\n# Encode the categorical Country name as a numeric Country ID\nencode_cat (agm, 'country', 'country_id')\nagm.head()\n\n# One-hot encode the categorical Variety column\nod = encode_cat_onehot (agm[['country', 'country_id', 'variety']], 'variety')\nod.head()\n\n# Note that we are aggregating stats for categorical columns which have now been one-hot encoded.\n# Since these were not originally numeric values, the most meaningful stats are mean and sum\n# Other stats like min and max do not make sense for such columns\n# \n# The sum represents the count of that category value for that Country ID and \n# the mean represents the normalized count of that category value for that Country ID \n# ie. count of that value / total count of all values for that Country ID\n# One-hot encoding makes the process of calculating these counts and normalised counts very easy\nodm = agg_numeric(od, group_var = 'country_id', stats = ['mean', 'sum'], df_name = 'ctry')\n\n# For some reason this step takes a long time and spikes the CPU to 100%. Need to check why.\nodn = od.merge(odm, on = 'country_id', how = 'left')\nodn.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3a2b2855dea64dd64f6d4717b8aa51741d95b1da"
      },
      "cell_type": "markdown",
      "source": "## Titanic Dataset"
    },
    {
      "metadata": {
        "_uuid": "83ce84d2c8fba4d398c6081ea0a84615cfa9736f"
      },
      "cell_type": "markdown",
      "source": "### Load the data and Preview it"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7a929f82efeab716106114640f10344b2a271751"
      },
      "cell_type": "code",
      "source": "# Load training data\ntrain_df = pd.read_csv('../input/titanic-machine-learning-from-disaster/train.csv')\n\n# Categorise features by type\ntarget_feature = ['Survived'] # Nominal\nnominal_features = ['Survived', 'Sex', 'Embarked']\nordinal_features = ['Pclass']\nnumeric_continuous_features = ['Age', 'Fare']\nnumeric_discrete_features = ['SibSp', 'Parch']\nremove_features = ['PassengerId', 'Ticket', 'Cabin', 'Name'] # These may require data to be cleaned\nfeatures_dict = {'nominal': nominal_features,\n                'ordinal' : ordinal_features,\n                'continuous' : numeric_continuous_features,\n                'discrete' : numeric_discrete_features,\n                'remove' : remove_features,\n                'target' : target_feature}\n\n# preview the data\ntrain_df.sample(10)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "60c9040d067eddae7883c7325d6d59cad1be2cc0"
      },
      "cell_type": "markdown",
      "source": "### Summarise the data\n* Shape, Columns, Datatypes\n* Min-max Values\n* Stats like Standard Dev, Quartiles\n* Anomalies and Outliers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cea2d85a19b764830a660ee69b8c49b7a00d4b5"
      },
      "cell_type": "code",
      "source": "# Check the quantiles and min/max to see if any of the features have anomalies or outliers\ntrain_df.describe(include = 'all')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e070eb6147a0a04f26ad176301ca8eaf16e5cbf5"
      },
      "cell_type": "code",
      "source": "data_summary (train_df, features_dict)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd7b9c6a389ffb63ef61bcbb767316d91ae7f171"
      },
      "cell_type": "markdown",
      "source": "### Visualise the data\n* Number of unique values for Categorical features\n* Distribution of the data\n* Missing Values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "94fa17caf7a90a80dffe08b4a90b43de2878a09f"
      },
      "cell_type": "code",
      "source": "data_stats (train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "db1ec8d191a3cee745f6701b015e8b001f0f0f01"
      },
      "cell_type": "code",
      "source": "data_distrib (train_df, ['Survived', 'Sex', 'Embarked', 'Pclass', 'SibSp', 'Parch'], ['Age', 'Fare'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7989fd3770b14b3bb13cd96daaea7d4d506dcd1f"
      },
      "cell_type": "markdown",
      "source": "### Correct and Clean the data\n* Remove any unnecessary columns\n* Remove columns with majority of missing values\n* Remove or clean outlier data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5d19c93edb5e3aa7e8856d5e76d2bd15c05c009"
      },
      "cell_type": "code",
      "source": "# CORRECT & CLEAN\n# --------------------------\n\n# Exclude features which will not impact the Target feature\nremove_features = ['PassengerId','Cabin', 'Ticket']\nremove_cols ([train_df], remove_features)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6630b69033e1b5310f464d0e6435232524db38b0"
      },
      "cell_type": "markdown",
      "source": "### Complete the data\n* Fill any missing values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b99ae62ec6a2b96faf5bae2bb5693e54bb036b23"
      },
      "cell_type": "code",
      "source": "# COMPLETE\n# --------------------------\n\n# Impute all missing values\nimpute_missing_values (train_df, ['Age', 'Embarked'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d1219dcf71a5f34d4074c7c94b62f3d973583798"
      },
      "cell_type": "markdown",
      "source": "### Convert the data\n* Convert Numeric Continuous features to Categorical by Binning\n* Encode Categorical Nominal (text) features with Label Encoding or One-Hot Encoding\n* Scale/Normalise Numeric Continuous features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be8c6cf45961dbaad27a91ef5510760c7b754550"
      },
      "cell_type": "code",
      "source": "# CONVERT\n# --------------------------\n# Define categorical variables\ntrain_df['Sex'] = pd.Categorical(train_df['Sex'])\ntrain_df['Embarked'] = pd.Categorical(train_df['Embarked'])\n\n# Convert numerical features to ordinal by binning\nnum_to_cat (train_df, 'Age', 'AgeBin', 8, range(1, 9))\nnum_to_cat (train_df, 'Age', 'AgeType', bins=[0, 12, 50, 200], labels=['Child','Adult','Elder'])\n\n# Label encode categorical features\nencode_cat (train_df, 'Sex', 'SexCode')\nencode_cat (train_df, 'Embarked', 'EmbarkedCode')\n\n# One-hot encode categorical features\n# NB: This works only for categorical features with string values not numbers\n# So doing this for the same features again simply as a test exercise\ntrain_df = encode_cat_onehot (train_df, ['Sex', 'Embarked'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f416ef236ac94132325b0057935c83e003ec1729"
      },
      "cell_type": "code",
      "source": "train_df.sample(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9cfbcdec8db2de8a5e8a06fd021efd0b908e766b"
      },
      "cell_type": "markdown",
      "source": "### Run a Baseline Model\n* Try variations with Label-Encoded and One-Hot Encoded versions\n* Try variations with and without Normalised feature values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45353f7ed2fa1bbaec6d8152295045c451887b5f"
      },
      "cell_type": "code",
      "source": "# First try with label encoded features\nX = train_df [['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'SexCode', 'EmbarkedCode']]\ny = train_df ['Survived']\n\nvalidate_model (X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a4d755753ce7a29cdb15642948850ccc2f870a5"
      },
      "cell_type": "code",
      "source": "# Try with one-hot encoded features\nX = train_df [['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']]\ny = train_df ['Survived']\n\nvalidate_model (X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14d0ba3d46e90ef2b3dad5714140d3b6bc4cb081"
      },
      "cell_type": "code",
      "source": "# CONVERT\n# --------------------------\n# Min-max scaling of numerical features\nminmax_scaling (train_df, ['Age', 'Fare'])\n\n# Try with one-hot encoded features\nX = train_df [['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex_male', 'Embarked_Q', 'Embarked_S']]\ny = train_df ['Survived']\n\nvalidate_model (X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4ee0f7c54f79e3a8e90ce057d8f81e9942d54fe9"
      },
      "cell_type": "markdown",
      "source": "### Exploratory Data Analysis - Univariate"
    },
    {
      "metadata": {
        "_uuid": "df8f58b149c0ee814b49e36defb3ad598185fcde"
      },
      "cell_type": "markdown",
      "source": "### Exploratory Data Analysis - Bivariate (with the Target feature)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1815de90a71c5caa9c2338e32c0650f56651eb42"
      },
      "cell_type": "code",
      "source": "data_target (train_df, ['Sex', 'Embarked', 'SibSp', 'Parch', 'Pclass', 'Title', 'AgeBin', 'AgeType', 'FamilySize'], ['Age', 'Fare'], 'Survived')\nplot_stackhist(train_df, ['Pclass', 'SibSp', 'Parch', 'Fare', 'Age', 'Sex'], 'Survived')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "573b86bce73ea0c7ecf42141e168cb709cb4cdd5"
      },
      "cell_type": "markdown",
      "source": "### Create new features based on Domain Knowledge"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "681d10dc3bc0bdb43b0baed2109c48957d84d32f"
      },
      "cell_type": "code",
      "source": "# CREATE\n# --------------------------\n# Create Family feature\ntrain_df['FamilySize'] = train_df['SibSp'] + train_df['Parch']\n\nX = train_df [['Pclass', 'Age', 'FamilySize', 'Fare', 'SexCode', 'EmbarkedCode']]\ny = train_df ['Survived']\n\nvalidate_model (X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "18cac0c12ed79fdf96928a27aef1c8e927bc924a"
      },
      "cell_type": "code",
      "source": "# CREATE\n# --------------------------\n# Create Title feature\n\n# Extract titles from name\ntrain_df['Title']=0\nfor i in train_df:\n    train_df['Title']=train_df['Name'].str.extract('([A-Za-z]+)\\.', expand=False)  # Use REGEX to define a search pattern\n\n# Count how many people have each of the titles\nprint (train_df.groupby(['Title'])['Name'].count())\n\n# Map of aggregated titles, so that less frequently used titles can be\n# merged with the common ones\ntitles_dict = {'Capt': 'Other',\n               'Major': 'Other',\n               'Jonkheer': 'Other',\n               'Don': 'Other',\n               'Sir': 'Other',\n               'Dr': 'Other',\n               'Rev': 'Other',\n               'Countess': 'Other',\n               'Dona': 'Other',\n               'Mme': 'Mrs',\n               'Mlle': 'Miss',\n               'Ms': 'Miss',\n               'Mr': 'Mr',\n               'Mrs': 'Mrs',\n               'Miss': 'Miss',\n               'Master': 'Master',\n               'Lady': 'Other'}\n\n# Group titles\ntrain_df['Title'] = train_df['Title'].map(titles_dict)\n\n# Transform into categorical\ntrain_df['Title'] = pd.Categorical(train_df['Title'])\n\n# Fill missing values and encode\ntrain_df['Title'] = train_df['Title'].fillna('Other')\nencode_cat (train_df, 'Title', 'TitleCode')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d1bc4e070e93bafc296f0ad26af65573c079813"
      },
      "cell_type": "code",
      "source": "data_correlations (train_df, ['dummy'], 'Survived')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59478fda7802a4819d477158a8a270185424b46b"
      },
      "cell_type": "markdown",
      "source": "### Create new features based on Polynomials"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de1e21a74df63d065e3207f97616bd1d886bd2f3"
      },
      "cell_type": "code",
      "source": "# CREATE (POLYNOMIAL)\n# --------------------------\n\nX = train_df [['Pclass', 'Age', 'FamilySize', 'Fare', 'SexCode', 'EmbarkedCode', 'TitleCode']]\nX, _ = polynomial_features (X, None)\ny = train_df ['Survived']\n\nvalidate_model (X, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "59c0724502c8835cfe339a030dc8bd1a03726ee7"
      },
      "cell_type": "markdown",
      "source": "### Feature Selection\n* Remove redundant features\n* Select the best combination of features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8e203472135842771cbf8c2b410fe05c2e0a4e62"
      },
      "cell_type": "code",
      "source": "base_score = 0.792\nk_best_features = feature_select(X, y, base_score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9a66b8205a2927d5cc6e915e6d65f5c073836651"
      },
      "cell_type": "code",
      "source": "# Validate the model with the best selection of features\nX_best = selected_df (X, y, k_best_features)\nvalidate_model (X_best, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "618cf2023e9bbbaba5602d684cc13919cb2be57f"
      },
      "cell_type": "code",
      "source": "# AGGREGATE\n# --------------------------\n\n# Compute aggregate statistics for numeric columns ...\n#agd = agg_numeric(train_df, group_var = 'country', stats = ['count', 'mean', 'max', 'min', 'sum'], df_name = 'ctry')\n# ...and merge with the main data, and fill in empty values with 0s\n#agm = md.merge(agd, on = 'country', how = 'left')\n#fill_missing (agm)\n\n# Note that we are aggregating stats for categorical columns which have now been one-hot encoded.\n# Since these were not originally numeric values, the most meaningful stats are mean and sum\n# Other stats like min and max do not make sense for such columns\n# \n# The sum represents the count of that category value for that Country ID and \n# the mean represents the normalized count of that category value for that Country ID \n# ie. count of that value / total count of all values for that Country ID\n# One-hot encoding makes the process of calculating these counts and normalised counts very easy\n#odm = agg_numeric(od, group_var = 'country_id', stats = ['mean', 'sum'], df_name = 'ctry')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a139fc36b33c980f65bc787c44b16d99665c70a0"
      },
      "cell_type": "markdown",
      "source": "### Exploratory Data Analysis - Multivariate \n* for specific features as needed\n\nThe following plots are practice examples of possible visualisations with Seaborn which may be useful. But at the moment I'm not able to figure out what concrete uses I can put them to, nor can I come up with patterns that can be applied generically."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1a2f42f33de87851e0a4b9b0b8af8491844b402"
      },
      "cell_type": "code",
      "source": "sns.set(style=\"darkgrid\")\n\n# Line plot with standard deviation\nsns.relplot(x=\"AgeBin\", y=\"Survived\", kind=\"line\", ci=\"sd\", data=train_df);",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "34142eb2df8fb25aef9fa83c2ee978aea3632af5"
      },
      "cell_type": "code",
      "source": "# 2 variables (Sex and Pclass) with target (Survived)\nsns.relplot(x=\"Pclass\", y=\"Survived\", hue = \"Sex\", kind=\"line\", ci=None, data=train_df);\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"point\", data=train_df);\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=train_df)\nsns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", kind=\"bar\", data=train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8d558d01200eade6a99369782a54aeb24a5930c1"
      },
      "cell_type": "code",
      "source": "# Plot totals not just mean\nsns.catplot(x=\"Sex\", y=\"Survived\", kind=\"bar\", estimator=sum, data=train_df)\n\n# One numeric and one categorical (Sex and Age) with Target\nsns.catplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", kind=\"violin\", split=\"True\", data=train_df);\n\n# One numeric and one categorical (Fare and Pclass) with Target\ng = sns.catplot(x=\"Fare\", y=\"Survived\", row=\"Pclass\",\n                kind=\"box\", orient=\"h\", height=1.5, aspect=4,\n                data=train_df)\ng.set(xscale=\"log\");",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7d924afae6a0442d70dff2d691d1651706466247"
      },
      "cell_type": "code",
      "source": "sns.catplot(x=\"AgeBin\", y=\"Survived\", hue=\"Sex\", kind=\"bar\", data=train_df)\n\nsns.catplot(x=\"SibSp\", y=\"Survived\", hue=\"Sex\", kind=\"bar\", data=train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89a09eef9beca38cd93c5c1d8f0aca72e52ec095"
      },
      "cell_type": "code",
      "source": "# Plot 4 variables using column facets\nsns.catplot(x=\"AgeBin\", y=\"Survived\", hue=\"Sex\", col = \"Pclass\", kind=\"bar\", data=train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7d141f223e0f85400344166c42d56171433f23c"
      },
      "cell_type": "code",
      "source": "# Plot KDE histograms with one variable and Target\nfig, axis = plt.subplots(1,2,figsize=(14,6))\n\n# Age vs Target on the left\nsns.kdeplot (train_df[train_df['Survived'] == 0]['Age'], label=\"Died\", ax=axis[0])\nsns.kdeplot (train_df[train_df['Survived'] == 1]['Age'], label=\"Lived\", ax=axis[0])\n\n# Fare vs Target on the right\nsns.kdeplot (train_df[train_df['Survived'] == 0]['Fare'], label=\"Died\", ax=axis[1])\nsns.kdeplot (train_df[train_df['Survived'] == 1]['Fare'], label=\"Lived\", ax=axis[1])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fadd6915dcab43c29b73b3a3d0be232c687cd19e"
      },
      "cell_type": "code",
      "source": "# Use FacetGrid instead of calling KDEplot directly\na = sns.FacetGrid( train_df, hue = 'Survived', aspect=3 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , train_df['Age'].max()))\na.add_legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d933052f3fc9f367a9917cd878ac3bd05fc5394"
      },
      "cell_type": "code",
      "source": "# Use FacetGrid to plot 2 variables (Sex and Pclass) vs Target\nh = sns.FacetGrid(train_df, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8719f0a275af0eebf8c358aad3b8f77178f8392a"
      },
      "cell_type": "code",
      "source": "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(14,8))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = train_df, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = train_df, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7eb11d950990982284bb76bf2fe513159c1d20fd"
      },
      "cell_type": "code",
      "source": "e = sns.FacetGrid(train_df, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3a2f1ffdce438dd932f36275f1393fb82877ccd6"
      },
      "cell_type": "code",
      "source": "sns.catplot (x=\"AgeBin\", y=\"Survived\", hue=\"Sex\", row=\"Embarked\", col = \"Pclass\", kind=\"bar\", data=train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ffec809782ac6a17574afc8c20ad5f723592d4fa"
      },
      "cell_type": "code",
      "source": "sns.catplot (x=\"Sex\", y=\"Age\", hue=\"Survived\", row=\"Embarked\", col = \"Pclass\", kind=\"violin\", data=train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edfb23f5ce5fa787b62791ddb765cb1b13ca5f87"
      },
      "cell_type": "code",
      "source": "plot_pair(train_df, ['Survived', 'Pclass', 'SibSp', 'Parch', 'Age'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5a00f7b4901e445f165df5558f701545ee247a1b"
      },
      "cell_type": "markdown",
      "source": "## Other Functions\n\nThis [notebook](http://https://www.kaggle.com/willkoehrsen/introduction-to-manual-feature-engineering-p2) had a few more functions. Include them if they are going to be generically useful.\n* Function to Convert Data Types (def convert_types(df, print_info = False)) - this will help reduce memory usage by using more efficient types for the variables. For example category is often a better type than object (unless the number of unique categories is close to the number of rows in the dataframe).\n* Function to Drop Missing Columns (def remove_missing_columns(train, test, threshold = 90))\n* Function to Aggregate Stats Per Client (def aggregate_client(df, group_vars, df_names)) - for parent-child data tables. In this case the child tables contained data per-loan for each user, while the parent contained data per-user. So data rolled up at the per-loan level had to be then be rolled up again at the user level and merged with the parent data.\n* Function to run a ML model (def model(features, test_features, encoding = 'ohe', n_folds = 5)) - Train and test a light gradient boosting model using cross validation, and also calculating feature importances\n* Function to plot Feature Importances (def plot_feature_importances(df))"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}