
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/app_lib.ipynb

from pathlib import Path
import math
from functools import partial
import torch
from torch import nn
import torch.nn.functional as F
import torch.optim as optim


from nb_util import mse
from nb_hooks import Hooks, DebugActivationHook
from nb_arch import ArchBase
from nb_training import Trainer, CudaCB, ProgressCallback, MetricsGrp, LossMetricsCB, DebugTracker, DebugYhatLossCB
from nb_optimiser import HyperParams, Recorder, LRRangeFind


from torch.nn.utils.rnn import pad_sequence
from nb_data import DataBundle, CSVItemContainer, DfItemList, SentenceItemList, SentenceWordItemList, SentenceWordIdItemList, SortishSampler, SortSampler

# ----------------------------
# Collate function to convert a list of item tuples into a single tensor which is fed
# as input to the model
#
# 'Samples' is a list of tuples ie. [(x1, y1), (x2, y2), ...] where 'xn' and 'yn' are
# both lists of word ids in the sentence. All the sentences could have different lengths, so
# after each 'xn' and 'yn' are converted to tensors, they are padded at the end to
# make them the same length.
# ----------------------------
def seq_collate(samples, pad_idx=1, pad_first=False):
  # Convert each sample sentence into a tensor and then create two lists of
  # tensors ie. [tensor x1, tensor x2, ...] and [tensor y1, tensor y2, ...]
  tx, ty = [torch.tensor(s[0]) for s in samples], [torch.tensor(s[1]) for s in samples]

  # Use the Pytorch pad_sequence function to pad each sample tensor to the same length and then
  # concatenate them into a single tensor. So 'px' and 'py' have shape (samples, max sequence length)
  px, py = pad_sequence(tx, batch_first=True, padding_value=pad_idx), pad_sequence(ty, batch_first=True, padding_value=pad_idx)
  return px, py

#----------------------------------------------------
# The custom sampler functions need to take a sort-key-function as an extra argument.
# The key function needs the 'data' argument pre-bound using a partial.
#
# This is a standalone function rather than a lambda function, because pickle is not able to
# save lambda functions.
#----------------------------------------------------
def len_key_fn(i,data):
  return len(data[i])

#----------------------------------------------------
# Text Translation from CSV data preparation pipeline
#----------------------------------------------------
class TextTranslationCSVDataBundle(DataBundle):
  def __init__(self, csv_path, bs):
    print ('--------- Text Translation DataBundle init', csv_path)

    # Load all rows from the given CSV file
    # Split randomly based on a percentage ratio for training and validation
    # 'x' items are taken from 'fr' column as text sentences and
    # 'y' labels are taken from 'en' column as class name labels
    # Convert the 'x' items from Sentences to Words to Word Ids
    # Convert the 'y' items from Sentences to Words to Word Ids

    load_params = {'source': CSVItemContainer, 'target_cls': DfItemList, 'csv_path': csv_path}
    split_params = {'split_procedure': 'split_random', 'train_ratio': 0.8, 'valid_ratio': 0.2}
    extract_x_params = {'extract_procedure': 'extract_colval', 'target_cls': SentenceItemList, 'col': 'fr', 'lang': 'fr'}
    extract_y_params = {'extract_procedure': 'extract_colval', 'target_cls': SentenceItemList, 'col': 'en'}
    convert_x_params = [
        {'target_cls': SentenceWordItemList, 'convert_procedure': 'SentenceToWord'},
        {'target_cls': SentenceWordIdItemList, 'convert_procedure': 'WordToWordId'}
    ]
    convert_y_params = [
        {'target_cls': SentenceWordItemList, 'convert_procedure': 'SentenceToWord'},
        {'target_cls': SentenceWordIdItemList, 'convert_procedure': 'WordToWordId'}
    ]

    # We use different samples for training and validation.
    dl_params = (
        {'bs': bs, 'sampler_fn': SortishSampler, 'key_fn': len_key_fn, 'collate_fn': seq_collate},    # for training
        {'bs': bs, 'sampler_fn': SortSampler, 'key_fn': len_key_fn, 'collate_fn': seq_collate}        # for valid/test
    )
    WAIT_dl_params = (
        {'bs': bs, 'shuffle': False, 'collate_fn': seq_collate},    # for training
        {'bs': bs, 'shuffle': False, 'collate_fn': seq_collate}     # for valid/test
    )
    self.display_params = {
        'layout_procedure': 'display_texts'
    }
    super().__init__(load_params, split_params, extract_x_params, extract_y_params, convert_x_params, convert_y_params, dl_params=dl_params)

  # ----------------------------
  # Since we use Sorting Samplers which require a key function (which also requires a partial wrapper)
  # we have to override the parent DataBundle with a custom get_sampler()
  # ----------------------------
  def get_sampler(self, ds, in_train, bs, sampler_fn, key_fn, **kwargs):
    key=partial(key_fn, data=ds.x)
    # The two sampler functions take different arguments. Since we know the sampler
    # functions we can hardcode their names. The sampler_fn that is passed in is the
    # same, but we ignore it.
    if (in_train):
      sampler = SortishSampler(ds.x, key=key, bs=bs)
    else:
      sampler = SortSampler(ds.x, key=key)
    return sampler


from math import exp

class NGram():
    def __init__(self, ngram, max_n=5000):
      #print('===', ngram)
      self.ngram,self.max_n = ngram,max_n
    def __eq__(self, other):
        if len(self.ngram) != len(other.ngram): return False
        return np.all(np.array(self.ngram) == np.array(other.ngram))
    def __hash__(self):
      hml = [o * self.max_n**i for i,o in enumerate(self.ngram)]
      hm = int(sum(hml))
      return hm

def get_grams(x, n, max_n=5000):
    return x if n==1 else [NGram(x[i:i+n], max_n=max_n) for i in range(len(x)-n+1)]

def get_correct_ngrams(pred, targ, n, max_n=5000):
    pred_grams,targ_grams = get_grams(pred, n, max_n=max_n),get_grams(targ, n, max_n=max_n)
    pred_cnt,targ_cnt = Counter(pred_grams),Counter(targ_grams)
    precm = [(c, targ_cnt[g]) for g,c in pred_cnt.items()]
    precl = [min(c, g) for c, g in precm]
    prec, ln = sum(precl),len(pred_grams)
    return prec, ln

def corpus_bleu(preds, targs, max_n=5000):
    pred_len,targ_len,n_precs,counts = 0,0,[0]*4,[0]*4
    tmp_preds = preds.argmax(dim=-1)
    tmp_preds, tmp_targs = tmp_preds.cpu().numpy(), targs.cpu().numpy()
    for pred,targ in zip(tmp_preds, tmp_targs):
        pred_len += len(pred)
        targ_len += len(targ)
        for i in range(4):
            c,t = get_correct_ngrams(pred, targ, i+1, max_n=max_n)
            n_precs[i] += c
            counts[i] += t
    #db.set_trace()
    n_precs = [c/t if (t > 0) else 0 for c,t in zip(n_precs,counts)]
    len_penalty = exp(1 - targ_len/pred_len) if pred_len < targ_len else 1
    return len_penalty * ((n_precs[0]*n_precs[1]*n_precs[2]*n_precs[3]) ** 0.25)



class AppBase():
  def __init__(self, loss_type='bin_classif', metrics_cbs=[]):
    self._arch = None
    self.db = None

    self.debug_cbs = []
    self.dtr = None
    self.hooks = None

    # Select the appropriate loss function for the type of problem
    if (loss_type == 'bin_classif'):
      # Binary classification problems
      self.loss_fn = F.binary_cross_entropy_with_logits
      #self.loss_fn = nn.BCELoss()
    elif (loss_type == 'multi_classif'):
      # Multi-class classification problems
      self.loss_fn = F.cross_entropy
    elif (loss_type == 'regression'):
      self.loss_fn = mse
    assert(self.loss_fn)

    metrics_cbs = [LossMetricsCB()] + metrics_cbs
    self.metrics_cbs = metrics_cbs + [MetricsGrp(metrics_cbs)]

  # ----------------------------
  # Create the debug settings
  # ----------------------------
  def create_debug(self, use_dtr=False, track_batches_per_epoch=5, disp_tb=False, disp_pd=True, debug_bkwd=False, debug_fwd=False, abort_iter=0):
    dtr, hooks, debug_cbs = None, None, []
    if (use_dtr):
      dtr = DebugTracker(max_count=track_batches_per_epoch, disp=(disp_tb, disp_pd))
      debug_cbs += [dtr, DebugYhatLossCB(fwd=False, bkwd=debug_bkwd)]

      # Add hooks for the forward pass activations
      if (debug_fwd):
        # Arch and Model should be created already
        assert(self._arch and self._arch.model)
        arch = self._arch
        model = arch.model

        # Add Debug Hooks to the hook_layers and save a list of all the hooks
        hook_cls=[[partial(DebugActivationHook, do_print=False, model=model, dtr=dtr)]]
        hook_groups = arch.hook_groups()
        hooks = Hooks(hook_groups, hook_cls)

    if (abort_iter > 0):
      debug_cbs += [AbortTrainCB(abort_iter)]

    self.dtr, self.hooks, self.debug_cbs = dtr, hooks, debug_cbs

  #----------------------------------------------------
  # Train the model
  #----------------------------------------------------
  def run_train(self, num_epochs=1, split_lr=[1e-3], weight_decay=0.2, one_cycle=False, app_cbs=[]):
    assert(isinstance(one_cycle, bool))

    train_dl = self.db.train_dl
    valid_dl = self.db.valid_dl

    # Loss function
    loss_func = self.loss_fn

    # Model
    arch = self._arch
    model = arch.model

    opt_adamw = partial(optim.AdamW, betas=(0.9, 0.99), weight_decay=weight_decay)
    lr_sched='one_cycle' if one_cycle else None
    opt, hyper_cbs = HyperParams.set(model, module_groups=None, split_lr=split_lr, split=False, lr_sched=lr_sched, opt_func=opt_adamw)

    gpu_cbs = [CudaCB(device = torch.device('cuda',0))]
    track_cbs = [Recorder(), ProgressCallback()]
    callbs = gpu_cbs + track_cbs
    callbs += app_cbs
    callbs += self.metrics_cbs + hyper_cbs + self.debug_cbs

    loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs, dtr=self.dtr)
    loop.hooks = self.hooks

    loop.fit(num_epochs=num_epochs)
    return loop

  # ----------------------------
  # Learning Rate Finder
  # ----------------------------
  def lr_find(self, start_lr, end_lr, num_iter, weight_decay=0.01, app_cbs=[]):

    train_dl = self.db.train_dl
    valid_dl = None

    # Loss function
    loss_func = self.loss_fn

    # Model
    arch = self._arch
    model = arch.model

    opt_adamw = partial(optim.AdamW, betas=(0.9, 0.99), weight_decay=weight_decay)
    opt, hyper_cbs = HyperParams.set(model, module_groups=None, split_lr=[start_lr], split=False, lr_sched='lrf', opt_func=opt_adamw, start_lr=start_lr, end_lr=end_lr, num_iter=num_iter)

    gpu_cbs = [CudaCB(device = torch.device('cuda',0))]
    track_cbs = [Recorder(), ProgressCallback()]
    lrf_cbs = [LRRangeFind(num_iter)]
    callbs = gpu_cbs + track_cbs
    callbs += app_cbs
    callbs += lrf_cbs + hyper_cbs

    loop = Trainer(train_dl, valid_dl, model, opt, loss_func, callbs, dtr=None)

    num_epochs = int(math.ceil(num_iter / len(train_dl)))
    loop.fit(num_epochs=num_epochs)
    return loop


#----------------------------------------------------
# Tabular model with Entity Embedding for categorical features followed
# by Linear blocks
#
# 'emb_szs' is a list of tuples [(num_categories, embedding_dim), (), ...] with one
# tuple per categorical feature
# 'lin_hs' is a list of hidden sizes for each linear block [hid1, hid2, ...]
#----------------------------------------------------
class Tabular(nn.Module):
  def __init__(self, n_cat, n_cont, emb_szs, emb_p, hidden_szs, hidden_ps, out_sz, out_range):
    super().__init__()
    self.n_cat, self.n_cont = n_cat, n_cont
    self.out_range = out_range

    # Embedding layers and Dropout for categorical features
    self.cat_emb = nn.ModuleList([nn.Embedding(ln,dim) for ln, dim in emb_szs])
    self.emb_drop = nn.Dropout(emb_p)

    # BatchNorm layers for continuous features
    # !!!!!!!!!!!!!
    #self.cont_bn = nn.BatchNorm1d(n_cont, momentum=0.5)
    self.cont_bn = nn.BatchNorm1d(n_cont)

    # Input and output feature sizes for each layer
    total_emb_dim = sum([dim for _, dim in emb_szs])
    inp_sz = total_emb_dim + n_cont
    n_ins = [inp_sz] + hidden_szs
    n_outs = hidden_szs + [out_sz]

    # Linear block layers with Linear, ReLU, BatchNorm and Dropout
    layers = []
    for n_in, n_out, drop_p in zip(n_ins, n_outs, hidden_ps):
      lin = nn.Linear(n_in, n_out)
      relu = nn.ReLU(inplace=True)
      bn = nn.BatchNorm1d(n_out)
      drop = nn.Dropout(drop_p)
      layers += [lin, relu, bn, drop]

    # Output layer
    out = nn.Linear(hidden_szs[-1], out_sz)

    # Wrap all the layers into Sequential
    layers += [out]
    self.layers = nn.Sequential(*layers)

  def forward(self, inp):
    # Separate categorical and continuous features. The initial columns are
    # categorical
    cat_inp = inp[:, :self.n_cat].long()
    cont_inp = inp[:, self.n_cat:].float()

    # Pass each categorical feature through its embedding and dropout
    if (self.n_cat > 0):
      emb_vals = [emb(cat_inp[:, i])  for i, emb in enumerate(self.cat_emb)]
      emb_val = torch.cat(emb_vals, axis=1)
      emb_val = self.emb_drop(emb_val)
    else:
      # Empty tensor
      emb_val = cat_inp

    # Process the continuous features through batch norm
    if (self.n_cont > 0):
      cont_val = self.cont_bn(cont_inp)
    else:
      # Empty tensor
      cont_val = cont_inp

    # Concatenate the embedding and continuous values
    combined_val = torch.cat([emb_val, cont_val], axis=1)

    # Process the sequential linear layers
    output = self.layers(combined_val)

    if (self.out_range):
      output = self.out_range[0] + torch.sigmoid(output) * (self.out_range[1] - self.out_range[0])
    return output

#----------------------------------------------------
# Create the Tabular architecture
#----------------------------------------------------
class ArchTabular(ArchBase):
  # ----------------------------
  # Create the Tabular model. Calculates the number and size of all the Entity Embedding
  # layers for the categorical variables
  # ----------------------------
  def create_model(self, cat_szs, n_cont, emb_p, hidden_szs, hidden_ps, out_sz=1, out_range=None):

    #def emb_sz_rule(cat_sz:int)->int: return min(50, (cat_sz//2)+1)
    def emb_sz_rule(cat_sz:int)->int: return min(600, round(1.6 * cat_sz**0.56))

    n_cat = len(cat_szs)
    emb_szs = [(cat_sz, emb_sz_rule(cat_sz)) for cat_sz in cat_szs]

    # Build the Tabular model
    self.model = Tabular(n_cat, n_cont, emb_szs, emb_p, hidden_szs, hidden_ps, out_sz, out_range)

  def hook_groups(self):
    model = self.model
    layer_modules = list(model.layers.modules())
    hk_groups = [list(model.cat_emb) + [model.cont_bn, layer_modules[1], layer_modules[3], layer_modules[5], layer_modules[7], layer_modules[-1]]]
    return hk_groups


#----------------------------------------------------
# Tabular Application
#----------------------------------------------------
class AppTabular(AppBase):
  # ----------------------------
  # Load the data using the Tabular Data Bundle
  # ----------------------------
  def load_data(self, app_db, main_file_path, test_file_path, related_csv_paths, steps=['load', 'post_load'], **kwargs):
    if ('load' in steps):
      self.db = app_db(main_file_path, test_file_path, related_csv_paths, **kwargs)
      self.db.process(steps=['load'])

    if ('post_load' in steps):
      self.db.process(steps=['post_load'])
      self.n_cont, self.cat_szs, self.n_tgt, self.tgt_range = self.db.col_szs()

  # ----------------------------
  # Create the architecture
  # ----------------------------
  def create_arch(self, emb_p=0.04, hidden_szs=[1000, 500], hidden_ps=[.001, .01]):
    self._arch = ArchTabular()
    self._arch.create_model(self.cat_szs, self.n_cont,
                            emb_p=emb_p, hidden_szs=hidden_szs, hidden_ps=hidden_ps,
                            out_sz=self.n_tgt, out_range=self.tgt_range)
    return self._arch

  @staticmethod
  def subset_path(app_dir, gd_path, subset_sz):
    subset_root_path = Path(gd_path)/'data'
    subset_data_path = subset_root_path/f'{app_dir}_{subset_sz}'
    return subset_data_path

  @staticmethod
  def set_data_path (app_dir, gd_path, subset_sz, data_files):
    root_path = Path.cwd()
    download_path = root_path/app_dir

    subset_data_path = AppTabular.subset_path(app_dir, gd_path, subset_sz)
    debug_path = subset_data_path/'debug'
    hist_path = subset_data_path/'hist'

    if (subset_sz == 'full'):
      # In this case data is taken from the download path, and subset data path is
      # used only for debug data.
      data_path = download_path
    else:
      data_path = subset_data_path

    main_file_path = data_path/data_files['main']
    test_file_path = data_path/data_files['test']
    related_csv_paths = [data_path/file for file in data_files['related']]

    return (root_path, download_path, data_path, debug_path, hist_path, main_file_path, test_file_path, related_csv_paths)